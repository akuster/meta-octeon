From b6e6589ebc1cab3b7cf06914ae0299bbc7522ac1 Mon Sep 17 00:00:00 2001
From: Armin Kuster <akuster@mvista.com>
Date: Fri, 11 Sep 2015 16:32:56 -0700
Subject: [PATCH 1/5] Octeon: update executive files to latest SDK

Signed-off-by: Armin Kuster <akuster@mvista.com>
---
 arch/mips/cavium-octeon/executive/Makefile         |    4 +-
 .../executive/cvmx-appcfg-transport.c              |    1 +
 arch/mips/cavium-octeon/executive/cvmx-bch.c       |  110 +-
 arch/mips/cavium-octeon/executive/cvmx-bootmem.c   |    6 +-
 arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c |    2 +-
 .../mips/cavium-octeon/executive/cvmx-dma-engine.c |    2 +-
 .../cavium-octeon/executive/cvmx-fpa-resource.c    |   14 +-
 .../executive/cvmx-global-resources.c              |    9 +-
 .../mips/cavium-octeon/executive/cvmx-helper-bgx.c | 2926 +++++++++++++++++---
 .../cavium-octeon/executive/cvmx-helper-board.c    |  388 ++-
 .../mips/cavium-octeon/executive/cvmx-helper-cfg.c |   73 +-
 .../cavium-octeon/executive/cvmx-helper-errata.c   |    4 +-
 .../mips/cavium-octeon/executive/cvmx-helper-ilk.c |   14 +-
 .../mips/cavium-octeon/executive/cvmx-helper-ipd.c |   32 +-
 .../cavium-octeon/executive/cvmx-helper-loop.c     |   37 +-
 .../mips/cavium-octeon/executive/cvmx-helper-npi.c |   12 +-
 .../mips/cavium-octeon/executive/cvmx-helper-pki.c |   35 +-
 .../mips/cavium-octeon/executive/cvmx-helper-pko.c |   19 +-
 .../cavium-octeon/executive/cvmx-helper-pko3.c     | 1557 ++++++++---
 .../cavium-octeon/executive/cvmx-helper-rgmii.c    |    2 +-
 .../cavium-octeon/executive/cvmx-helper-sgmii.c    |    5 +-
 .../cavium-octeon/executive/cvmx-helper-srio.c     |  232 +-
 .../cavium-octeon/executive/cvmx-helper-util.c     |  138 +-
 .../cavium-octeon/executive/cvmx-helper-xaui.c     |    4 +-
 arch/mips/cavium-octeon/executive/cvmx-helper.c    |  586 ++--
 arch/mips/cavium-octeon/executive/cvmx-ila.c       |  295 ++
 arch/mips/cavium-octeon/executive/cvmx-ilk.c       |   61 +-
 arch/mips/cavium-octeon/executive/cvmx-ipd.c       |   18 +-
 arch/mips/cavium-octeon/executive/cvmx-l2c.c       |   72 +-
 arch/mips/cavium-octeon/executive/cvmx-pcie.c      |  405 ++-
 .../cavium-octeon/executive/cvmx-pki-resources.c   |   35 +-
 arch/mips/cavium-octeon/executive/cvmx-pki.c       |  649 ++++-
 arch/mips/cavium-octeon/executive/cvmx-pko.c       |   28 +-
 .../mips/cavium-octeon/executive/cvmx-pko3-queue.c |  529 ++--
 .../cavium-octeon/executive/cvmx-pko3-resources.c  |   95 +-
 arch/mips/cavium-octeon/executive/cvmx-pko3.c      |  391 ++-
 arch/mips/cavium-octeon/executive/cvmx-qlm.c       | 1517 +++++++++-
 arch/mips/cavium-octeon/executive/cvmx-range.c     |   61 +-
 arch/mips/cavium-octeon/executive/cvmx-srio.c      |  427 ++-
 .../cavium-octeon/executive/cvmx-sso-resources.c   |    7 +-
 arch/mips/cavium-octeon/executive/cvmx-twsi.c      |    2 +-
 arch/mips/cavium-octeon/executive/cvmx-usb.c       |    4 +-
 arch/mips/cavium-octeon/executive/octeon-feature.c |    3 +-
 arch/mips/cavium-octeon/executive/octeon-model.c   |   42 +-
 44 files changed, 8934 insertions(+), 1919 deletions(-)
 create mode 100644 arch/mips/cavium-octeon/executive/cvmx-ila.c

diff --git a/arch/mips/cavium-octeon/executive/Makefile b/arch/mips/cavium-octeon/executive/Makefile
index 06bdadc..0ce1908 100644
--- a/arch/mips/cavium-octeon/executive/Makefile
+++ b/arch/mips/cavium-octeon/executive/Makefile
@@ -24,10 +24,10 @@ obj-y += cvmx-pko.o cvmx-spi.o cvmx-cmd-queue.o cvmx-helper-cfg.o	\
 	cvmx-helper-bgx.o cvmx-pko3.o cvmx-helper-pki.o			\
 	cvmx-helper-pko3.o cvmx-pko3-resources.o cvmx-helper-pko.o	\
 	cvmx-helper-ipd.o cvmx-sso-resources.o cvmx-ocla.o cvmx-boot-vector.o \
-	cvmx-lap.o cvmx-osm.o
+	cvmx-lap.o cvmx-osm.o cvmx-ila.o
 
 obj-y += cvmx-helper-errata.o cvmx-helper-jtag.o
-obj-y += cvmx-pcie.o
+obj-$(CONFIG_PCI) += cvmx-pcie.o
 
 obj-$(CONFIG_USB_OCTEON_HCD)		+= cvmx-usb.o
 obj-$(CONFIG_CAVIUM_OCTEON_RAPIDIO) 	+= cvmx-dma-engine.o
diff --git a/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c b/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c
index b8f1e32..21b155b 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-appcfg-transport.c
@@ -209,6 +209,7 @@ static void __cvmx_export_fpa_config(uint64_t fpa_config_addr)
 	/* copy wqe pool information */
 	fpa_config.pool_config.pool_num = cvmx_fpa_get_wqe_pool();
 	fpa_config.pool_config.buffer_size = cvmx_fpa_get_wqe_pool_block_size();
+	fpa_config.pool_config.buffer_count = 0;	/* not used */
 	fpa_config.pool_type = FPA_WQE_POOL;
 
 	__cvmx_copy_to_bootmem(&fpa_config, fpa_config_addr, 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-bch.c b/arch/mips/cavium-octeon/executive/cvmx-bch.c
index 06598d6..7d7fe64 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-bch.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-bch.c
@@ -90,6 +90,7 @@ CVMX_SHARED cvmx_bch_app_config_t bch_config = {
 
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 extern int cvm_oct_alloc_fpa_pool(int pool, int size);
+const unsigned bch_buf_count = 16;
 #endif
 
 /**
@@ -104,16 +105,22 @@ int cvmx_bch_initialize(void)
 	cvmx_cmd_queue_result_t result;
 	int bch_pool;
 	uint64_t bch_pool_size;
+	int i, buf_cnt;
 
 	/* Initialize FPA pool for BCH pool buffers */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
-	int i;
 	bch_pool = CVMX_FPA_OUTPUT_BUFFER_POOL;
 	bch_pool_size = CVMX_FPA_OUTPUT_BUFFER_POOL_SIZE;
+	buf_cnt = bch_buf_count;
 
-	debug("pool: %d, pool size: %llu\n", bch_pool, bch_pool_size);
+	debug("pool: %d, pool size: %llu, bufcount %d\n",
+		bch_pool, bch_pool_size, buf_cnt);
 	/* Setup the FPA */
-	cvmx_fpa1_enable();
+	if (octeon_has_feature(OCTEON_FEATURE_FPA3))
+		/* FIXME */
+		return -ENOMEM;
+	else
+		cvmx_fpa1_enable();
 
 	bch_pool = cvm_oct_alloc_fpa_pool(bch_pool, bch_pool_size);
 	if (bch_pool < 0) {
@@ -122,23 +129,28 @@ int cvmx_bch_initialize(void)
 		return -ENOMEM;
 	}
 
-	for (i = 0; i < 16; i++)
-		cvmx_fpa1_free(kmalloc(bch_pool_size, GFP_KERNEL), bch_pool, 0);
+	for (i = 0; i < buf_cnt; i++)
+		cvmx_fpa_free(kmalloc(bch_pool_size, GFP_KERNEL), bch_pool, 0);
 #else
 	bch_pool = (int)cvmx_fpa_get_bch_pool();
 	bch_pool_size = cvmx_fpa_get_bch_pool_block_size();
+	i = buf_cnt = bch_config.command_queue_pool.buffer_count;
 
 	debug("%s: pool: %d, pool size: %llu, buffer count: %llu\n", __func__,
-	      bch_pool, bch_pool_size,
-	      bch_config.command_queue_pool.buffer_count);
+	      bch_pool, bch_pool_size, buf_cnt);
 
 	cvmx_fpa_global_initialize();
 
-	if (bch_config.command_queue_pool.buffer_count != 0)
-		__cvmx_helper_initialize_fpa_pool(bch_pool, bch_pool_size,
-			bch_config.command_queue_pool.buffer_count,
-			"BCH Buffers");
-
+	if (buf_cnt != 0) {
+		i = cvmx_fpa_setup_pool(bch_pool, "BCH Buffers", NULL,
+			bch_pool_size, buf_cnt);
+		if (i < 0) {
+			cvmx_printf("ERROR: %s: failed to init pool %d\n",
+				__func__, bch_pool);
+			return -1;
+		}
+		bch_pool = i;
+	}
 #endif
 	result = cvmx_cmd_queue_initialize(CVMX_CMD_QUEUE_BCH, 0, bch_pool,
 					   bch_pool_size);
@@ -152,18 +164,46 @@ int cvmx_bch_initialize(void)
 	}
 
 	bch_cmd_buf.u64 = 0;
-	bch_cmd_buf.s.dwb = bch_pool_size / 128;
-	bch_cmd_buf.s.pool = bch_pool;
-	bch_cmd_buf.s.size = bch_pool_size / 8;
-	bch_cmd_buf.s.ptr = cvmx_ptr_to_phys(
-		cvmx_cmd_queue_buffer(CVMX_CMD_QUEUE_BCH)) >> 7;
-	cvmx_write_csr(CVMX_BCH_CMD_BUF, bch_cmd_buf.u64);
-	cvmx_write_csr(CVMX_BCH_GEN_INT, 7);
-	cvmx_write_csr(CVMX_BCH_GEN_INT_EN, 0);
+
+	if (OCTEON_IS_MODEL(OCTEON_CN70XX)) {
+		bch_cmd_buf.cn70xx.dwb = bch_pool_size / 128;
+		bch_cmd_buf.cn70xx.pool = bch_pool;
+		bch_cmd_buf.cn70xx.size = bch_pool_size / 8;
+		bch_cmd_buf.cn70xx.ptr = cvmx_ptr_to_phys(
+			cvmx_cmd_queue_buffer(CVMX_CMD_QUEUE_BCH)) >> 7;
+		cvmx_write_csr(CVMX_BCH_CMD_BUF, bch_cmd_buf.u64);
+	} else if (OCTEON_IS_MODEL(OCTEON_CN73XX) ||
+		OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+		cvmx_bch_cmd_ptr_t bch_cmd_ptr;
+
+		bch_cmd_buf.cn73xx.ldwb = bch_pool_size / 128;
+		bch_cmd_buf.cn73xx.aura = bch_pool;
+		bch_cmd_buf.cn73xx.size = bch_pool_size / 8;
+		bch_cmd_ptr.cn73xx.ptr = cvmx_ptr_to_phys(
+			cvmx_cmd_queue_buffer(CVMX_CMD_QUEUE_BCH)) >> 7;
+		cvmx_write_csr(CVMX_BCH_CMD_PTR, bch_cmd_ptr.u64);
+		cvmx_write_csr(CVMX_BCH_CMD_BUF, bch_cmd_buf.u64);
+	}
+
+	/* Clear pending error interrupts */
+	if (OCTEON_IS_MODEL(OCTEON_CN70XX)) {
+		cvmx_write_csr(CVMX_BCH_GEN_INT, 7);
+		cvmx_write_csr(CVMX_BCH_GEN_INT_EN, 0);
+	}
+
 	bch_ctl.u64 = cvmx_read_csr(CVMX_BCH_CTL);
-	bch_ctl.s.free_ena = 1;
-	bch_ctl.s.one_cmd = 0;
-	bch_ctl.s.erase_disable = 0;
+
+	/* Enable the unit */
+	if (OCTEON_IS_MODEL(OCTEON_CN70XX)) {
+		bch_ctl.cn70xx.free_ena = 1;
+		bch_ctl.cn70xx.one_cmd = 0;
+		bch_ctl.cn70xx.erase_disable = 0;
+	} else if (OCTEON_IS_MODEL(OCTEON_CN73XX) ||
+		OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+		bch_ctl.cn73xx.one_cmd = 0;
+		bch_ctl.cn73xx.erase_disable = 0;
+	}
+
 	cvmx_write_csr(CVMX_BCH_CTL, bch_ctl.u64);
 	cvmx_read_csr(CVMX_BCH_CMD_BUF);
 	return 0;
@@ -182,28 +222,30 @@ int cvmx_bch_shutdown(void)
 
 	debug("%s: ENTER\n", __func__);
 	bch_ctl.u64 = cvmx_read_csr(CVMX_BCH_CTL);
-	bch_ctl.s.reset = 1;
+
+	if (OCTEON_IS_MODEL(OCTEON_CN70XX)) {
+		bch_ctl.cn70xx.reset = 1;
+	} else if (OCTEON_IS_MODEL(OCTEON_CN73XX) ||
+		OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+		bch_ctl.cn73xx.reset = 1;
+	}
+
 	cvmx_write_csr(CVMX_BCH_CTL, bch_ctl.u64);
-	cvmx_wait(4);
+	cvmx_wait(128);
 
-#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
-	bch_pool = CVMX_FPA_OUTPUT_BUFFER_POOL;
-#else
-	bch_pool = (int)cvmx_fpa_get_bch_pool();
-#endif
 	cvmx_cmd_queue_shutdown(CVMX_CMD_QUEUE_BCH);
 
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
-	/* FIXME: BCH cleanup in SE : AJ */
+	bch_pool = CVMX_FPA_OUTPUT_BUFFER_POOL;
 	{
 		int i;
-		for (i = 0; i < 16; i++)
-			kfree(cvmx_fpa1_alloc(bch_pool));
+		for (i = 0; i < bch_buf_count; i++)
+			kfree(cvmx_fpa_alloc(bch_pool));
 	}
 #else
+	bch_pool = (int)cvmx_fpa_get_bch_pool();
 	cvmx_fpa_shutdown_pool(bch_pool);
 #endif
-	/* AJ: Fix for FPA3 */
 	return 0;
 }
 EXPORT_SYMBOL(cvmx_bch_shutdown);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-bootmem.c b/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
index 750d606..7ad99a0 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-bootmem.c
@@ -43,7 +43,7 @@
  * Simple allocate only memory allocator.  Used to allocate memory at
  * application start time.
  *
- * <hr>$Revision: 113619 $<hr>
+ * <hr>$Revision: 112705 $<hr>
  *
  */
 
@@ -463,8 +463,8 @@ static int __cvmx_validate_mem_range(uint64_t *min_addr_ptr,
 		/* Narrow range requests to be bounded by the 32 bit limits.  octeon_phy_mem_block_alloc()
 		 ** will reject inconsistent req_size/range requests, so we don't repeat those checks here.
 		 ** If max unspecified, set to 32 bit maximum. */
-		/* cvmx_dprintf("linux_mem32_min=%llx linux_mem32_max=%llx\n", (unsigned long long) linux_mem32_min,
-			     (unsigned long long) linux_mem32_max); */
+		//cvmx_dprintf("linux_mem32_min=%llx linux_mem32_max=%llx\n", (unsigned long long) linux_mem32_min,
+		//	     (unsigned long long) linux_mem32_max);
 		*min_addr_ptr = MIN(MAX(*min_addr_ptr, linux_mem32_min),
 				    linux_mem32_max);
 		if (!*max_addr_ptr)
diff --git a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
index 5089e87..6101027 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-cmd-queue.c
@@ -43,7 +43,7 @@
  * Support functions for managing command queues used for
  * various hardware blocks.
  *
- * <hr>$Revision: 114199 $<hr>
+ * <hr>$Revision: 112009 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/export.h>
diff --git a/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c b/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c
index d7a9357..0627c21 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-dma-engine.c
@@ -43,7 +43,7 @@
  * Interface to the PCI / PCIe DMA engines. These are only avialable
  * on chips with PCI / PCIe.
  *
- * <hr>$Revision: 113619 $<hr>
+ * <hr>$Revision: 112023 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/export.h>
diff --git a/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c b/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
index e20673a..d06a53c 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-fpa-resource.c
@@ -73,7 +73,7 @@ get_fpa3_pool_resource_tag(int node)
 int cvmx_fpa_get_max_pools(void)
 {
 	if (octeon_has_feature(OCTEON_FEATURE_FPA3))
-		return CVMX_FPA3_NUM_AURAS;
+		return cvmx_fpa3_num_auras();
 	else if (OCTEON_IS_MODEL(OCTEON_CN68XX))
 		/* 68xx pool 8 is not available via API */
 		return  CVMX_FPA1_NUM_POOLS;
@@ -118,7 +118,8 @@ cvmx_fpa3_reserve_aura(int node, int desired_aura_num)
 
 	tag = get_fpa3_aura_resource_tag(node);
 
-	if (cvmx_create_global_resource_range(tag, CVMX_FPA3_NUM_AURAS) != 0) {
+	if (cvmx_create_global_resource_range(tag,
+			cvmx_fpa3_num_auras()) != 0) {
 		cvmx_printf("ERROR: %s: global resource create node=%u\n",
 			__func__, node);
 		return CVMX_FPA3_INVALID_GAURA;
@@ -167,7 +168,8 @@ cvmx_fpa3_reserve_pool(int node, int desired_pool_num)
 
 	tag = get_fpa3_pool_resource_tag(node);
 
-	if (cvmx_create_global_resource_range(tag, CVMX_FPA3_NUM_POOLX) != 0) {
+	if (cvmx_create_global_resource_range(tag,
+			cvmx_fpa3_num_pools()) != 0) {
 		cvmx_printf("ERROR: %s: global resource create node=%u\n",
 			__func__, node);
 		return CVMX_FPA3_INVALID_POOL;
@@ -180,8 +182,7 @@ cvmx_fpa3_reserve_pool(int node, int desired_pool_num)
 		rv = cvmx_resource_alloc_reverse(tag, owner);
 
 	if (rv < 0) {
-		cvmx_printf("ERROR: %s: node=%u desired_pool=%d\n",
-			__func__, node, desired_pool_num);
+		/* Desired pool is already in use */
 		return CVMX_FPA3_INVALID_POOL;
 	}
 
@@ -198,7 +199,8 @@ int cvmx_fpa3_release_pool(cvmx_fpa3_pool_t pool)
 	if (!__cvmx_fpa3_pool_valid(pool))
 		return -1;
 
-	if (cvmx_create_global_resource_range(tag, CVMX_FPA3_NUM_POOLX) != 0) {
+	if (cvmx_create_global_resource_range(tag,
+			cvmx_fpa3_num_pools()) != 0) {
 		cvmx_printf("ERROR: %s: global resource create node=%u\n",
 			__func__, pool.node);
 		return -1;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-global-resources.c b/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
index 364b0b6..7892a68 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-global-resources.c
@@ -416,7 +416,7 @@ int cvmx_allocate_global_resource_range(struct global_resource_tag tag, uint64_t
 	if (addr == 0) {
 		char tagname[256];
 		__cvmx_get_tagname(&tag, tagname);
-		cvmx_printf("ERROR: %s: cannot find resource %s\n", 
+		cvmx_printf("ERROR: %s: cannot find resource %s\n",
 			__func__, tagname);
 		return -1;
 	}
@@ -602,11 +602,10 @@ void cvmx_global_resources_show(void)
 		rtag.lo = CVMX_RESOURCE_TAG_GET_FIELD(p, lo);
 		rtag.hi = CVMX_RESOURCE_TAG_GET_FIELD(p, hi);
 		__cvmx_get_tagname(&rtag, tagname);
-		if (dbg)
-			cvmx_dprintf("Global Resource tag name: %s Resource Address: %llx\n",
-				     tagname, CAST_ULL(phys_addr));
+		cvmx_dprintf("Global Resource tag name: %s Resource Address: %llx\n",
+			     tagname, CAST_ULL(phys_addr));
 	}
-
+	cvmx_dprintf("<End of Global Resources>\n");
 	__cvmx_global_resource_unlock();
 
 }
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c b/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
index da22760..4ec8e31 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-bgx.c
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2014  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2014-2015  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -54,6 +54,7 @@
 #include <asm/octeon/cvmx-helper-cfg.h>
 #include <asm/octeon/cvmx-bgxx-defs.h>
 #include <asm/octeon/cvmx-gserx-defs.h>
+#include <asm/octeon/cvmx-xcv-defs.h>
 #else
 #include "cvmx.h"
 #include "cvmx-helper.h"
@@ -63,141 +64,244 @@
 #include "cvmx-qlm.h"
 #endif
 
-int __cvmx_helper_bgx_enumerate(int xiface)
-{
-	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	int node = xi.node;
-	int qlm;
-	enum cvmx_qlm_mode mode;
+static const int debug = 0;
 
-	/*
-	 * Check the QLM is configured correctly for SGMII, verify the
-	 * speed as well as the mode.
-	 */
-	qlm = cvmx_qlm_interface(xiface);
-	if (qlm == -1)
-		return 0;
+/**
+ * Delay after enabling an interface based on the mode.  Different modes take
+ * different amounts of time.
+ */
+static void
+__cvmx_helper_bgx_interface_enable_delay(cvmx_helper_interface_mode_t mode)
+{
+	/* Don't delay if we running under the simulator. */
+	if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
+		return;
 
-	mode = cvmx_qlm_get_mode_cn78xx(node, qlm);
-	if (mode == CVMX_QLM_MODE_SGMII) {
-		return 4;
-	} else if (mode == CVMX_QLM_MODE_XAUI
-		   || mode == CVMX_QLM_MODE_XLAUI
-		   || mode == CVMX_QLM_MODE_40G_KR4) {
-		return 1;
-	} else if (mode == CVMX_QLM_MODE_RXAUI) {
-		return 2;
-	} else if (mode == CVMX_QLM_MODE_XFI
-		   || mode == CVMX_QLM_MODE_10G_KR) {
-		return 4;
-	} else
-		return 0;
+	switch (mode) {
+	case CVMX_HELPER_INTERFACE_MODE_10G_KR:
+	case CVMX_HELPER_INTERFACE_MODE_40G_KR4:
+	case CVMX_HELPER_INTERFACE_MODE_XLAUI:
+	case CVMX_HELPER_INTERFACE_MODE_XFI:
+		cvmx_wait_usec(250000);
+		break;
+	case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+	case CVMX_HELPER_INTERFACE_MODE_XAUI:
+		cvmx_wait_usec(100000);
+		break;
+	case CVMX_HELPER_INTERFACE_MODE_SGMII:
+		cvmx_wait_usec(50000);
+		break;
+	default:
+		cvmx_wait_usec(50000);
+		break;
+	}
 }
 
 /**
  * @INTERNAL
- * Disable the BGX port
  *
- * @param xipd_port IPD port of the BGX interface to disable
+ * Returns number of ports based on interface
+ * @param xiface Which xiface
+ * @return Number of ports based on xiface
  */
-void cvmx_helper_bgx_disable(int xipd_port)
+int __cvmx_helper_bgx_enumerate(int xiface)
 {
-	int xiface = cvmx_helper_get_interface_num(xipd_port);
+	cvmx_bgxx_cmr_tx_lmacs_t lmacs;
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
-	int interface = xi.interface;
-	int node = xi.node;
-	int index = cvmx_helper_get_interface_index_num(xp.port);
-	cvmx_bgxx_cmrx_config_t cmr_config;
-
-	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
-	if (!OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_0) || index)
-		cmr_config.s.enable = 0;
-	cmr_config.s.data_pkt_tx_en = 0;
-	cmr_config.s.data_pkt_rx_en = 0;
-	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
-}
 
+	lmacs.u64 = cvmx_read_csr_node(xi.node, CVMX_BGXX_CMR_TX_LMACS(xi.interface));
+	return lmacs.s.lmacs;
 
+}
 
 /**
  * @INTERNAL
- * Configure the bgx mac.
  *
- * @param xiface Interface to bring up
- * @param index  port on interface to bring up
+ * Returns mode of each port in a BDK
+ * @param xiface Which xiface
+ * @param index port in a BDK
+ * @returns mode of each port in a BDK
  */
-static void __cvmx_bgx_common_init(int xiface, int index)
+static cvmx_helper_interface_mode_t cvmx_helper_bgx_get_mode(int xiface, int index)
 {
-	cvmx_bgxx_cmrx_config_t	cmr_config;
-	cvmx_bgxx_cmr_rx_lmacs_t bgx_cmr_rx_lmacs;
-	cvmx_bgxx_cmr_tx_lmacs_t bgx_cmr_tx_lmacs;
-	cvmx_helper_interface_mode_t mode;
-	int num_ports;
-	int lmac_type = 0;
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	int interface = xi.interface;
-	int node = xi.node;
-	int lane_to_sds = 0;
+	cvmx_bgxx_cmrx_config_t cmr_config;
+	cvmx_bgxx_spux_br_pmd_control_t pmd_control;
 
-	num_ports = cvmx_helper_ports_on_interface(xiface);
-	mode = cvmx_helper_interface_get_mode(xiface);
+	cmr_config.u64 = cvmx_read_csr_node(xi.node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
 
-	switch (mode) {
-	case CVMX_HELPER_INTERFACE_MODE_SGMII:
-		lmac_type = 0;
-		lane_to_sds = 1;
+	switch (cmr_config.s.lmac_type)
+	{
+	case 0:
+		return CVMX_HELPER_INTERFACE_MODE_SGMII;
 		break;
-	case CVMX_HELPER_INTERFACE_MODE_XAUI:
-		lmac_type = 1;
-		lane_to_sds = 0xe4;
+	case 1:
+		return CVMX_HELPER_INTERFACE_MODE_XAUI;
 		break;
-	case CVMX_HELPER_INTERFACE_MODE_RXAUI:
-		lmac_type = 2;
+	case 2:
+		return CVMX_HELPER_INTERFACE_MODE_RXAUI;
 		break;
-	case CVMX_HELPER_INTERFACE_MODE_XFI:
-	case CVMX_HELPER_INTERFACE_MODE_10G_KR:
-		lmac_type = 3;
-		lane_to_sds = 1;
+	case 3:
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
+			return cvmx_helper_interface_get_mode(xiface);
+		pmd_control.u64 = cvmx_read_csr_node(xi.node,
+					CVMX_BGXX_SPUX_BR_PMD_CONTROL(index,
+							xi.interface));
+		if (pmd_control.s.train_en)
+			return CVMX_HELPER_INTERFACE_MODE_10G_KR;
+		else
+			return CVMX_HELPER_INTERFACE_MODE_XFI;
 		break;
-	case CVMX_HELPER_INTERFACE_MODE_XLAUI:
-	case CVMX_HELPER_INTERFACE_MODE_40G_KR4:
-		lmac_type = 4;
-		lane_to_sds = 0xe4;
+	case 4:
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
+			return cvmx_helper_interface_get_mode(xiface);
+		pmd_control.u64 = cvmx_read_csr_node(xi.node,
+					CVMX_BGXX_SPUX_BR_PMD_CONTROL(index,
+							xi.interface));
+		if (pmd_control.s.train_en)
+			return CVMX_HELPER_INTERFACE_MODE_40G_KR4;
+		else
+			return CVMX_HELPER_INTERFACE_MODE_XLAUI;
 		break;
 	default:
+		return CVMX_HELPER_INTERFACE_MODE_DISABLED;
 		break;
 	}
+}
 
-	/* Set mode and lanes for all interface ports */
-	cmr_config.u64 =
-		cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
-	cmr_config.s.enable = 0;
+/**
+ * @INTERNAL
+ * Disable the BGX port
+ *
+ * @param xipd_port IPD port of the BGX interface to disable
+ */
+void cvmx_helper_bgx_disable(int xipd_port)
+{
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
+	int node = xi.node;
+	int index = cvmx_helper_get_interface_index_num(xp.port);
+	cvmx_bgxx_cmrx_config_t cmr_config;
+
+	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
+	if (!OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_0) || index)
+		cmr_config.s.enable = 0;
 	cmr_config.s.data_pkt_tx_en = 0;
 	cmr_config.s.data_pkt_rx_en = 0;
-	cmr_config.s.lmac_type = lmac_type;
-	cmr_config.s.lane_to_sds = ((lane_to_sds == 1) ? index
-				: ((lane_to_sds == 0)
-				? (index ? 0xe : 4) : lane_to_sds));
-	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
-
-	if (index == 0) {
-		bgx_cmr_rx_lmacs.u64 = 0;
-		bgx_cmr_rx_lmacs.s.lmacs = num_ports;
-		cvmx_write_csr_node(node, CVMX_BGXX_CMR_RX_LMACS(interface), bgx_cmr_rx_lmacs.u64);
-
-		bgx_cmr_tx_lmacs.u64 = 0;
-		bgx_cmr_tx_lmacs.s.lmacs = num_ports;
-		cvmx_write_csr_node(node, CVMX_BGXX_CMR_TX_LMACS(interface), bgx_cmr_tx_lmacs.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
+}
+
+static int __cvmx_helper_bgx_rgmii_speed(cvmx_helper_link_info_t link_info)
+{
+	cvmx_xcv_reset_t xcv_reset;
+	cvmx_xcv_ctl_t xcv_ctl;
+	cvmx_xcv_batch_crd_ret_t crd_ret;
+	cvmx_xcv_dll_ctl_t dll_ctl;
+	cvmx_xcv_comp_ctl_t comp_ctl;
+	int speed;
+	int up = link_info.s.link_up;
+	int do_credits;
+
+	if (link_info.s.speed == 100)
+		speed = 1;
+	else if (link_info.s.speed == 10)
+		speed = 0;
+	else
+		speed = 2;
+
+	xcv_reset.u64 = cvmx_read_csr(CVMX_XCV_RESET);
+	xcv_ctl.u64 = cvmx_read_csr(CVMX_XCV_CTL);
+	do_credits = up && !xcv_reset.s.enable;
+
+	if (up && (!xcv_reset.s.enable || (xcv_ctl.s.speed != speed))) {
+		/* Enable the XCV block */
+		xcv_reset.u64 = cvmx_read_csr(CVMX_XCV_RESET);
+		xcv_reset.s.enable = 1;
+		cvmx_write_csr(CVMX_XCV_RESET, xcv_reset.u64);
+
+		/* Set operating mode */
+		xcv_ctl.u64 = cvmx_read_csr(CVMX_XCV_CTL);
+		xcv_ctl.s.speed = speed;
+		cvmx_write_csr(CVMX_XCV_CTL, xcv_ctl.u64);
+
+		/* Configure DLL - enable or bypass bypass */
+		dll_ctl.u64 = cvmx_read_csr(CVMX_XCV_DLL_CTL);
+		dll_ctl.s.clkrx_set = 0;
+		dll_ctl.s.clktx_set = 31;  /* FIXME */
+		dll_ctl.s.clkrx_byp = 1;
+		dll_ctl.s.clktx_byp = 1;
+		cvmx_write_csr(CVMX_XCV_DLL_CTL, dll_ctl.u64);
+
+		/* Enable */
+		dll_ctl.u64 = cvmx_read_csr(CVMX_XCV_DLL_CTL);
+		dll_ctl.s.refclk_sel = 0;
+		cvmx_write_csr(CVMX_XCV_DLL_CTL, dll_ctl.u64);
+		xcv_reset.u64 = cvmx_read_csr(CVMX_XCV_RESET);
+		xcv_reset.s.dllrst = 0;
+		cvmx_write_csr(CVMX_XCV_RESET, xcv_reset.u64);
+
+		comp_ctl.u64 = cvmx_read_csr(CVMX_XCV_COMP_CTL);
+		//comp_ctl.s.drv_pctl = 0;
+		//comp_ctl.s.drv_nctl = 0;
+		comp_ctl.s.drv_byp = 0;
+		cvmx_write_csr(CVMX_XCV_COMP_CTL, comp_ctl.u64);
+
+		/* enable */
+		xcv_reset.u64 = cvmx_read_csr(CVMX_XCV_RESET);
+		xcv_reset.s.comp = 1;
+		cvmx_write_csr(CVMX_XCV_RESET, xcv_reset.u64);
+
+		/* setup the RXC */
+		xcv_reset.u64 = cvmx_read_csr(CVMX_XCV_RESET);
+		xcv_reset.s.clkrst = 1;
+		cvmx_write_csr(CVMX_XCV_RESET, xcv_reset.u64);
+
+		/* datapaths come out of the reset
+		 * - the datapath resets will disengage BGX from the RGMII
+		 *   interface
+		 * - XCV will continue to return TX credits for each tick that
+		 *   is sent on the TX data path
+		 */
+		xcv_reset.u64 = cvmx_read_csr(CVMX_XCV_RESET);
+		xcv_reset.s.tx_dat_rst_n = 1;
+		xcv_reset.s.rx_dat_rst_n = 1;
+		cvmx_write_csr(CVMX_XCV_RESET, xcv_reset.u64);
 	}
+
+	/* enable the packet flow
+	 * - The packet resets will be only disengage on packet boundaries
+	 * - XCV will continue to return TX credits for each tick that is
+	 *   sent on the TX datapath
+	 */
+	xcv_reset.u64 = cvmx_read_csr(CVMX_XCV_RESET);
+	xcv_reset.s.tx_pkt_rst_n = up;
+	xcv_reset.s.rx_pkt_rst_n = up;
+	cvmx_write_csr(CVMX_XCV_RESET, xcv_reset.u64);
+
+	/* Full reset when link is down */
+	if (!up) {
+		/* wait 2*MTU in time */
+		cvmx_wait_usec(10000);
+		/* reset the world */
+		cvmx_write_csr(CVMX_XCV_RESET, 0);
+	}
+
+	/* grant PKO TX credits */
+	if (do_credits) {
+		crd_ret.u64 = cvmx_read_csr(CVMX_XCV_BATCH_CRD_RET);
+		crd_ret.s.crd_ret = 1;
+		cvmx_write_csr(CVMX_XCV_BATCH_CRD_RET, crd_ret.u64);
+	}
+
+	return link_info.s.speed;
 }
 
 static void __cvmx_bgx_common_init_pknd(int xiface, int index)
 {
 	int num_ports;
-	int num_chl = 16; /*modify it to 64 for xlaui and xaui*/
+	int num_chl = 16;
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	int interface = xi.interface;
 	int node = xi.node;
 	int pknd;
 	cvmx_bgxx_cmrx_rx_bp_on_t bgx_rx_bp_on;
@@ -205,6 +309,10 @@ static void __cvmx_bgx_common_init_pknd(int xiface, int index)
 	cvmx_bgxx_cmr_chan_msk_and_t chan_msk_and;
 	cvmx_bgxx_cmr_chan_msk_or_t chan_msk_or;
 
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
+
 	num_ports = cvmx_helper_ports_on_interface(xiface);
 	/* Modify bp_on mark, depending on number of LMACS on that interface
 	and write it for every port */
@@ -213,19 +321,52 @@ static void __cvmx_bgx_common_init_pknd(int xiface, int index)
 
 	/* Setup pkind */
 	pknd = cvmx_helper_get_pknd(xiface, index);
-	cmr_rx_id_map.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_RX_ID_MAP(index, interface));
+	cmr_rx_id_map.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_RX_ID_MAP(index, xi.interface));
 	cmr_rx_id_map.s.pknd = pknd;
-	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_ID_MAP(index, interface),
+	/* Change the default reassembly id (RID), as max 14 RIDs allowed */
+	if (OCTEON_IS_MODEL(OCTEON_CN73XX) || OCTEON_IS_MODEL(OCTEON_CNF75XX))
+		cmr_rx_id_map.s.rid = ((4 * xi.interface) + 2 + index);
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_ID_MAP(index, xi.interface),
 			    cmr_rx_id_map.u64);
 	/* Set backpressure channel mask AND/OR registers */
-	chan_msk_and.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_AND(interface));
-	chan_msk_or.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_OR(interface));
+	chan_msk_and.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_AND(xi.interface));
+	chan_msk_or.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_OR(xi.interface));
 	chan_msk_and.s.msk_and |= ((1 << num_chl) - 1) << (16 * index);
 	chan_msk_or.s.msk_or |= ((1 << num_chl) - 1) << (16 * index);
-	cvmx_write_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_AND(interface), chan_msk_and.u64);
-	cvmx_write_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_OR(interface), chan_msk_or.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_AND(xi.interface), chan_msk_and.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_CMR_CHAN_MSK_OR(xi.interface), chan_msk_or.u64);
 	/* set rx back pressure (bp_on) on value */
-	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_BP_ON(index, interface), bgx_rx_bp_on.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_BP_ON(index, xi.interface), bgx_rx_bp_on.u64);
+}
+
+static void __cvmx_helper_bgx_adjust_index(int xiface, int *start, int *end)
+{
+	int qlm = cvmx_qlm_lmac(xiface, 0);
+	int num_ports = cvmx_helper_ports_on_interface(xiface);
+
+	if (OCTEON_IS_MODEL(OCTEON_CN73XX)) {
+		if (qlm <= 3) {
+			*start = 0;
+			*end = num_ports;
+		} else if (qlm == 5 || qlm == 6) {
+			cvmx_gserx_cfg_t gser1, gser2;
+			gser1.u64 = cvmx_read_csr(CVMX_GSERX_CFG(5));
+			gser2.u64 = cvmx_read_csr(CVMX_GSERX_CFG(6));
+			if (gser1.s.bgx && gser2.s.bgx) {
+				*start = 0;
+				*end = num_ports;
+			} else if (gser1.s.bgx) {
+				*start = 0;
+				*end = 2;
+			} else if (gser2.s.bgx) {
+				*start = 2;
+				*end = num_ports;
+			}
+		}
+	} else {
+		*start = 0;
+		*end = num_ports;
+	}
 }
 
 /**
@@ -240,11 +381,6 @@ static void __cvmx_bgx_common_init_pknd(int xiface, int index)
  */
 int __cvmx_helper_bgx_probe(int xiface)
 {
-	int num_ports = cvmx_helper_ports_on_interface(xiface);
-	int index;
-
-	for (index = 0; index < num_ports; index++)
-		__cvmx_bgx_common_init(xiface, index);
 	return __cvmx_helper_bgx_enumerate(xiface);
 }
 EXPORT_SYMBOL(__cvmx_helper_bgx_probe);
@@ -261,29 +397,32 @@ EXPORT_SYMBOL(__cvmx_helper_bgx_probe);
 static int __cvmx_helper_bgx_sgmii_hardware_init_one_time(int xiface, int index)
 {
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	int interface = xi.interface;
 	int node = xi.node;
 	const uint64_t clock_mhz = cvmx_clock_get_rate_node(node, CVMX_CLOCK_SCLK) / 1000000;
 	cvmx_bgxx_gmp_pcs_miscx_ctl_t gmp_misc_ctl;
 	cvmx_bgxx_gmp_pcs_linkx_timer_t gmp_timer;
 
-	if (!cvmx_helper_is_port_valid(interface, index))
+	if (!cvmx_helper_is_port_valid(xi.interface, index))
 		return 0;
 
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
+
 	/*
 	 * Write PCS*_LINK*_TIMER_COUNT_REG[COUNT] with the
 	 * appropriate value. 1000BASE-X specifies a 10ms
 	 * interval. SGMII specifies a 1.6ms interval.
 	 */
-	gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
+	gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, xi.interface));
 	/* Adjust the MAC mode if requested by device tree */
 	gmp_misc_ctl.s.mac_phy =
 		cvmx_helper_get_mac_phy_mode(xiface, index);
 	gmp_misc_ctl.s.mode =
 		cvmx_helper_get_1000x_mode(xiface, index);
-	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface), gmp_misc_ctl.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, xi.interface), gmp_misc_ctl.u64);
 
-	gmp_timer.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_LINKX_TIMER(index, interface));
+	gmp_timer.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_LINKX_TIMER(index, xi.interface));
 	if (gmp_misc_ctl.s.mode)
 		/* 1000BASE-X */
 		gmp_timer.s.count = (10000ull * clock_mhz) >> 10;
@@ -291,7 +430,7 @@ static int __cvmx_helper_bgx_sgmii_hardware_init_one_time(int xiface, int index)
 		/* SGMII */
 		gmp_timer.s.count = (1600ull * clock_mhz) >> 10;
 
-	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_LINKX_TIMER(index, interface), gmp_timer.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_LINKX_TIMER(index, xi.interface), gmp_timer.u64);
 
 	/*
 	 * Write the advertisement register to be used as the
@@ -305,20 +444,20 @@ static int __cvmx_helper_bgx_sgmii_hardware_init_one_time(int xiface, int index)
 	if (gmp_misc_ctl.s.mode) {
 		/* 1000BASE-X */
 		cvmx_bgxx_gmp_pcs_anx_adv_t gmp_an_adv;
-		gmp_an_adv.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_ANX_ADV(index, interface));
+		gmp_an_adv.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_ANX_ADV(index, xi.interface));
 		gmp_an_adv.s.rem_flt = 0;
 		gmp_an_adv.s.pause = 3;
 		gmp_an_adv.s.hfd = 1;
 		gmp_an_adv.s.fd = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_ANX_ADV(index, interface), gmp_an_adv.u64);
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_ANX_ADV(index, xi.interface), gmp_an_adv.u64);
 	} else {
 		if (gmp_misc_ctl.s.mac_phy) {
 			/* PHY Mode */
 			cvmx_bgxx_gmp_pcs_sgmx_an_adv_t gmp_sgmx_an_adv;
-			gmp_sgmx_an_adv.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_SGMX_AN_ADV(index, interface));
+			gmp_sgmx_an_adv.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_SGMX_AN_ADV(index, xi.interface));
 			gmp_sgmx_an_adv.s.dup = 1;
 			gmp_sgmx_an_adv.s.speed = 2;
-			cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_SGMX_AN_ADV(index, interface),
+			cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_SGMX_AN_ADV(index, xi.interface),
 				       gmp_sgmx_an_adv.u64);
 		} else {
 			/* MAC Mode - Nothing to do */
@@ -342,15 +481,17 @@ static int __cvmx_helper_bgx_sgmii_hardware_init(int xiface, int num_ports)
 {
 	int index;
 	int do_link_set = 1;
+	int start, end;
 
-	for (index = 0; index < num_ports; index++) {
-		int xipd_port = cvmx_helper_get_ipd_port(xiface, index);
+	__cvmx_helper_bgx_adjust_index(xiface, &start, &end);
 
-		__cvmx_helper_bgx_port_init(xipd_port, 0);
+	for (index = start; index < end; index++) {
+		int xipd_port = cvmx_helper_get_ipd_port(xiface, index);
 
 		if (!cvmx_helper_is_port_valid(xiface, index))
 			continue;
 
+		__cvmx_helper_bgx_port_init(xipd_port, 0);
 
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 		/*
@@ -393,7 +534,7 @@ int __cvmx_helper_bgx_sgmii_enable(int xiface)
 
 /**
  * @INTERNAL
- * Initialize the SERTES link for the first time or after a loss
+ * Initialize the SERDES link for the first time or after a loss
  * of link.
  *
  * @param xiface Interface to init
@@ -413,15 +554,19 @@ static int __cvmx_helper_bgx_sgmii_hardware_init_link(int xiface, int index)
 	if (!cvmx_helper_is_port_valid(xiface, index))
 		return 0;
 
-	gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
+
+	gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, xi.interface));
 	/* Take PCS through a reset sequence */
 	if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) {
 		gmp_control.s.reset = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface),
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, xi.interface),
 		       					     gmp_control.u64);
 
 		/* Wait until GMP_PCS_MRX_CONTROL[reset] comes out of reset */
-		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface),
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, xi.interface),
 				cvmx_bgxx_gmp_pcs_mrx_control_t, reset, ==, 0, 10000)) {
 			cvmx_dprintf("SGMII%d: Timeout waiting for port %d to finish reset\n", interface, index);
 			return -1;
@@ -430,21 +575,21 @@ static int __cvmx_helper_bgx_sgmii_hardware_init_link(int xiface, int index)
 
 	/* Write GMP_PCS_MR*_CONTROL[RST_AN]=1 to ensure a fresh SGMII
 	   negotiation starts. */
-	gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
+	gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, xi.interface));
 	gmp_control.s.rst_an = 1;
 	gmp_control.s.an_en = 1;
 	gmp_control.s.pwr_dn = 0;
-	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface),
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, xi.interface),
 		       gmp_control.u64);
 
 
 	phy_mode = cvmx_helper_get_mac_phy_mode(xiface, index);
 	mode_1000x = cvmx_helper_get_1000x_mode(xiface, index);
 
-	gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
+	gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, xi.interface));
 	gmp_misc_ctl.s.mac_phy = phy_mode;
 	gmp_misc_ctl.s.mode = mode_1000x;
-	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface), gmp_misc_ctl.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, xi.interface), gmp_misc_ctl.u64);
 
 	if (phy_mode)
 		/* In PHY mode we can't query the link status so we just
@@ -456,7 +601,7 @@ static int __cvmx_helper_bgx_sgmii_hardware_init_link(int xiface, int index)
 	   ethernet link, but a link between OCTEON and PHY. */
 
 	if ((cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) &&
-	     CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_PCS_MRX_STATUS(index, interface),
+	     CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_PCS_MRX_STATUS(index, xi.interface),
 				   cvmx_bgxx_gmp_pcs_mrx_status_t, an_cpt,
 				   ==, 1, 10000)) {
 		cvmx_dprintf("SGMII%d: Port %d link timeout\n", interface, index);
@@ -468,7 +613,7 @@ static int __cvmx_helper_bgx_sgmii_hardware_init_link(int xiface, int index)
 
 /**
  * @INTERNAL
- * Configure an SGMII link to the specified speed after the SERTES
+ * Configure an SGMII link to the specified speed after the SERDES
  * link is up.
  *
  * @param xiface Interface to init
@@ -486,39 +631,42 @@ static int __cvmx_helper_bgx_sgmii_hardware_init_link_speed(int xiface,
 	cvmx_bgxx_gmp_pcs_miscx_ctl_t gmp_miscx_ctl;
 	cvmx_bgxx_gmp_gmi_prtx_cfg_t gmp_prtx_cfg;
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	int interface = xi.interface;
 	int node = xi.node;
 
 	if (!cvmx_helper_is_port_valid(xiface, index))
 		return 0;
 
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
+
 	/* Errata bgx-22429*/
 	if (!OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X) || index) {
 		/* Disable GMX before we make any changes. Remember the enable state */
-		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
 		is_enabled = cmr_config.s.enable;
 		cmr_config.s.enable = 0;
-		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
 	}
 
 	/* Wait for GMX to be idle */
-	if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, interface),
+	if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, xi.interface),
 				  cvmx_bgxx_gmp_gmi_prtx_cfg_t, rx_idle, ==, 1, 10000) ||
-	    CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, interface),
+	    CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, xi.interface),
 				  cvmx_bgxx_gmp_gmi_prtx_cfg_t, tx_idle, ==, 1, 10000)) {
 		cvmx_dprintf("SGMII%d:%d: Timeout waiting for port %d to be idle\n",
-			     node, interface, index);
+			     node, xi.interface, index);
 		return -1;
 	}
 
 	/* Read GMX CFG again to make sure the disable completed */
-	gmp_prtx_cfg.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, interface));
+	gmp_prtx_cfg.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, xi.interface));
 
 	/*
 	 * Get the misc control for PCS. We will need to set the
 	 * duplication amount.
 	 */
-	gmp_miscx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
+	gmp_miscx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, xi.interface));
 
 	/*
 	 * Use GMXENO to force the link down if the status we get says
@@ -538,54 +686,54 @@ static int __cvmx_helper_bgx_sgmii_hardware_init_link_speed(int xiface,
 		gmp_prtx_cfg.s.slottime = 0;
 		/* Setting from GMX-603 */
 		gmp_miscx_ctl.s.samp_pt = 25;
-		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SLOT(index, interface), 64);
-		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_BURST(index, interface), 0);
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SLOT(index, xi.interface), 64);
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_BURST(index, xi.interface), 0);
 		break;
 	case 100:
 		gmp_prtx_cfg.s.speed = 0;
 		gmp_prtx_cfg.s.speed_msb = 0;
 		gmp_prtx_cfg.s.slottime = 0;
 		gmp_miscx_ctl.s.samp_pt = 0x5;
-		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SLOT(index, interface), 64);
-		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_BURST(index, interface), 0);
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SLOT(index, xi.interface), 64);
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_BURST(index, xi.interface), 0);
 		break;
 	case 1000:
 		gmp_prtx_cfg.s.speed = 1;
 		gmp_prtx_cfg.s.speed_msb = 0;
 		gmp_prtx_cfg.s.slottime = 1;
 		gmp_miscx_ctl.s.samp_pt = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SLOT(index, interface), 512);
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SLOT(index, xi.interface), 512);
 		if (gmp_prtx_cfg.s.duplex)
 			/* full duplex */
-			cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_BURST(index, interface), 0);
+			cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_BURST(index, xi.interface), 0);
 		else
 			/* half duplex */
-			cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_BURST(index, interface), 8192);
+			cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_BURST(index, xi.interface), 8192);
 		break;
 	default:
 		break;
 	}
 
 	/* Write the new misc control for PCS */
-	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface),
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, xi.interface),
 		       gmp_miscx_ctl.u64);
 
 	/* Write the new GMX settings with the port still disabled */
-	cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, interface), gmp_prtx_cfg.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, xi.interface), gmp_prtx_cfg.u64);
 
 	/* Read GMX CFG again to make sure the config completed */
-	cvmx_read_csr_node(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, interface));
+	cvmx_read_csr_node(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, xi.interface));
 
 	/* Restore the enabled/disabled state */
 	/* bgx-22429 */
-	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
 	if (!OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X) || index)
 		cmr_config.s.enable = is_enabled;
 #ifndef CVMX_BUILD_FOR_UBOOT
 	cmr_config.s.data_pkt_tx_en = 1;
 	cmr_config.s.data_pkt_rx_en = 1;
 #endif
-	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
 
 	return 0;
 }
@@ -610,41 +758,54 @@ cvmx_helper_link_info_t __cvmx_helper_bgx_sgmii_link_get(int xipd_port)
 	int xiface = cvmx_helper_get_interface_num(xipd_port);
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
-	int interface = xi.interface;
 	int node = xi.node;
 	int index = cvmx_helper_get_interface_index_num(xp.port);
-	int speed = 1000;
 
 	result.u64 = 0;
 
 	if (!cvmx_helper_is_port_valid(xiface, index))
 		return result;
 
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
+
 	if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM) {
 		/* The simulator gives you a simulated 1Gbps full duplex link */
 		result.s.link_up = 1;
 		result.s.full_duplex = 1;
-		result.s.speed = speed;
+		result.s.speed = 1000;
 		return result;
 	}
 
-	speed = cvmx_qlm_get_gbaud_mhz(0) * 8 / 10;
-
-	gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
+	gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, xi.interface));
 	if (gmp_control.s.loopbck1) {
+		int qlm = cvmx_qlm_lmac(xiface, index);
+		int speed;
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+			speed = cvmx_qlm_get_gbaud_mhz_node(node, qlm);
+		else
+			speed = cvmx_qlm_get_gbaud_mhz(qlm);
 		/* Force 1Gbps full duplex link for internal loopback */
 		result.s.link_up = 1;
 		result.s.full_duplex = 1;
-		result.s.speed = speed;
+		result.s.speed = speed * 8 / 10;
 		return result;
 	}
 
-	gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
-	if (gmp_misc_ctl.s.mac_phy) {
+	gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, xi.interface));
+	if (gmp_misc_ctl.s.mac_phy ||
+	    cvmx_helper_get_port_force_link_up(xiface, index)) {
+		int qlm = cvmx_qlm_lmac(xiface, index);
+		int speed;
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+			speed = cvmx_qlm_get_gbaud_mhz_node(node, qlm);
+		else
+			speed = cvmx_qlm_get_gbaud_mhz(qlm);
 		/* PHY Mode */
 		/* Note that this also works for 1000base-X mode */
 
-		result.s.speed = speed;
+		result.s.speed = speed * 8 / 10;
 		result.s.full_duplex = 1;
 		result.s.link_up = 1;
 		return result;
@@ -672,7 +833,7 @@ int cvmx_helper_bgx_errata_22429(int xipd_port, int link_up)
 
 	/* Errata BGX-22429 */
 	if (link_up) {
-		if (!index) /* does not apply for port 0 */
+		if (!index) /*does not apply for port 0*/
 			return 0;
 		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(0, interface));
 		if (!cmr_config.s.enable) {
@@ -712,47 +873,59 @@ int __cvmx_helper_bgx_sgmii_link_set(int xipd_port,
 	int xiface = cvmx_helper_get_interface_num(xipd_port);
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
-	int interface = xi.interface;
 	int node = xi.node;
 	int index = cvmx_helper_get_interface_index_num(xp.port);
+	int rc = 0;
 
 	if (!cvmx_helper_is_port_valid(xiface, index))
 		return 0;
 
-	cvmx_helper_bgx_errata_22429(xipd_port, link_info.s.link_up);
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
 
-	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
+		cvmx_helper_bgx_errata_22429(xipd_port, link_info.s.link_up);
+	}
+
+	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
 	if (link_info.s.link_up) {
 		cmr_config.s.enable = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
 		__cvmx_helper_bgx_sgmii_hardware_init_link(xiface, index);
 	} else {
 		cvmx_bgxx_gmp_pcs_miscx_ctl_t gmp_misc_ctl;
 		cmr_config.s.data_pkt_tx_en = 0;
 		cmr_config.s.data_pkt_rx_en = 0;
-		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
-		gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
+		gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, xi.interface));
 
 		/* Disable autonegotiation only when MAC mode. */
 		if (gmp_misc_ctl.s.mac_phy == 0) {
 			cvmx_bgxx_gmp_pcs_mrx_control_t gmp_control;
 
-			gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
+			gmp_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, xi.interface));
 			gmp_control.s.an_en = 0;
-			cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface), gmp_control.u64);
-			cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
+			cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, xi.interface), gmp_control.u64);
+			cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, xi.interface));
 		}
 		/*
 		 * Use GMXENO to force the link down it will get
 		 * reenabled later...
 		 */
 		gmp_misc_ctl.s.gmxeno = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface),
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, xi.interface),
 			       gmp_misc_ctl.u64);
-		cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
+		cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, xi.interface));
+		if (index == 0 && cmr_config.s.lmac_type == 5)
+			__cvmx_helper_bgx_rgmii_speed(link_info);
 		return 0;
 	}
-	return __cvmx_helper_bgx_sgmii_hardware_init_link_speed(xiface, index, link_info);
+	rc = __cvmx_helper_bgx_sgmii_hardware_init_link_speed(xiface, index, link_info);
+	if (index == 0 && cmr_config.s.lmac_type == 5)
+		rc = __cvmx_helper_bgx_rgmii_speed(link_info);
+
+	return rc;
 }
 
 
@@ -769,6 +942,7 @@ int __cvmx_helper_bgx_sgmii_link_set(int xipd_port,
 static int __cvmx_helper_bgx_xaui_init(int index, int xiface)
 {
 	cvmx_bgxx_cmrx_config_t cmr_config;
+	cvmx_bgxx_spux_br_pmd_control_t pmd_control;
 	cvmx_bgxx_spux_misc_control_t spu_misc_control;
 	cvmx_bgxx_spux_control1_t spu_control1;
 	cvmx_bgxx_spux_an_control_t spu_an_control;
@@ -784,12 +958,18 @@ static int __cvmx_helper_bgx_xaui_init(int index, int xiface)
 	int use_auto_neg = 0;
 	int xipd_port = cvmx_helper_get_ipd_port(xiface, index);
 
-	mode = cvmx_helper_interface_get_mode(xiface);
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
+
+	mode = cvmx_helper_bgx_get_mode(xiface, index);
 
 	if (mode == CVMX_HELPER_INTERFACE_MODE_10G_KR
-	    || mode == CVMX_HELPER_INTERFACE_MODE_40G_KR4) {
+	    || mode == CVMX_HELPER_INTERFACE_MODE_40G_KR4)
 		use_auto_neg = 1;
-	}
+
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
+		use_auto_neg = 0;
 
 	/* NOTE: This code was moved first, out of order compared to the HRM
 	   because the RESET causes all SPU registers to loose their value */
@@ -800,12 +980,12 @@ static int __cvmx_helper_bgx_xaui_init(int index, int xiface)
 	   BGX(0..5)_SPU(0..3)_MISC_CONTROL[RX_PACKET_DIS] = 1 to disable
 	   reception. */
 	if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) {
-		spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface));
+		spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, xi.interface));
 		spu_control1.s.reset = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface), spu_control1.u64);
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, xi.interface), spu_control1.u64);
 
 		/* 1. Wait for PCS to come out of reset */
-		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_CONTROL1(index, interface),
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_CONTROL1(index, xi.interface),
 				cvmx_bgxx_spux_control1_t, reset, ==, 0, 10000)) {
 			cvmx_dprintf("BGX%d:%d: SPU stuck in reset\n", node, interface);
 			return -1;
@@ -815,29 +995,29 @@ static int __cvmx_helper_bgx_xaui_init(int index, int xiface)
 		      BGX(0..5)_SPU(0..3)_CONTROL1[LO_PWR] = 1 and
 		      BGX(0..5)_SPU(0..3)_MISC_CONTROL[RX_PACKET_DIS] = 1. */
 		if (!cvmx_helper_bgx_errata_22429(xipd_port, 0)) {
-			cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+			cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
 			cmr_config.s.enable = 0;
-			cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+			cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
 		}
-		spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface));
+		spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, xi.interface));
 		spu_control1.s.lo_pwr = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface), spu_control1.u64);
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, xi.interface), spu_control1.u64);
 
-		spu_misc_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface));
+		spu_misc_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, xi.interface));
 		spu_misc_control.s.rx_packet_dis = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface), spu_misc_control.u64);
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, xi.interface), spu_misc_control.u64);
 
 		/* 3. At this point, it may be appropriate to disable all BGX and SMU/SPU
 		    interrupts, as a number of them will occur during bring-up of the Link.
 		    - zero BGX(0..5)_SMU(0..3)_RX_INT
 		    - zero BGX(0..5)_SMU(0..3)_TX_INT
 		    - zero BGX(0..5)_SPU(0..3)_INT */
-		cvmx_write_csr_node(node, CVMX_BGXX_SMUX_RX_INT(index, interface),
-			cvmx_read_csr_node(node, CVMX_BGXX_SMUX_RX_INT(index, interface)));
-		cvmx_write_csr_node(node, CVMX_BGXX_SMUX_TX_INT(index, interface),
-			cvmx_read_csr_node(node, CVMX_BGXX_SMUX_TX_INT(index, interface)));
-		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_INT(index, interface),
-			cvmx_read_csr_node(node, CVMX_BGXX_SPUX_INT(index, interface)));
+		cvmx_write_csr_node(node, CVMX_BGXX_SMUX_RX_INT(index, xi.interface),
+			cvmx_read_csr_node(node, CVMX_BGXX_SMUX_RX_INT(index, xi.interface)));
+		cvmx_write_csr_node(node, CVMX_BGXX_SMUX_TX_INT(index, xi.interface),
+			cvmx_read_csr_node(node, CVMX_BGXX_SMUX_TX_INT(index, xi.interface)));
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_INT(index, xi.interface),
+			cvmx_read_csr_node(node, CVMX_BGXX_SPUX_INT(index, xi.interface)));
 
 		/* 4. Configure the BGX LMAC. */
 		/* 4a. Configure the LMAC type (40GBASE-R/10GBASE-R/RXAUI/XAUI) and
@@ -847,67 +1027,66 @@ static int __cvmx_helper_bgx_xaui_init(int index, int xiface)
 
 		/* 4b. Write BGX(0..5)_SPU(0..3)_CONTROL1[LO_PWR] = 1 and
 		     BGX(0..5)_SPU(0..3)_MISC_CONTROL[RX_PACKET_DIS] = 1. */
-		spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface));
-		spu_control1.s.lo_pwr = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface), spu_control1.u64);
-
-		spu_misc_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface));
-		spu_misc_control.s.rx_packet_dis = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface), spu_misc_control.u64);
-
 		/* 4b. Initialize the selected SerDes lane(s) in the QLM. See Section
 		      28.1.2.2 in the GSER chapter. */
 		/* Already done in QLM setup */
 
 		/* 4c. For 10GBASE-KR or 40GBASE-KR, enable link training by writing
 		     BGX(0..5)_SPU(0..3)_BR_PMD_CONTROL[TRAIN_EN] = 1. */
-		/* Moved to xaui_link */
+		if (use_auto_neg) {
+			cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_LP_CUP(index, interface), 0);
+			cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_LD_CUP(index, interface), 0);
+			cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_LD_REP(index, interface), 0);
+			pmd_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, interface));
+			pmd_control.s.train_en = 1;
+			cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, interface), pmd_control.u64);
+		}
 	} else { /* enable for simulator */
-		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
 		cmr_config.s.enable = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
 	}
 
 	/* 4d. Program all other relevant BGX configuration while
 	       BGX(0..5)_CMR(0..3)_CONFIG[ENABLE] = 0. This includes all things
 	       described in this chapter. */
 	/* Always add FCS to PAUSE frames */
-	smu_tx_append.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_TX_APPEND(index, interface));
+	smu_tx_append.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_TX_APPEND(index, xi.interface));
 	smu_tx_append.s.fcs_d = 1;
-	cvmx_write_csr_node(node, CVMX_BGXX_SMUX_TX_APPEND(index, interface), smu_tx_append.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_SMUX_TX_APPEND(index, xi.interface), smu_tx_append.u64);
 
 	/* 4e. If Forward Error Correction is desired for 10GBASE-R or 40GBASE-R,
 	       enable it by writing BGX(0..5)_SPU(0..3)_FEC_CONTROL[FEC_EN] = 1. */
 	if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) {
 		/* FEC is optional for 10GBASE-KR, 40GBASE-KR4, and XLAUI. We're going
 		to disable it by default */
-		spu_fec_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, interface));
+		spu_fec_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_FEC_CONTROL(index, xi.interface));
 		spu_fec_control.s.fec_en = 0;
-		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, interface), spu_fec_control.u64);
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_FEC_CONTROL(index, xi.interface), spu_fec_control.u64);
 
 		/* 4f. If Auto-Negotiation is desired, configure and enable
 		      Auto-Negotiation as described in Section 33.6.2. */
-		spu_an_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, interface));
+		spu_an_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, xi.interface));
 		spu_an_control.s.an_en = use_auto_neg;
-		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, interface), spu_an_control.u64);
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, xi.interface), spu_an_control.u64);
 
-		spu_fec_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_FEC_CONTROL(index, interface));
-		spu_an_adv.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_AN_ADV(index, interface));
+		spu_fec_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_FEC_CONTROL(index, xi.interface));
+		spu_an_adv.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_AN_ADV(index, xi.interface));
 		spu_an_adv.s.fec_req = spu_fec_control.s.fec_en;
 		spu_an_adv.s.fec_able = 1;
 		spu_an_adv.s.a100g_cr10 = 0;
 		spu_an_adv.s.a40g_cr4 = 0;
-		spu_an_adv.s.a40g_kr4 = (mode == CVMX_HELPER_INTERFACE_MODE_40G_KR4) ;
-		spu_an_adv.s.a10g_kr = (mode == CVMX_HELPER_INTERFACE_MODE_10G_KR) ;
+		spu_an_adv.s.a40g_kr4 = (mode == CVMX_HELPER_INTERFACE_MODE_40G_KR4 && use_auto_neg);
+		spu_an_adv.s.a10g_kr = (mode == CVMX_HELPER_INTERFACE_MODE_10G_KR && use_auto_neg);
 		spu_an_adv.s.a10g_kx4 = 0;
 		spu_an_adv.s.a1g_kx = 0;
 		spu_an_adv.s.rf = 0;
-		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_AN_ADV(index, interface), spu_an_adv.u64);
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_AN_ADV(index, xi.interface), spu_an_adv.u64);
 
 		/* 3. Set BGX(0..5)_SPU_DBG_CONTROL[AN_ARB_LINK_CHK_EN] = 1. */
-		spu_dbg_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPU_DBG_CONTROL(interface));
-		spu_dbg_control.s.an_arb_link_chk_en = use_auto_neg;
-		cvmx_write_csr_node(node, CVMX_BGXX_SPU_DBG_CONTROL(interface), spu_dbg_control.u64);
+		spu_dbg_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPU_DBG_CONTROL(xi.interface));
+		spu_dbg_control.s.an_arb_link_chk_en |= use_auto_neg;
+		cvmx_write_csr_node(node, CVMX_BGXX_SPU_DBG_CONTROL(xi.interface), spu_dbg_control.u64);
 
 		/* 4. Execute the link bring-up sequence in Section 33.6.3. */
 
@@ -918,13 +1097,13 @@ static int __cvmx_helper_bgx_xaui_init(int index, int xiface)
 		/* 3h. Set BGX(0..5)_CMR(0..3)_CONFIG[ENABLE] = 1 and
 		    BGX(0..5)_SPU(0..3)_CONTROL1[LO_PWR] = 0 to enable the LMAC. */
 		cvmx_helper_bgx_errata_22429(xipd_port, 1);
-		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
 		cmr_config.s.enable = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+		cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
 
-		spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface));
+		spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, xi.interface));
 		spu_control1.s.lo_pwr = 0;
-		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface), spu_control1.u64);
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, xi.interface), spu_control1.u64);
 	}
 
 	/* 4g. Set the polarity and lane swapping of the QLM SerDes. Refer to
@@ -932,30 +1111,32 @@ static int __cvmx_helper_bgx_xaui_init(int index, int xiface)
 	   and BGX(0..5)_SPU(0..3)_MISC_CONTROL[TXPLRT,RXPLRT]. */
 
 	/* 4c. Write BGX(0..5)_SPU(0..3)_CONTROL1[LO_PWR] = 0. */
-	spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface));
+	spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, xi.interface));
 	spu_control1.s.lo_pwr = 0;
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface), spu_control1.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, xi.interface), spu_control1.u64);
 
 	/* 4d. Select Deficit Idle Count mode and unidirectional enable/disable
 	   via BGX(0..5)_SMU(0..3)_TX_CTL[DIC_EN,UNI_EN]. */
-	smu_tx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_TX_CTL(index, interface));
+	smu_tx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_TX_CTL(index, xi.interface));
 	smu_tx_ctl.s.dic_en = 1;
 	smu_tx_ctl.s.uni_en = 0;
-	cvmx_write_csr_node(node, CVMX_BGXX_SMUX_TX_CTL(index, interface), smu_tx_ctl.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_SMUX_TX_CTL(index, xi.interface), smu_tx_ctl.u64);
 
 	{
 		/* Calculate the number of s-clk cycles per usec. */
 		const uint64_t clock_mhz = cvmx_clock_get_rate_node(node, CVMX_CLOCK_SCLK) / 1000000;
 		cvmx_bgxx_spu_dbg_control_t dbg_control;
-		dbg_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPU_DBG_CONTROL(interface));
+		dbg_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPU_DBG_CONTROL(xi.interface));
 		dbg_control.s.us_clk_period = clock_mhz - 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_SPU_DBG_CONTROL(interface), dbg_control.u64);
+		cvmx_write_csr_node(node, CVMX_BGXX_SPU_DBG_CONTROL(xi.interface), dbg_control.u64);
 	}
-
+	/* The PHY often takes at least 100ms to stabilize */
+	__cvmx_helper_bgx_interface_enable_delay(mode);
 	return 0;
 }
 
-static void __cvmx_bgx_start_training(int node, int unit, int lmac)
+#if 0
+static void __cvmx_bgx_start_training(int node, int unit, int index)
 {
 	cvmx_bgxx_spux_int_t spu_int;
 	cvmx_bgxx_spux_br_pmd_control_t pmd_control;
@@ -965,32 +1146,33 @@ static void __cvmx_bgx_start_training(int node, int unit, int lmac)
 	spu_int.u64 = 0;
 	spu_int.s.training_failure = 1;
 	spu_int.s.training_done = 1;
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_INT(lmac, unit), spu_int.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_INT(index, unit), spu_int.u64);
 
 	/* These registers aren't cleared when training is restarted. Manually
-	   clear them. */
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_LP_CUP(lmac, unit), 0);
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_LD_CUP(lmac, unit), 0);
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_LD_REP(lmac, unit), 0);
+	   clear them as per Errata BGX-20968. */
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_LP_CUP(index, unit), 0);
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_LD_CUP(index, unit), 0);
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_LD_REP(index, unit), 0);
 
 	/* Disable autonegotiation */
-	an_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(lmac, unit));
+	an_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, unit));
 	an_control.s.an_en = 0;
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(lmac, unit), an_control.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, unit), an_control.u64);
 	cvmx_wait_usec(1);
 
 	/* Restart training */
-	pmd_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(lmac, unit));
+	pmd_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, unit));
 	pmd_control.s.train_en = 1;
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(lmac, unit), pmd_control.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, unit), pmd_control.u64);
 
 	cvmx_wait_usec(1);
-	pmd_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(lmac, unit));
+	pmd_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, unit));
 	pmd_control.s.train_restart = 1;
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(lmac, unit), pmd_control.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, unit), pmd_control.u64);
 }
+#endif
 
-static void __cvmx_bgx_restart_training(int node, int unit, int lmac)
+static void __cvmx_bgx_restart_training(int node, int unit, int index)
 {
 	cvmx_bgxx_spux_int_t spu_int;
 	cvmx_bgxx_spux_br_pmd_control_t pmd_control;
@@ -999,20 +1181,20 @@ static void __cvmx_bgx_restart_training(int node, int unit, int lmac)
 	spu_int.u64 = 0;
 	spu_int.s.training_failure = 1;
 	spu_int.s.training_done = 1;
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_INT(lmac, unit), spu_int.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_INT(index, unit), spu_int.u64);
 
 	cvmx_wait_usec(1700);  /* Wait 1.7 msec */
 
 	/* These registers aren't cleared when training is restarted. Manually
-	   clear them. */
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_LP_CUP(lmac, unit), 0);
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_LD_CUP(lmac, unit), 0);
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_LD_REP(lmac, unit), 0);
+	   clear them as per Errata BGX-20968. */
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_LP_CUP(index, unit), 0);
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_LD_CUP(index, unit), 0);
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_LD_REP(index, unit), 0);
 
 	/* Restart training */
-	pmd_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(lmac, unit));
+	pmd_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, unit));
 	pmd_control.s.train_restart = 1;
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(lmac, unit), pmd_control.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, unit), pmd_control.u64);
 }
 
 /*
@@ -1034,9 +1216,11 @@ int __cvmx_helper_bgx_port_init(int xipd_port, int phy_pres)
 	int index = cvmx_helper_get_interface_index_num(xp.port);
 	cvmx_helper_interface_mode_t mode;
 
-	mode = cvmx_helper_interface_get_mode(xiface);
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
 
-	__cvmx_bgx_common_init(xiface, index);
+	mode = cvmx_helper_bgx_get_mode(xiface, index);
 
 	__cvmx_bgx_common_init_pknd(xiface, index);
 
@@ -1048,14 +1232,14 @@ int __cvmx_helper_bgx_port_init(int xipd_port, int phy_pres)
 		/* Set TX Threshold */
 		gmi_tx_thresh.u64 = 0;
 		gmi_tx_thresh.s.cnt = 0x20;
-		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_THRESH(index, interface),
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_THRESH(index, xi.interface),
 				    gmi_tx_thresh.u64);
 		__cvmx_helper_bgx_sgmii_hardware_init_one_time(xiface, index);
 		gmp_txx_append.u64 = cvmx_read_csr_node(node,
-					CVMX_BGXX_GMP_GMI_TXX_APPEND(index, interface));
-		gmp_sgmii_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SGMII_CTL(index, interface));
+					CVMX_BGXX_GMP_GMI_TXX_APPEND(index, xi.interface));
+		gmp_sgmii_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SGMII_CTL(index, xi.interface));
 		gmp_sgmii_ctl.s.align = gmp_txx_append.s.preamble ? 0 : 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SGMII_CTL(index, interface),
+		cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_TXX_SGMII_CTL(index, xi.interface),
 			    gmp_sgmii_ctl.u64);
 
 	} else {
@@ -1073,16 +1257,16 @@ int __cvmx_helper_bgx_port_init(int xipd_port, int phy_pres)
 		* big to adversly effect shaping.
 		*/
 		smu_tx_thresh.s.cnt = 0x100;
-		cvmx_write_csr_node(node, CVMX_BGXX_SMUX_TX_THRESH(index, interface),
+		cvmx_write_csr_node(node, CVMX_BGXX_SMUX_TX_THRESH(index, xi.interface),
 				    smu_tx_thresh.u64);
 		/* Set disparity for RXAUI interface as described in the
 		Marvell RXAUI Interface specification. */
 		if (mode == CVMX_HELPER_INTERFACE_MODE_RXAUI && phy_pres) {
 			cvmx_bgxx_spux_misc_control_t misc_control;
 			misc_control.u64 = cvmx_read_csr_node(node,
-					CVMX_BGXX_SPUX_MISC_CONTROL(index, interface));
+					CVMX_BGXX_SPUX_MISC_CONTROL(index, xi.interface));
 			misc_control.s.intlv_rdisp = 1;
-			cvmx_write_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface),
+			cvmx_write_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, xi.interface),
 					    misc_control.u64);
 		}
 	}
@@ -1112,7 +1296,6 @@ int __cvmx_helper_bgx_sgmii_configure_loopback(int xipd_port, int enable_interna
 	int xiface = cvmx_helper_get_interface_num(xipd_port);
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
-	int interface = xi.interface;
 	int node = xi.node;
 	int index = cvmx_helper_get_interface_index_num(xp.port);
 	cvmx_bgxx_gmp_pcs_mrx_control_t gmp_mrx_control;
@@ -1121,13 +1304,17 @@ int __cvmx_helper_bgx_sgmii_configure_loopback(int xipd_port, int enable_interna
 	if (!cvmx_helper_is_port_valid(xiface, index))
 		return 0;
 
-	gmp_mrx_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface));
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
+
+	gmp_mrx_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, xi.interface));
 	gmp_mrx_control.s.loopbck1 = enable_internal;
-	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, interface), gmp_mrx_control.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MRX_CONTROL(index, xi.interface), gmp_mrx_control.u64);
 
-	gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface));
+	gmp_misc_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, xi.interface));
 	gmp_misc_ctl.s.loopbck2 = enable_external;
-	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, interface), gmp_misc_ctl.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_PCS_MISCX_CTL(index, xi.interface), gmp_misc_ctl.u64);
 
 	__cvmx_helper_bgx_sgmii_hardware_init_link(xiface, index);
 
@@ -1146,34 +1333,37 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 	cvmx_bgxx_cmrx_config_t cmr_config;
 	cvmx_helper_interface_mode_t mode;
 	int use_training = 0;
+	int rgmii_first = 0;
+
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
 
-	mode = cvmx_helper_interface_get_mode(xiface);
+	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(0, xi.interface));
+	rgmii_first = (cmr_config.s.lmac_type == 5);
 
-	if (mode == CVMX_HELPER_INTERFACE_MODE_10G_KR || mode == CVMX_HELPER_INTERFACE_MODE_40G_KR4)
+	mode = cvmx_helper_bgx_get_mode(xiface, index);
+	if (mode == CVMX_HELPER_INTERFACE_MODE_10G_KR
+	    || mode == CVMX_HELPER_INTERFACE_MODE_40G_KR4)
 		use_training = 1;
 
 	/* Disable packet reception */
-	spu_misc_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface));
+	spu_misc_control.u64 = cvmx_read_csr_node(node,
+						  CVMX_BGXX_SPUX_MISC_CONTROL(index, xi.interface));
 	spu_misc_control.s.rx_packet_dis = 1;
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface), spu_misc_control.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, xi.interface),
+			    spu_misc_control.u64);
 
 	if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) {
 		cvmx_bgxx_spux_an_control_t spu_an_control;
-		cvmx_bgxx_spux_br_pmd_control_t pmd_control;
+		cvmx_bgxx_spux_an_status_t spu_an_status;
 
 		spu_an_control.u64 = cvmx_read_csr_node(node,
 					CVMX_BGXX_SPUX_AN_CONTROL(index, interface));
 		if (spu_an_control.s.an_en) {
-			spu_int.u64 = cvmx_read_csr_node(node,
-						CVMX_BGXX_SPUX_INT(index, interface));
-			if (!spu_int.s.an_link_good) {
-				/* Clear the auto negotiation (W1C) */
-				spu_int.u64 = 0;
-				spu_int.s.an_complete = 1;
-				spu_int.s.an_link_good = 1;
-				spu_int.s.an_page_rx = 1;
-				cvmx_write_csr_node(node, CVMX_BGXX_SPUX_INT(index, interface), spu_int.u64);
-
+			spu_an_status.u64 = cvmx_read_csr_node(node,
+						CVMX_BGXX_SPUX_AN_STATUS(index, interface));
+			if (!spu_an_status.s.an_complete) {
 				/* Restart auto negotiation */
 				spu_an_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_AN_CONTROL(index, interface));
 				spu_an_control.s.an_restart = 1;
@@ -1183,45 +1373,76 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 		}
 
 		if (use_training) {
-			pmd_control.u64 = cvmx_read_csr_node(node,
-						CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, interface));
-
-			if (pmd_control.s.train_en == 0) {
-				__cvmx_bgx_start_training(node, interface, index);
-				return -1;
-			} else {
-				spu_int.u64 = cvmx_read_csr_node(node,
+			spu_int.u64 = cvmx_read_csr_node(node,
 						  CVMX_BGXX_SPUX_INT(index, interface));
-				if (spu_int.s.training_failure) {
-					__cvmx_bgx_restart_training(node, interface, index);
-					return -1;
+			if (spu_int.s.training_failure) {
+				__cvmx_bgx_restart_training(node, interface, index);
+				return -1;
+			}
+			if (!spu_int.s.training_done) {
+				cvmx_dprintf("Waiting for link training\n");
+				return -1;
+			}
+		}
+
+		/* (GSER-21957) GSER RX Equalization may make >= 5gbaud non-KR
+		   channel with DXAUI, RXAUI, XFI and XLAUI, we need to perform
+		   RX equalization when the link is receiving data the first time */
+		if (use_training == 0) {
+			int qlm = cvmx_qlm_lmac(xiface, index);
+			int lane = index;
+			cmr_config.u64 = cvmx_read_csr_node(node,
+					CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
+			if (mode == CVMX_HELPER_INTERFACE_MODE_XLAUI
+			    || mode == CVMX_HELPER_INTERFACE_MODE_XAUI) { // XLAUI/DXAUI
+				lane = -1;
+				__cvmx_qlm_rx_equalization(node, qlm, lane);
+				/* If BGX2 uses both dlms, then configure other dlm also. */
+				if (OCTEON_IS_MODEL(OCTEON_CN73XX) && xi.interface == 2)
+					__cvmx_qlm_rx_equalization(node, 6, lane);
+			} else if (CVMX_HELPER_INTERFACE_MODE_RXAUI) { // RXAUI
+				lane = index * 2;
+				if (OCTEON_IS_MODEL(OCTEON_CN73XX)
+				    && index >= 2
+				    && xi.interface == 2) {
+					lane = 0;
 				}
-				if (!spu_int.s.training_done) {
-					cvmx_dprintf("Waiting for link training\n");
-					return -1;
+				if (rgmii_first)
+					lane--;
+				__cvmx_qlm_rx_equalization(node, qlm, lane);
+				__cvmx_qlm_rx_equalization(node, qlm, lane + 1);
+			} else if (cmr_config.s.lmac_type != 5) { // !RGMII
+				if (rgmii_first)
+					lane--;
+				if (OCTEON_IS_MODEL(OCTEON_CN73XX)
+				    && index >= 2
+				    && xi.interface == 2) {
+					lane = 0;
 				}
+				__cvmx_qlm_rx_equalization(node, qlm, lane);
 			}
 		}
 
-		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_CONTROL1(index, interface),
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_CONTROL1(index, xi.interface),
 					  cvmx_bgxx_spux_control1_t, reset, ==, 0, 10000)) {
 			cvmx_dprintf("ERROR: %d:BGX%d:%d: PCS in reset", node, interface, index);
 			return -1;
 		}
 
+
 		if (mode == CVMX_HELPER_INTERFACE_MODE_XFI
 		    || mode == CVMX_HELPER_INTERFACE_MODE_XLAUI
 		    || mode == CVMX_HELPER_INTERFACE_MODE_10G_KR
 		    || mode == CVMX_HELPER_INTERFACE_MODE_40G_KR4) {
-			if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_BR_STATUS1(index, interface),
+			if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_BR_STATUS1(index, xi.interface),
 					  cvmx_bgxx_spux_br_status1_t, blk_lock, ==, 1, 10000)) {
-				cvmx_dprintf("ERROR: %d:BGX%d:%d: BASE-R PCS block not locked\n", node, interface, index);
+				//cvmx_dprintf("ERROR: %d:BGX%d:%d: BASE-R PCS block not locked\n", node, interface, index);
                 		return -1;
 			}
 		} else {
 			/* (5) Check to make sure that the link appears up and stable. */
 			/* Wait for PCS to be aligned */
-			if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_BX_STATUS(index, interface),
+			if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_BX_STATUS(index, xi.interface),
 				  cvmx_bgxx_spux_bx_status_t, alignd, ==, 1, 10000)) {
 				cvmx_dprintf("ERROR: %d:BGX%d:%d: PCS not aligned\n", node, interface, index);
 				return -1;
@@ -1229,55 +1450,50 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 		}
 
 		/* Clear rcvflt bit (latching high) and read it back */
-		spu_status2.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS2(index, interface));
+		spu_status2.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS2(index, xi.interface));
 		spu_status2.s.rcvflt = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_STATUS2(index, interface), spu_status2.u64);
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_STATUS2(index, xi.interface), spu_status2.u64);
 
-		spu_status2.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS2(index, interface));
+		spu_status2.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS2(index, xi.interface));
 		if (spu_status2.s.rcvflt) {
 			cvmx_dprintf("ERROR: %d:BGX%d:%d: Receive fault, need to retry\n",
 					node, interface, index);
-
-			if (use_training)
-				__cvmx_bgx_restart_training(node, interface, index);
-			/*cvmx_dprintf("training restarting\n"); */
-			return -1;
 		}
 
 		/* Wait for MAC RX to be ready */
-		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SMUX_RX_CTL(index, interface),
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SMUX_RX_CTL(index, xi.interface),
 					  cvmx_bgxx_smux_rx_ctl_t, status, ==, 0, 10000)) {
 			cvmx_dprintf("ERROR: %d:BGX%d:%d: RX not ready\n", node, interface, index);
 			return -1;
 		}
 
 		/* Wait for BGX RX to be idle */
-		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SMUX_CTRL(index, interface),
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SMUX_CTRL(index, xi.interface),
 				  cvmx_bgxx_smux_ctrl_t, rx_idle, ==, 1, 10000)) {
 			cvmx_dprintf("ERROR: %d:BGX%d:%d: RX not idle\n", node, interface, index);
 			return -1;
 		}
 
 		/* Wait for GMX TX to be idle */
-		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SMUX_CTRL(index, interface),
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SMUX_CTRL(index, xi.interface),
 				  cvmx_bgxx_smux_ctrl_t, tx_idle, ==, 1, 10000)) {
 			cvmx_dprintf("ERROR: %d:BGX%d:%d: TX not idle\n", node, interface, index);
 			return -1;
 		}
 
 		/* rcvflt should still be 0 */
-		spu_status2.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS2(index, interface));
+		spu_status2.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS2(index, xi.interface));
 		if (spu_status2.s.rcvflt) {
 			cvmx_dprintf("ERROR: %d:BGX%d:%d: Receive fault, need to retry\n", node, interface, index);
 			return -1;
 		}
 
 		/* Receive link is latching low. Force it high and verify it */
-		spu_status1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS1(index, interface));
+		spu_status1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS1(index, xi.interface));
 		spu_status1.s.rcv_lnk = 1;
-		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_STATUS1(index, interface), spu_status1.u64);
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_STATUS1(index, xi.interface), spu_status1.u64);
 
-		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_STATUS1(index, interface),
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SPUX_STATUS1(index, xi.interface),
 				cvmx_bgxx_spux_status1_t, rcv_lnk, ==, 1, 10000)) {
 			cvmx_dprintf("ERROR: %d:BGX%d:%d: Receive link down\n", node, interface, index);
 			return -1;
@@ -1285,14 +1501,14 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 	}
 
 	/* (7) Enable packet transmit and receive */
-	spu_misc_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface));
+	spu_misc_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, xi.interface));
 	spu_misc_control.s.rx_packet_dis = 0;
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, interface), spu_misc_control.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_MISC_CONTROL(index, xi.interface), spu_misc_control.u64);
 
-	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
 	cmr_config.s.data_pkt_tx_en = 1;
 	cmr_config.s.data_pkt_rx_en = 1;
-	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
 
 	return 0;
 }
@@ -1300,19 +1516,21 @@ static int __cvmx_helper_bgx_xaui_link_init(int index, int xiface)
 int __cvmx_helper_bgx_xaui_enable(int xiface)
 {
 	int index;
-	int num_ports = cvmx_helper_ports_on_interface(xiface);
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	int interface = xi.interface;
 	int node = xi.node;
 	cvmx_helper_interface_mode_t mode;
+	int start = 0, end = 0;
 
-	mode = cvmx_helper_interface_get_mode(xiface);
+	__cvmx_helper_bgx_adjust_index(xiface, &start, &end);
 
-	for (index = 0; index < num_ports; index++) {
+	for (index = start; index < end; index++) {
 		int res;
 		int xipd_port = cvmx_helper_get_ipd_port(xiface, index);
 		int phy_pres;
 
+		mode = cvmx_helper_bgx_get_mode(xiface, index);
+
 		/* Set disparity for RXAUI interface as described in the
 		   Marvell RXAUI Interface specification. */
 		if (mode == CVMX_HELPER_INTERFACE_MODE_RXAUI &&
@@ -1320,8 +1538,7 @@ int __cvmx_helper_bgx_xaui_enable(int xiface)
 			phy_pres = 1;
 		else
 			phy_pres = 0;
-		if (__cvmx_helper_bgx_port_init(xipd_port, phy_pres))
-			return -1;
+		__cvmx_helper_bgx_port_init(xipd_port, phy_pres);
 
 		res = __cvmx_helper_bgx_xaui_link_init(index, xiface);
 		if (res == -1) {
@@ -1343,40 +1560,56 @@ cvmx_helper_link_info_t __cvmx_helper_bgx_xaui_link_get(int xipd_port)
 	cvmx_bgxx_spux_status1_t spu_status1;
 	cvmx_bgxx_smux_tx_ctl_t smu_tx_ctl;
 	cvmx_bgxx_smux_rx_ctl_t smu_rx_ctl;
+	cvmx_bgxx_cmrx_config_t cmr_config;
 	cvmx_helper_link_info_t result;
 
 	result.u64 = 0;
 
-	spu_status1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS1(index, interface));
-	smu_tx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_TX_CTL(index, interface));
-	smu_rx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_RX_CTL(index, interface));
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
 
-	if ((smu_tx_ctl.s.ls == 0)
-	    && (smu_rx_ctl.s.status == 0)
-	    && (spu_status1.s.rcv_lnk)) {
+	spu_status1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS1(index, xi.interface));
+	smu_tx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_TX_CTL(index, xi.interface));
+	smu_rx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_RX_CTL(index, xi.interface));
+
+	if ((smu_tx_ctl.s.ls == 0)     &&
+	    (smu_rx_ctl.s.status == 0) &&
+	    (spu_status1.s.rcv_lnk)) {
 		int lanes;
-		int qlm = cvmx_qlm_interface(xiface);
+		int qlm = cvmx_qlm_lmac(xiface, index);
 		uint64_t speed;
-		cvmx_helper_interface_mode_t mode;
 		result.s.link_up = 1;
 		result.s.full_duplex = 1;
-		speed = cvmx_qlm_get_gbaud_mhz(qlm);
-		mode = cvmx_helper_interface_get_mode(xiface);
-		lanes = 4 / cvmx_helper_ports_on_interface(xiface);
-
-		switch(mode) {
-		case CVMX_HELPER_INTERFACE_MODE_XFI:
-		case CVMX_HELPER_INTERFACE_MODE_XLAUI:
-		case CVMX_HELPER_INTERFACE_MODE_10G_KR:
-		case CVMX_HELPER_INTERFACE_MODE_40G_KR4:
-			/* Using 64b66b symbol encoding */
-			speed = (speed * 64 + 33) / 66;
-			break;
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+			speed = cvmx_qlm_get_gbaud_mhz_node(node, qlm);
+		else
+			speed = cvmx_qlm_get_gbaud_mhz(qlm);
+
+		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
+		switch(cmr_config.s.lmac_type) {
 		default:
-			/* Using 8b10b symbol encoding */
+		case 1:  // XAUI
 			speed = (speed * 8 + 5) / 10;
+			lanes = 4;
+			break;
+		case 2:  // RXAUI
+			speed = (speed * 8 + 5) / 10;
+			lanes = 2;
+			break;
+		case 3:  // XFI
+			speed = (speed * 64 + 33) / 66;
+			lanes = 1;
+			break;
+		case 4:  // XLAUI
+			speed = (speed * 64 + 33) / 66;
+			lanes = 4;
 			break;
 		}
+
+		if (debug)
+			cvmx_dprintf("%s: baud: %llu, lanes: %d\n", __func__,
+				     (unsigned long long)speed, lanes);
 		speed *= lanes;
 		result.s.speed = speed;
 	} else {
@@ -1396,16 +1629,19 @@ int __cvmx_helper_bgx_xaui_link_set(int xipd_port, cvmx_helper_link_info_t link_
 	int xiface = cvmx_helper_get_interface_num(xipd_port);
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
-	int interface = xi.interface;
 	int node = xi.node;
 	int index = cvmx_helper_get_interface_index_num(xp.port);
 	cvmx_bgxx_smux_tx_ctl_t smu_tx_ctl;
 	cvmx_bgxx_smux_rx_ctl_t smu_rx_ctl;
 	cvmx_bgxx_spux_status1_t spu_status1;
 
-	smu_tx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_TX_CTL(index, interface));
-	smu_rx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_RX_CTL(index, interface));
-	spu_status1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS1(index, interface));
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
+
+	smu_tx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_TX_CTL(index, xi.interface));
+	smu_rx_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_RX_CTL(index, xi.interface));
+	spu_status1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_STATUS1(index, xi.interface));
 
 	/* If the link shouldn't be up, then just return */
 	if (!link_info.s.link_up)
@@ -1426,36 +1662,140 @@ int __cvmx_helper_bgx_xaui_configure_loopback(int xipd_port,
 	int xiface = cvmx_helper_get_interface_num(xipd_port);
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(xipd_port);
-	int interface = xi.interface;
 	int node = xi.node;
 	int index = cvmx_helper_get_interface_index_num(xp.port);
 	cvmx_bgxx_spux_control1_t spu_control1;
 	cvmx_bgxx_smux_ext_loopback_t smu_ext_loopback;
 
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
+
 	/* Set the internal loop */
-	spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface));
+	spu_control1.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, xi.interface));
 	spu_control1.s.loopbck = enable_internal;
-	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, interface), spu_control1.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_SPUX_CONTROL1(index, xi.interface), spu_control1.u64);
 	/* Set the external loop */
-	smu_ext_loopback.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_EXT_LOOPBACK(index, interface));
+	smu_ext_loopback.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_EXT_LOOPBACK(index, xi.interface));
 	smu_ext_loopback.s.en = enable_external;
-	cvmx_write_csr_node(node, CVMX_BGXX_SMUX_EXT_LOOPBACK(index, interface), smu_ext_loopback.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_SMUX_EXT_LOOPBACK(index, xi.interface), smu_ext_loopback.u64);
 
 	return __cvmx_helper_bgx_xaui_link_init(index, xiface);
 }
 
+
+int __cvmx_helper_bgx_mixed_enable(int xiface)
+{
+	int index;
+	int num_ports = cvmx_helper_ports_on_interface(xiface);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
+	int node = xi.node;
+	cvmx_helper_interface_mode_t mode;
+
+	for (index = 0; index < num_ports; index++) {
+		int xipd_port, phy_pres = 0;
+
+		if (!cvmx_helper_is_port_valid(xiface, index))
+			continue;
+
+		mode = cvmx_helper_bgx_get_mode(xiface, index);
+
+		xipd_port = cvmx_helper_get_ipd_port(xiface, index);
+
+		if (mode == CVMX_HELPER_INTERFACE_MODE_RXAUI
+		    && (cvmx_helper_get_port_phy_present(xiface, index)))
+			phy_pres = 1;
+
+		if (__cvmx_helper_bgx_port_init(xipd_port, phy_pres))
+			continue;
+
+		/* Call SGMII init code for lmac_type = 0 */
+		if (mode == CVMX_HELPER_INTERFACE_MODE_SGMII) {
+			int do_link_set = 1;
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+			if (!(cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM))
+				do_link_set = 0;
+#endif
+
+			if (do_link_set)
+				__cvmx_helper_bgx_sgmii_link_set(xipd_port,
+					__cvmx_helper_bgx_sgmii_link_get(xipd_port));
+		/* All other lmac type call XAUI init code */
+		} else {
+			if (__cvmx_helper_bgx_xaui_link_init(index, xiface)) {
+				cvmx_dprintf("Failed to get %d:BGX(%d,%d) link\n", node, interface, index);
+				continue;
+			}
+		}
+	}
+	return 0;
+}
+
+cvmx_helper_link_info_t __cvmx_helper_bgx_mixed_link_get(int xipd_port)
+{
+	cvmx_bgxx_cmrx_config_t cmr_config;
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int index = cvmx_helper_get_interface_index_num(xipd_port);
+
+	cmr_config.u64 = cvmx_read_csr_node(xi.node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
+	if (cmr_config.s.lmac_type == 0)
+		return __cvmx_helper_bgx_sgmii_link_get(xipd_port);
+	else
+		return __cvmx_helper_bgx_xaui_link_get(xipd_port);
+}
+
+int __cvmx_helper_bgx_mixed_link_set(int xipd_port, cvmx_helper_link_info_t link_info)
+{
+	cvmx_bgxx_cmrx_config_t cmr_config;
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int index = cvmx_helper_get_interface_index_num(xipd_port);
+
+	cmr_config.u64 = cvmx_read_csr_node(xi.node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
+	if (cmr_config.s.lmac_type == 0)
+		return __cvmx_helper_bgx_sgmii_link_set(xipd_port, link_info);
+	else
+		return __cvmx_helper_bgx_xaui_link_set(xipd_port, link_info);
+}
+
+int __cvmx_helper_bgx_mixed_configure_loopback(int xipd_port,
+						     int enable_internal,
+						     int enable_external)
+{
+	cvmx_bgxx_cmrx_config_t cmr_config;
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int index = cvmx_helper_get_interface_index_num(xipd_port);
+
+	cmr_config.u64 = cvmx_read_csr_node(xi.node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
+	if (cmr_config.s.lmac_type == 0)
+		return __cvmx_helper_bgx_sgmii_configure_loopback(xipd_port,
+						enable_internal, enable_external);
+	else
+		return __cvmx_helper_bgx_xaui_configure_loopback(xipd_port,
+						enable_internal, enable_external);
+}
+
 /**
  * @INTERNAL
  * Configure Priority-Based Flow Control (a.k.a. PFC/CBFC)
  * on a specific BGX interface/port.
  */
 void __cvmx_helper_bgx_xaui_config_pfc(unsigned node,
-		unsigned interface, unsigned port, bool pfc_enable)
+		unsigned interface, unsigned index, bool pfc_enable)
 {
+	int xiface = cvmx_helper_node_interface_to_xiface(node, interface);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	cvmx_bgxx_smux_cbfc_ctl_t cbfc_ctl;
 
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
+
 	cbfc_ctl.u64 = cvmx_read_csr_node(node,
-		CVMX_BGXX_SMUX_CBFC_CTL(port, interface)
+		CVMX_BGXX_SMUX_CBFC_CTL(index, xi.interface)
 		);
 
 	/* Enable all PFC controls if requiested */
@@ -1467,12 +1807,11 @@ void __cvmx_helper_bgx_xaui_config_pfc(unsigned node,
 	cbfc_ctl.s.logl_en = 0xff;
 	cbfc_ctl.s.drp_en = pfc_enable;
 #endif
-#ifdef DEBUG
-	printf("%s: CVMX_BGXX_SMUX_CBFC_CTL(%d,%d)=%#llx\n",
-		__func__, port, interface, (unsigned long long)cbfc_ctl.u64);
-#endif
+	if (debug)
+		cvmx_dprintf("%s: CVMX_BGXX_SMUX_CBFC_CTL(%d,%d)=%#llx\n",
+			__func__, index, xi.interface, (unsigned long long)cbfc_ctl.u64);
 	cvmx_write_csr_node(node,
-		CVMX_BGXX_SMUX_CBFC_CTL(port, interface),
+		CVMX_BGXX_SMUX_CBFC_CTL(index, xi.interface),
 		cbfc_ctl.u64);
 }
 
@@ -1485,19 +1824,25 @@ void __cvmx_helper_bgx_xaui_config_pfc(unsigned node,
  * ctl_bck = 0, ctl_drp = 1: all PAUSE frames are completely ignored
  * @param node		node number.
  * @param interface	interface number
- * @param port		port number
+ * @param index		port number
  * @param ctl_bck	1: Forward PAUSE information to TX block
  * @param ctl_drp	1: Drop control PAUSE frames.
  */
 void cvmx_helper_bgx_rx_pause_ctl(unsigned node, unsigned interface,
-			unsigned port, unsigned ctl_bck, unsigned ctl_drp)
+			unsigned index, unsigned ctl_bck, unsigned ctl_drp)
 {
+	int xiface = cvmx_helper_node_interface_to_xiface(node, interface);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	cvmx_bgxx_smux_rx_frm_ctl_t frm_ctl;
 
-	frm_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_RX_CTL(port, interface));
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
+
+	frm_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SMUX_RX_CTL(index, xi.interface));
 	frm_ctl.s.ctl_bck = ctl_bck;
 	frm_ctl.s.ctl_drp = ctl_drp;
-	cvmx_write_csr_node(node, CVMX_BGXX_SMUX_RX_CTL(port, interface), frm_ctl.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_SMUX_RX_CTL(index, xi.interface), frm_ctl.u64);
 }
 
 /**
@@ -1505,7 +1850,7 @@ void cvmx_helper_bgx_rx_pause_ctl(unsigned node, unsigned interface,
  * and dmac filter match packets.
  * @param node		node number.
  * @param interface	interface number
- * @param port		port number
+ * @param index		port number
  * @param cam_accept	0: reject packets on dmac filter match
  *                      1: accept packet on dmac filter match
  * @param mcast_mode	0x0 = Force reject all multicast packets
@@ -1514,17 +1859,23 @@ void cvmx_helper_bgx_rx_pause_ctl(unsigned node, unsigned interface,
  * @param bcast_accept  0 = Reject all broadcast packets
  *                      1 = Accept all broadcast packets
  */
-void cvmx_helper_bgx_rx_adr_ctl(unsigned node, unsigned interface, unsigned port,
+void cvmx_helper_bgx_rx_adr_ctl(unsigned node, unsigned interface, unsigned index,
                                  unsigned cam_accept, unsigned mcast_mode, unsigned bcast_accept)
 {
+	int xiface = cvmx_helper_node_interface_to_xiface(node, interface);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
         cvmx_bgxx_cmrx_rx_adr_ctl_t adr_ctl;
 
-        adr_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_RX_ADR_CTL(port, interface));
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
+
+        adr_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_RX_ADR_CTL(index, xi.interface));
         adr_ctl.s.cam_accept = cam_accept;
         adr_ctl.s.mcst_mode = mcast_mode;
         adr_ctl.s.bcst_accept = bcast_accept;
 
-        cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_ADR_CTL(port, interface), adr_ctl.u64);
+        cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_ADR_CTL(index, xi.interface), adr_ctl.u64);
 }
 
 /**
@@ -1536,54 +1887,56 @@ void cvmx_helper_bgx_tx_options(unsigned node,
 	bool fcs_enable, bool pad_enable)
 {
 	cvmx_bgxx_cmrx_config_t cmr_config;
-	cvmx_bgxx_cmrx_config_t cmr_config0;
 	cvmx_bgxx_gmp_gmi_txx_append_t gmp_txx_append;
 	cvmx_bgxx_gmp_gmi_txx_min_pkt_t gmp_min_pkt;
 	cvmx_bgxx_smux_tx_min_pkt_t smu_min_pkt;
 	cvmx_bgxx_smux_tx_append_t  smu_tx_append;
+	int xiface = cvmx_helper_node_interface_to_xiface(node, interface);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+
+	if (!cvmx_helper_is_port_valid(xiface, index))
+		return;
+
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
 
-	cmr_config0.u64 = cvmx_read_csr_node(node,
-		CVMX_BGXX_CMRX_CONFIG(0, interface));
 	cmr_config.u64 = cvmx_read_csr_node(node,
-		CVMX_BGXX_CMRX_CONFIG(index, interface));
-
-	/* Temp: initial mode setting is only applied to LMAC(0), use that */
-	if (cmr_config0.s.lmac_type != cmr_config.s.lmac_type) {
-		cvmx_dprintf("WARNING: %s: "
-			"%d:BGX(%d).CMR(0).LMAC_TYPE != BGX(%d).CMR(%d).LMAC_TYPE\n",
-			__func__, node, interface, interface, index);
-		cmr_config.s.lmac_type = cmr_config0.s.lmac_type;
-	}
+		CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
 
-	if (cmr_config.s.lmac_type == 0) {
-		gmp_min_pkt.u64 = 0;
-		/* per HRM Sec 34.3.4.4 */
-		gmp_min_pkt.s.min_size = 59;
-		cvmx_write_csr_node(node,
-                        CVMX_BGXX_GMP_GMI_TXX_MIN_PKT(index, interface),
-			gmp_min_pkt.u64);
-		gmp_txx_append.u64 = cvmx_read_csr_node(node,
-			CVMX_BGXX_GMP_GMI_TXX_APPEND(index, interface));
-		gmp_txx_append.s.fcs = fcs_enable;
-		gmp_txx_append.s.pad = pad_enable;
-		cvmx_write_csr_node(node,
-			CVMX_BGXX_GMP_GMI_TXX_APPEND(index, interface),
-			gmp_txx_append.u64);
-	} else {
-		smu_min_pkt.u64 = 0;
-		/* HRM Sec 33.3.4.3 should read 64 */
-		 smu_min_pkt.s.min_size = 0x40;
-		cvmx_write_csr_node(node,
-                        CVMX_BGXX_SMUX_TX_MIN_PKT(index, interface),
-			smu_min_pkt.u64);
-		smu_tx_append.u64 = cvmx_read_csr_node(node,
-			CVMX_BGXX_SMUX_TX_APPEND(index, interface));
-		smu_tx_append.s.fcs_c = fcs_enable;
-		smu_tx_append.s.pad = pad_enable;
-		cvmx_write_csr_node(node,
-			CVMX_BGXX_SMUX_TX_APPEND(index, interface),
-			smu_tx_append.u64);
-	}
+	(void) cmr_config;	/* In case we need LMAC_TYPE later */
+
+	/* Setting options for both BGX subsystems, regardless of LMAC type */
+
+	/* Set GMP (SGMII) Tx options */
+	gmp_min_pkt.u64 = 0;
+	/* per HRM Sec 34.3.4.4 */
+	gmp_min_pkt.s.min_size = 59;
+	cvmx_write_csr_node(node,
+		CVMX_BGXX_GMP_GMI_TXX_MIN_PKT(index, xi.interface),
+		gmp_min_pkt.u64);
+	gmp_txx_append.u64 = cvmx_read_csr_node(node,
+		CVMX_BGXX_GMP_GMI_TXX_APPEND(index, xi.interface));
+	gmp_txx_append.s.fcs = fcs_enable;
+	gmp_txx_append.s.pad = pad_enable;
+	cvmx_write_csr_node(node,
+		CVMX_BGXX_GMP_GMI_TXX_APPEND(index, xi.interface),
+		gmp_txx_append.u64);
+
+	/* Set SMUX (XAUI/XFI) Tx options */
+	smu_min_pkt.u64 = 0;
+	/* HRM Sec 33.3.4.3 should read 64 */
+	 smu_min_pkt.s.min_size = 0x40;
+	cvmx_write_csr_node(node,
+		CVMX_BGXX_SMUX_TX_MIN_PKT(index, xi.interface),
+		smu_min_pkt.u64);
+	smu_tx_append.u64 = cvmx_read_csr_node(node,
+		CVMX_BGXX_SMUX_TX_APPEND(index, xi.interface));
+	smu_tx_append.s.fcs_c = fcs_enable;
+	smu_tx_append.s.pad = pad_enable;
+	cvmx_write_csr_node(node,
+		CVMX_BGXX_SMUX_TX_APPEND(index, xi.interface),
+		smu_tx_append.u64);
 }
 
 /**
@@ -1595,41 +1948,1778 @@ void cvmx_helper_bgx_tx_options(unsigned node,
  * 		    0 = Force reject all multicast packets
  * 		    1 = Force accept all multicast packets
  * 		    2 = use the address filter CAM.
- * @param mac       mac address for the ipd_port
+ * @param mac       mac address for the ipd_port, or 0 to disable MAC filtering
  */
 void cvmx_helper_bgx_set_mac(int xipd_port, int bcst, int mcst, uint64_t mac)
 {
 	int xiface = cvmx_helper_get_interface_num(xipd_port);
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	int interface = xi.interface;
 	int node = xi.node;
-	int index = (xipd_port >> 4) & 0xf;
+	int index;
 	cvmx_bgxx_cmr_rx_adrx_cam_t adr_cam;
 	cvmx_bgxx_cmrx_rx_adr_ctl_t adr_ctl;
 	cvmx_bgxx_cmrx_config_t cmr_config;
 	int saved_state;
+	cvmx_helper_interface_mode_t mode;
+
+	index = cvmx_helper_get_interface_index_num(xipd_port);
+
+	if (!cvmx_helper_is_port_valid(xiface, index))
+		return;
 
-	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, xi.node, xi.interface, index);
+
+	cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
 	saved_state = cmr_config.s.enable;
 	cmr_config.s.enable = 0;
-	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
 
 	/* Set the mac */
 	adr_cam.u64 = 0;
 	adr_cam.s.id = index;
-	adr_cam.s.en = 1;
+
+	if (mac != 0ull)
+		adr_cam.s.en = 1;
 	adr_cam.s.adr = mac;
-	cvmx_write_csr_node(node, CVMX_BGXX_CMR_RX_ADRX_CAM(index * 8, interface), adr_cam.u64);
 
-	adr_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_RX_ADR_CTL(index, interface));
-	adr_ctl.s.cam_accept = 1;  /* Accept the packet on DMAC CAM address */
+	cvmx_write_csr_node(node, CVMX_BGXX_CMR_RX_ADRX_CAM(index * 8, xi.interface), adr_cam.u64);
+
+	adr_ctl.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_RX_ADR_CTL(index, xi.interface));
+	if (mac != 0ull)
+		adr_ctl.s.cam_accept = 1;  /* Accept the packet on DMAC CAM address */
+	else
+		adr_ctl.s.cam_accept = 0;  /* No filtering, promiscous */
+
 	adr_ctl.s.mcst_mode = mcst;   /* Use the address filter CAM */
 	adr_ctl.s.bcst_accept = bcst; /* Accept all broadcast packets */
-	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_ADR_CTL(index, interface), adr_ctl.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_ADR_CTL(index, xi.interface), adr_ctl.u64);
 	/* Set SMAC for PAUSE frames */
-	cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_SMACX(index, interface), mac);
+	cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_SMACX(index, xi.interface), mac);
 
 	/* Restore back the interface state */
 	cmr_config.s.enable = saved_state;
-	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface), cmr_config.u64);
+	/* Wait 100ms after bringing up the link to give the PHY some time */
+	if (cmr_config.s.enable) {
+		mode = cvmx_helper_bgx_get_mode(xiface, index);
+		__cvmx_helper_bgx_interface_enable_delay(mode);
+	}
+}
+
+/**
+ * Disables the sending of flow control (pause) frames on the specified
+ * BGX port(s).
+ *
+ * @param xiface Which xiface
+ * @param port_mask Mask (4bits) of which ports on the interface to disable
+ *                  backpressure on.
+ *                  1 => disable backpressure
+ *                  0 => enable backpressure
+ *
+ * @return 0 on success
+ *         -1 on error
+ *
+ * FIXME: Should change the API to handle a single port in every
+ * invokation, for consistency with other API calls.
+ */
+int cvmx_bgx_set_backpressure_override(int xiface, unsigned port_mask)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	cvmx_bgxx_cmr_rx_ovr_bp_t rx_ovr_bp;
+	int node = xi.node;
+
+	if (xi.interface >= CVMX_HELPER_MAX_GMX)
+		return 0;
+
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d port_mask=%#x\n",
+			__func__, xi.node, xi.interface, port_mask);
+
+	/* Check for valid arguments */
+	rx_ovr_bp.u64 = 0;
+	rx_ovr_bp.s.en = port_mask;	/* Per port Enable back pressure override */
+	rx_ovr_bp.s.ign_fifo_bp = port_mask;	/* Ignore the RX FIFO full when computing BP */
+
+	cvmx_write_csr_node(node, CVMX_BGXX_CMR_RX_OVR_BP(xi.interface), rx_ovr_bp.u64);
+	return 0;
+}
+
+/**
+ * Set maximum packet size for a BGX port
+ *
+ */
+void cvmx_helper_bgx_set_jabber(int xiface, unsigned index,
+	unsigned size)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	cvmx_bgxx_cmrx_config_t cmr_config;
+	int node;
+
+	if (!octeon_has_feature(OCTEON_FEATURE_BGX))
+		return;
+
+	if (!cvmx_helper_is_port_valid(xiface, index))
+		return;
+
+	node = xi.node;
+
+	/* Get LMAC type from common config */
+	cmr_config.u64 = cvmx_read_csr_node(node,
+		CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
+
+	/* Set GMI or SMUX register based on lmac_type */
+	if (cmr_config.s.lmac_type == 0) {
+		cvmx_write_csr_node(node,
+				CVMX_BGXX_GMP_GMI_RXX_JABBER(index, xi.interface), size);
+	} else {
+		cvmx_write_csr_node(node,
+			CVMX_BGXX_SMUX_RX_JABBER(index, xi.interface), size);
+	}
+}
+
+/**
+ * Shutdown a BGX port
+ *
+ */
+int cvmx_helper_bgx_shutdown_port(int xiface, int index)
+{
+	cvmx_bgxx_cmrx_config_t cmr_config;
+	int node;
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+
+	node = xi.node;
+
+	if (xi.interface >= CVMX_HELPER_MAX_GMX)
+		return 0;
+
+	if (debug)
+		cvmx_dprintf("%s: interface %u:%d/%d\n",
+		__func__, node, xi.interface, index);
+
+	if (!cvmx_helper_is_port_valid(xiface, index))
+		return 0;
+
+	/* Disable BGX CMR before we make any changes. */
+	cmr_config.u64 = cvmx_read_csr_node(node,
+		CVMX_BGXX_CMRX_CONFIG(index, xi.interface));
+
+	cmr_config.s.enable = 0;
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, xi.interface),
+		cmr_config.u64);
+
+	/* Clear pending common interrupts */
+	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_INT(index, xi.interface), 0x7);
+
+	if (cmr_config.s.lmac_type == 0) {	/* SGMII */
+		/* Clear GMP interrupts */
+		cvmx_write_csr_node(node,
+			CVMX_BGXX_GMP_GMI_RXX_INT(index, xi.interface), 0xfff);
+		cvmx_write_csr_node(node,
+			CVMX_BGXX_GMP_GMI_TXX_INT(index, xi.interface), 0x1f);
+		/* Wait for GMX to be idle */
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node,
+			CVMX_BGXX_GMP_GMI_PRTX_CFG(index, xi.interface),
+			cvmx_bgxx_gmp_gmi_prtx_cfg_t, rx_idle, ==, 1, 10000) ||
+		    CVMX_WAIT_FOR_FIELD64_NODE(node,
+			CVMX_BGXX_GMP_GMI_PRTX_CFG(index, xi.interface),
+			cvmx_bgxx_gmp_gmi_prtx_cfg_t, tx_idle, ==, 1, 10000)) {
+				cvmx_printf("ERROR: %s: SGMII: "
+				"Timeout waiting for port %u:%d/%d to stop\n",
+				__func__, node, xi.interface, index);
+				return -1;
+			}
+		/* Read GMX CFG again to make sure the disable completed */
+		cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_GMI_PRTX_CFG(index, xi.interface));
+	} else {		/* XAUI/XFI/10-KR */
+		/* Clear all pending SMUX interrupts */
+		cvmx_write_csr_node(node, CVMX_BGXX_SMUX_RX_INT(index, xi.interface),
+			0xfff);
+		cvmx_write_csr_node(node, CVMX_BGXX_SMUX_TX_INT(index, xi.interface),
+			0x1f);
+		cvmx_write_csr_node(node, CVMX_BGXX_SPUX_INT(index, xi.interface),
+			0x7fff);
+
+		/* Wait for GMX RX to be idle */
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node,
+			CVMX_BGXX_SMUX_CTRL(index, xi.interface),
+			cvmx_bgxx_smux_ctrl_t, rx_idle, ==, 1, 10000) ||
+		    CVMX_WAIT_FOR_FIELD64_NODE(node,
+			CVMX_BGXX_SMUX_CTRL(index, xi.interface),
+			cvmx_bgxx_smux_ctrl_t, tx_idle, ==, 1, 10000)) {
+				cvmx_printf("ERROR: %s: XAUI: "
+				"Timeout waiting for port %u:%d/%d to stop\n",
+				__func__, node, xi.interface, index);
+				return -1;
+		}
+	}
+	return 0;
+}
+
+#ifdef CVMX_DUMP_BGX
+/**
+ * Dump configuration and status of BGX ports
+ */
+/* The following (high level) funcs are implemented in this section */
+int cvmx_dump_bgx_config(unsigned bgx);
+int cvmx_dump_bgx_status(unsigned bgx);
+int cvmx_dump_bgx_config_node(unsigned node, unsigned bgx);
+int cvmx_dump_bgx_status_node(unsigned node, unsigned bgx);
+
+/*
+ * The following macros helps to easyly print 'formated table'
+ * for up to 4 LMACs of single BGX device (same macros used for GSER)
+ * MACROS automaticaly handle data types ('unsigned' or 'const char *') and
+ * indexing (by predefined 'ind' string used as index).
+ * NOTE that there are different groups of macros:
+ *  'PRn' 	means it prints unconditionaly, data are 'unsigned' type
+ *  'PRns'	means it prints unconditionaly, data are 'const char *' type
+ *  'PRc' 	means it print only if 'cond' argument is 'true'
+ *  'PRd' 	means it print only if 'data' argument is != 0
+ *  'PRcd' 	means it print only if ('cond'== true && 'data' != 0)
+ *  'PRM' 	means it print only when 'mask' bits are != 0, skip otherwise
+ * 		NOTE: the loop is 'for (_mask = mask; _mask > 0; _mask >>= 1)'
+ * 		which means order is mask_bits 0,1,2,3.. and the last unset bits
+ * 		will not be handled at all (i.e. not 'skipped' with '<   ---   >')
+ * Other useful combinations of them exists like:
+ *  'PRMc' 	means it print only when 'mask' bits are != 0 and 'cond' = true
+ *  'PRMcs' 	same as above but data type is 'const char *' instead of 'unsigned'
+ * For example one use 'PRMcd' to print 'data' only for 'mask' mapped LMAC fields
+ * if 'cond' is met and 'data' != 0 (example: Frame Check 'FSCERR' (check for
+ * FCS errors) was enabled with 'cond' and err realy happens (detected) (i.e.
+ * 'data' != 0) and only then it will be printed
+ * (and only for LMACs mapped with 'mask' bits = 1, other fields skipped)
+ * NOTE: Where applicable references to Hardware Manual are available.
+ */
+
+
+/* define FORCE_COND=1 in order to force prints unconditionally (DEBUG, test)
+ * define FORCE_COND=0 in order to prints conditionally (NORMAL MODE, skip unimportant)
+ * if run time control is needed just define FORCE_COND as local static variable
+ * Enable only one of the following '#define FORCED_COND(cond)' macros
+ * use it like this: 'if ( FORCED_COND(cond) )'
+ */
+/* #define FORCE_COND	1	*/
+/* #define FORCED_COND(cond)	((cond) || FORCE_COND) */
+/* ..or.. the next line to exclude all overhead code */
+#define FORCED_COND(cond)	(cond)
+
+
+#ifndef USE_PRx_MACROS
+#define USE_PRx_MACROS
+
+/* Always print - no test of 'data' values */
+/* for (1..N) printf(format, data) */
+#define PRn(header, N, format, data)			\
+do {							\
+	unsigned ind;					\
+	cvmx_dprintf("%-48s", header);			\
+	for (ind = 0; ind < N; ind++)			\
+		cvmx_dprintf(format, data);		\
+	cvmx_dprintf("\n");				\
+} while(0);
+
+#define PRns	PRn
+
+/* Always print - no test, skip data for mask[x]=0 */
+#define PRMn(header, mask, format, data)				\
+do {									\
+	unsigned ind, _mask;						\
+	cvmx_dprintf("%-48s", header);					\
+	for (_mask = mask, ind = 0; _mask > 0; _mask >>= 1, ind++)	\
+		if (_mask & 1)						\
+			cvmx_dprintf(format, data);			\
+		else							\
+			cvmx_dprintf("%15s","");			\
+	cvmx_dprintf("\n");						\
+} while(0);
+
+#define PRMns PRMn
+
+/* mask + data != 0 */
+#define PRMd(header, mask, format, data)					\
+do {										\
+	unsigned cnt, ind, _mask = mask;					\
+	for (cnt = 0, ind = 0; _mask > 0; _mask >>= 1, ind++)			\
+		if (data != 0)							\
+			cnt++;							\
+	if (FORCED_COND(cnt)) { /* at least one item != 0 =>print */		\
+		cvmx_dprintf("%-48s", header);					\
+		for (_mask = mask, ind = 0; _mask > 0; _mask >>= 1, ind++)	\
+			if (FORCED_COND((_mask & 1) && data!= 0))		\
+				cvmx_dprintf(format, data);			\
+			else							\
+				cvmx_dprintf("%15s","");			\
+		cvmx_dprintf("\n");						\
+	}									\
+} while(0);
+
+/* mask + cond != 0 */
+#define PRMc(header, mask, cond, format, data)					\
+do {										\
+	unsigned cnt, ind, _mask = mask;					\
+	for (cnt = 0, ind = 0; _mask > 0; _mask >>= 1, ind++)			\
+		if (cond != 0)							\
+			cnt++;							\
+	if (FORCED_COND(cnt)) { /* at least one item != 0 =>print */		\
+		cvmx_dprintf("%-48s", header);					\
+		for (_mask = mask, ind = 0; _mask > 0; _mask >>= 1, ind++)	\
+			if (FORCED_COND((_mask & 1) && cond))			\
+				cvmx_dprintf(format, data);			\
+			else							\
+				cvmx_dprintf("%15s","");			\
+		cvmx_dprintf("\n");						\
+	}									\
+} while(0);
+
+#define PRMcs	PRMc
+
+/* for (mask[i]==1) if (cond && data) printf(format, data) */
+#define PRMcd(header, mask, cond, format, data)					\
+do {										\
+	unsigned cnt, ind, _mask = mask;					\
+	for (cnt = 0, ind = 0; _mask > 0; _mask >>= 1, ind++)			\
+		if (cond && data != 0)						\
+			cnt++;							\
+	if (FORCED_COND(cnt)) { /* at least one item != 0 =>print */		\
+		cvmx_dprintf("%-48s", header);					\
+		for (_mask = mask, ind = 0; _mask > 0; _mask >>= 1, ind++)	\
+		if ( FORCED_COND(cond && (data!=0) && (_mask & 1)) )		\
+			cvmx_dprintf(format, data);				\
+		else								\
+			cvmx_dprintf("%15s","");				\
+		cvmx_dprintf("\n");						\
+	}									\
+} while(0);
+
+/* Test 'data' int values and print only if data != 0 */
+/* for (1..N) if (data != 0) printf(format, data) */
+#define PRd(header, N, format, data)					\
+do {									\
+	unsigned ind, cnt;						\
+	for (cnt = 0, ind = 0; ind < N; ind++)				\
+		if (data != 0)						\
+			cnt++;						\
+	if (FORCED_COND(cnt)) { /* at least one item != 0 =>print */	\
+		cvmx_dprintf("%-48s", header);				\
+		for (ind = 0; ind < N; ind++)				\
+			cvmx_dprintf(format, data);			\
+		cvmx_dprintf("\n");					\
+	}								\
+} while(0);
+
+/* for (1..N) if (cond) printf(format, data) */
+#define PRc(header, N, cond, format, data)				\
+do {									\
+	unsigned ind, cnt;						\
+	for (cnt = 0, ind = 0; ind < N; ind++)				\
+		if (cond != 0)						\
+			cnt++;						\
+	if (FORCED_COND(cnt)) { /* at least one item != 0 =>print */	\
+		cvmx_dprintf("%-48s", header);				\
+		for (ind = 0; ind < N; ind++)				\
+		if ( FORCED_COND(cond) )				\
+			cvmx_dprintf(format, data);			\
+		else							\
+			cvmx_dprintf("%15s","");			\
+		cvmx_dprintf("\n");					\
+	}								\
+} while(0);
+
+#define PRcs PRc
+
+/* for (1..N) if (cond && data) printf(format, data) */
+#define PRcd(header, N, cond, format, data)				\
+do {									\
+	unsigned ind, cnt;						\
+	for (cnt = 0, ind = 0; ind < N; ind++)				\
+		if (cond && data != 0)					\
+			cnt++;						\
+	if (FORCED_COND(cnt)) { /* at least one item != 0 =>print */	\
+		cvmx_dprintf("%-48s", header);				\
+		for (ind = 0; ind < N; ind++)				\
+		if ( FORCED_COND(cond && data) )			\
+			cvmx_dprintf(format, data);			\
+		else							\
+			cvmx_dprintf("%15s","");			\
+		cvmx_dprintf("\n");					\
+	}								\
+} while(0);
+
+#endif
+
+/*===================== common helping funcs =====================*/
+/* NOTE: Even many of these functions gets as arguments numer of used LMACs
+ * when needed they create an apropriate 'mask' and skip unrelated LMAC fields
+ * (for example if a functions is GMP specific it will handle only LMACs with
+ * SGMII or 1000BASE-X type and skip XFI LMAC)
+ */
+/* The following funcs are implemented in this section */
+const char * get_lmac_type_name(uint8_t lmac_type, uint8_t pcs_misc_ctl_mode);
+int get_num_pcs_lanes(uint8_t lmac_type);
+const char * get_bind_lanes_per_lmac(uint8_t ind, uint8_t lmac_type, uint8_t lane_to_sds);
+int cvmx_helper_bgx_link_status(int node, int bgx, unsigned N);
+
+/* return name of interface type ("SGMII", "XAUI", etc. */
+const char * get_lmac_type_name(uint8_t lmac_type, uint8_t pcs_misc_ctl_mode)
+{
+	static char *lmac_types[] = {
+		"  SGMII   ",
+		"XAUI/DXAUI",
+		"  RXAUI   ",
+		"  10G_R   ",
+		"  40G_R   ",
+		"  RGMII   "	/* only 73xx has that */
+	};
+
+	if ( (OCTEON_IS_MODEL(OCTEON_CN73XX) && lmac_type > 5) ||
+		(!OCTEON_IS_MODEL(OCTEON_CN78XX) && lmac_type > 4) )
+		return "?????";
+	if (lmac_type == 0 && pcs_misc_ctl_mode == 1)
+		return "1000BASE-X";
+	return lmac_types[lmac_type];
+}
+
+/* return number of lanes per interface type (SGMII, XAUI, etc. */
+int get_num_pcs_lanes(uint8_t lmac_type)
+{
+	static int num_pcs_lanes[] = {
+		1/*SGMII*/, 4/*XAUI*/, 2/*RXAUI*/, 1/*10G_R*/, 4/*40G_R*/
+	};
+
+	if (lmac_type > 4)
+		return 0;	/* i.e 73xx lmac=5 (RGMII) or out of range */
+	return num_pcs_lanes[lmac_type];
+}
+
+/* return SerDes lanes connected to lmac - string (up to 7 chars '0,1,2,3')
+ * or up to 9 chars "RGMII/XCV" for RGMII(lmac_type=5)
+ */
+const char * get_bind_lanes_per_lmac(uint8_t ind, uint8_t lmac_type, uint8_t lane_to_sds)
+{
+	static char tmp[4][9], bind_lanes_per_lmac[4][9];
+	uint8_t i, n;
+
+	if (lmac_type > 5 || ind > 3)
+		return "  ???  ";
+	if (ind != 0 && lmac_type == 5/*RGMII*/)
+		return "ERR:RGMII";	/* Only LMAC0 can connect to RGMII/XCV */
+	if (ind == 0 && lmac_type == 5/*RGMII*/)
+		return "RGMII/XCV";	/* Only LMAC0 can connect to RGMII/XCV */
+	bind_lanes_per_lmac[ind][0] = 0;
+	for (i=0; i < get_num_pcs_lanes(lmac_type); i++) {
+		sprintf(tmp[ind], ",%1d", (lane_to_sds >> (/*2*i*/i<<1)) & 3);
+		strcat( bind_lanes_per_lmac[ind], tmp[ind]);
+	}
+	/* max returned strlen("0,1,2,3")=7; let's center the string "nnn%snnn" */
+	/* &bind_lanes_per_lmac[1]; - skip the first ',' */
+	n = (9 - strlen(&bind_lanes_per_lmac[ind][1])) / 2;
+	sprintf(tmp[ind], "%*s%s%*s", n, "", &bind_lanes_per_lmac[ind][1], n, "");
+
+	/* 'ind' make sure we store results in different places and return
+	 * different pointers - otherwise, if we store on same location and
+	 * return the same pointer always (like return tmp;), ONLY the
+	 * last result will be available. because it overwritten previous values
+	 */
+	return tmp[ind];
+}
+
+int cvmx_helper_bgx_number_rx_tx_lmacs(unsigned node, unsigned bgx, unsigned *pN)
+{
+	int N;
+	cvmx_bgxx_cmr_rx_lmacs_t rx_lmacs;
+	cvmx_bgxx_cmr_tx_lmacs_t tx_lmacs;
+
+	*pN = 1; /* default return - most likely will be overwritten */
+
+	rx_lmacs.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMR_RX_LMACS(bgx));
+	tx_lmacs.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMR_TX_LMACS(bgx));
+	cvmx_dprintf("NODE%d: BGX%d RX_LMACs = %d\n",
+		     node, bgx, rx_lmacs.s.lmacs);
+	cvmx_dprintf("NODE%d: BGX%d TX_LMACs = %d\n",
+		     node, bgx, tx_lmacs.s.lmacs);
+	if (rx_lmacs.s.lmacs == tx_lmacs.s.lmacs ) {
+		*pN = rx_lmacs.s.lmacs;
+		return 0;
+	} else { /* This can happen only if BGX_config is wrong, so report it */
+		N = __cvmx_helper_bgx_enumerate(
+			cvmx_helper_node_interface_to_xiface(node, bgx) );
+		cvmx_dprintf("<<<WARNING>>>: RX_LMACS != TX_LMACS;"
+			" return  __cvmx_helper_bgx_enumerate(xiface)=%d\n", N);
+		if (N >= 0)
+			*pN = N;
+		return -1;
+	}
+}
+
+/* CN73xx-HM-0.95E 32.7 RGMII: XCV Registers */
+
+int cvmx_helper_bgx_rgmii_config(int node, int bgx, unsigned N)
+{
+	cvmx_xcv_reset_t		xcv_reset;
+	cvmx_xcv_dll_ctl_t		xcv_dll_ctl;
+	cvmx_xcv_comp_ctl_t		xcv_comp_ctl;
+	cvmx_xcv_ctl_t			xcv_ctl;
+
+	xcv_reset.u64 = cvmx_read_csr_node(node, CVMX_XCV_RESET);
+	xcv_dll_ctl.u64 = cvmx_read_csr_node(node, CVMX_XCV_DLL_CTL);
+	xcv_comp_ctl.u64 = cvmx_read_csr_node(node, CVMX_XCV_COMP_CTL);
+	xcv_ctl.u64 = cvmx_read_csr_node(node, CVMX_XCV_CTL);
+
+	cvmx_dprintf("/*=== NODE%d: BGX%d RGMII config ===*/\n", node, bgx);
+
+	PRns("RGMII: Port Enable(enable)", 1, "   %8s    ",
+		xcv_reset.s.enable ? " Enabled" : "Disabled");
+	PRn("RGMII: DLL CLK reset(clkrst)", 1, "       %1d       ",
+		xcv_reset.s.clkrst);
+	PRn("RGMII: DLL reset(dllrst)", 1, "       %1d       ",
+		xcv_reset.s.dllrst);
+	PRns("RGMII: (Drive strenght)Compensation enable(comp)", 1, "   %8s    ",
+		xcv_reset.s.comp ? " Enabled" : "Disabled");
+	PRn("RGMII: Packet reset for TX(tx_pkt_rst_n)", 1, "       %1d       ",
+		xcv_reset.s.tx_pkt_rst_n);
+	PRn("RGMII: Datapath reset for TX(tx_dat_rst_n)", 1, "       %1d       ",
+		xcv_reset.s.tx_dat_rst_n);
+	PRn("RGMII: Packet reset for Rx(rx_pkt_rst_n)", 1, "       %1d       ",
+		xcv_reset.s.rx_pkt_rst_n);
+	PRn("RGMII: Datapath reset for RX(rx_dat_rst_n)", 1, "       %1d       ",
+		xcv_reset.s.rx_dat_rst_n);
+
+	PRn("RGMII: The clock delay measured by DLL(clk_set)",
+		1, "      %3d      ", xcv_dll_ctl.s.clk_set);
+	PRn("RGMII: Bypass the RX clock delay(clkrx_byp)",
+		1, "       %1d       ", xcv_dll_ctl.s.clkrx_byp);
+	PRn("RGMII: RX clk delay when bypass mode(clkrx_set)",
+		1, "      %3d      ", xcv_dll_ctl.s.clkrx_set);
+	PRn("RGMII: Bypass the TX clock delay(clktx_byp)",
+		1, "       %1d       ", xcv_dll_ctl.s.clktx_byp);
+	PRn("RGMII: TX clk delay when bypass mode(clktx_set)",
+		1, "      %3d      ", xcv_dll_ctl.s.clktx_set);
+	PRns("RGMII: Reference clock to use(refclk_sel)", 1, "  %10s   ",
+		xcv_dll_ctl.s.refclk_sel == 0 ? "   RGMII   " :
+		xcv_dll_ctl.s.refclk_sel == 1 ? " RGMII RXC " :
+		xcv_dll_ctl.s.refclk_sel == 2 ? " CopClk/N  " : " Reserved ");
+
+	PRn("RGMII: Bypass Comp.(use DRV_[P,N]CTL)(drv_byp)", 1,
+		"       %1d       ", xcv_comp_ctl.s.drv_byp);
+	PRn("RGMII: Contr. PCTL drive strength(cmp_pctl)", 1,
+		"       %2d      ", xcv_comp_ctl.s.cmp_pctl);
+	PRn("RGMII: Contr. NCTL drive strength(cmp_nctl)", 1,
+		"       %2d      ", xcv_comp_ctl.s.cmp_nctl);
+	PRn("RGMII: Bypass PCTL drive strength(drv_pctl)", 1,
+		"       %2d      ", xcv_comp_ctl.s.drv_pctl);
+	PRn("RGMII: Bypass NCTL drive strength(drv_nctl)", 1,
+		"       %2d      ", xcv_comp_ctl.s.drv_nctl);
+	PRn("RGMII: PCTL Lock(pctl_lock)", 1,
+		"       %1d       ", xcv_comp_ctl.s.pctl_lock);
+	PRn("RGMII: PCTL Saturate(pctl_sat)", 1,
+		"       %1d       ", xcv_comp_ctl.s.pctl_sat);
+	PRn("RGMII: NCTL Lock(nctl_lock)", 1,
+		"       %1d       ", xcv_comp_ctl.s.nctl_lock);
+	PRn("RGMII: NCTL Saturate(nctl_sat)", 1,
+		"       %1d       ", xcv_comp_ctl.s.nctl_sat);
+
+	PRns("RGMII:XCV: External Loopback Enabled(lpbk_ext)", 1, "   %8s    ",
+		xcv_ctl.s.lpbk_ext ? " Enabled" : "Disabled");
+	PRns("RGMII:XCV: Internal Loopback Enabled(lpbk_int)", 1, "   %8s    ",
+		xcv_ctl.s.lpbk_int ? " Enabled" : "Disabled");
+	PRns("RGMII:XCV: Speed (speed)", 1, "   %8s    ",
+		xcv_ctl.s.speed == 0 ? " 10 Mbps " :
+		xcv_ctl.s.speed == 1 ? " 100 Mbps" :
+		xcv_ctl.s.speed == 2 ? " 1 Gbps  " : "Reserved");
+	return 0;
+}
+
+int cvmx_helper_bgx_rgmii_status(int node, int bgx, unsigned N)
+{
+	cvmx_xcv_int_t			xcv_int;
+	cvmx_xcv_inbnd_status_t		xcv_inbnd_status;
+
+	xcv_int.u64 = cvmx_read_csr_node(node, CVMX_XCV_INT);
+	xcv_inbnd_status.u64 = cvmx_read_csr_node(node, CVMX_XCV_INBND_STATUS);
+
+	cvmx_dprintf("/*=== NODE%d: BGX%d RGMII status ===*/\n", node, bgx);
+
+	PRd("RGMII: TX FIFO overflow(tx_ovrflw)", 1,
+		"       %1d       ", xcv_int.s.tx_ovrflw);
+	PRd("RGMII: TX FIFO underflow(tx_undflw)", 1,
+		"       %1d       ", xcv_int.s.tx_undflw);
+	PRd("RGMII: Incomplete byte -10/100 mode(incomp_byte)", 1,
+		"       %1d       ", xcv_int.s.incomp_byte);
+	PRd("RGMII: Inband status change on link duplex", 1,
+		"       %1d       ", xcv_int.s.duplex);
+	PRd("RGMII: Inband status change on link duplex", 1,
+		"       %1d       ", xcv_int.s.duplex);
+	PRd("RGMII: Inband status change on link up/down", 1,
+		"       %1d       ", xcv_int.s.speed);
+
+	PRns("RGMII: RGMII inband status (duplex)", 1, "  %10s   ",
+		xcv_inbnd_status.s.duplex ? "Full-duplex" : "Half-duplex");
+	PRns("RGMII: RGMII inband status (speed)", 1, "   %8s    ",
+		xcv_inbnd_status.s.speed == 0 ? " 10 Mbps " :
+		xcv_inbnd_status.s.speed == 1 ? " 100 Mbps" :
+		xcv_inbnd_status.s.speed == 2 ? " 1 Gbps " : "Reserved");
+	PRns("RGMII: RGMII inband status (link)", 1, "   %9s   ",
+		xcv_inbnd_status.s.duplex ? " Link-Up " : "Link-Down");
+	return 0;
+}
+
+
+typedef struct {
+	cvmx_bgxx_cmrx_config_t 	cmr_config;
+	cvmx_bgxx_spux_bx_status_t	spu_bx_status;
+	cvmx_bgxx_spux_br_status1_t	spu_br_status1;
+	cvmx_bgxx_spux_br_algn_status_t	br_algn_status;
+	cvmx_bgxx_smux_rx_ctl_t		rx_ctl;
+	cvmx_bgxx_gmp_pcs_mrx_status_t	gmp_pcs_mr_status;
+} lmac_link_status_t;
+
+/* CN78xx-HM-0.992E 33.6.5 (SPU) Link Status and Changes */
+/* CN78xx-HM-0.992E 34.2.2 (GMP) PCS Mode Determination */
+/* Collect/Dump the BGX Link status for all modes (SGMII, XAUI, etc. */
+int cvmx_helper_bgx_link_status(int node, int bgx, unsigned N)
+{
+	lmac_link_status_t lmac[4];
+	unsigned ind;
+	uint8_t mask, mask_sgmii, mask_xaui, mask_10g, mask_40g, lmac_type;
+
+	mask = mask_sgmii = mask_xaui = mask_10g = mask_40g = 0;
+	for (ind = 0; ind < N; ind++) {
+		lmac[ind].cmr_config.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_CONFIG(ind, bgx));
+		lmac[ind].rx_ctl.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SMUX_RX_CTL(ind, bgx));
+		lmac_type = lmac[ind].cmr_config.s.lmac_type;
+		switch (lmac_type) {
+			case 0: /* SGMII/1000BASE-X */
+				/* the first read reads/clears the old lnk_st;
+					repeat this func to see the current lnk_st value */
+				lmac[ind].gmp_pcs_mr_status.u64 = cvmx_read_csr_node(node,
+					CVMX_BGXX_GMP_PCS_MRX_STATUS(ind, bgx));
+				break;
+			case 1: /* 10GBASE-X/XAUI or DXAUI */
+			case 2: /* RXAUI */
+				lmac[ind].spu_bx_status.u64 = cvmx_read_csr_node(node,
+					   CVMX_BGXX_SPUX_BX_STATUS(ind, bgx));
+				break;
+			case 3: /* 10BASE-R */
+				lmac[ind].spu_br_status1.u64 = cvmx_read_csr_node(node,
+					   CVMX_BGXX_SPUX_BR_STATUS1(ind, bgx));
+				break;
+			case 4: /* 40GBASE-R */
+				lmac[ind].br_algn_status.u64 = cvmx_read_csr_node(node,
+					   CVMX_BGXX_SPUX_BR_ALGN_STATUS(ind, bgx));
+				break;
+		}
+
+		if (lmac_type == 0 /* SGMII/1000BASE-X */ )
+			mask_sgmii |= (1 << ind);
+		else if (lmac_type == 1 /* 10GBASE-X/XAUI or DXAUI */
+			|| lmac_type == 2 /* RXAUI */)
+			mask_xaui |= (1 << ind);
+		else if (lmac_type == 3 /* 10GBASE-R */)
+			mask_10g |= (1 << ind);
+		else if (lmac_type == 3 /* 10GBASE-R */)
+			mask_10g |= (1 << ind);
+	}
+
+	if (mask_sgmii != 0) {
+		PRMns("GMP: LINK: Link Status(lnk_st)", mask_sgmii,
+			"      %4s     ",
+			lmac[ind].gmp_pcs_mr_status.s.lnk_st ? " Up " : "Down");
+	}
+	if (mask_xaui != 0) {
+		PRMns("LINK: xXAUI SerDes Lane Sync(lsync)", mask_xaui,
+			"      %3s      ",
+			(lmac[ind].spu_bx_status.s.lsync & (1<<ind))
+				? "Yes" : " No");
+		PRMns("LINK: xXAUI SerDes Lane Align(align)", mask_xaui,
+			"      %3s      ",
+			(lmac[ind].spu_bx_status.s.alignd & (1<<ind))
+				? "Yes" : " No");
+	}
+	if (mask_10g != 0) {
+		PRMns("LINK: 10GBASE-R PCS Receive-link(rcv_lnk)", mask_10g,
+			"      %3s      ",
+			lmac[ind].spu_br_status1.s.rcv_lnk ? "Yes" : " No");
+		PRMns("LINK: 10GBASE-R PCS block lock(blk_lock)", mask_10g,
+			"      %3s      ",
+			lmac[ind].spu_br_status1.s.blk_lock ? "Yes" : " No");
+	}
+	if (mask_40g != 0) {
+		PRMn("LINK: 40GBASE-R PCS lane Lock(block_lock)", mask_40g,
+			"       %d       ",
+			(lmac[ind].br_algn_status.s.block_lock & (1<<ind)) != 0 );
+		PRMn("LINK: 40GBASE-R PCS lane Align(align)", mask_40g,
+			"       %d       ",
+			lmac[ind].br_algn_status.s.alignd );
+	}
+	mask = (1 << N) - 1;
+	PRMns("LINK: Reconcilation Status(status)", mask,"  %11s  ",
+		lmac[ind].rx_ctl.s.status == 0 ? "  Link Up  " :
+		lmac[ind].rx_ctl.s.status == 1 ? " Local Fail" :
+		lmac[ind].rx_ctl.s.status == 2 ? "Remote Fail" : "???????????");
+
+	return 0;
+}
+
+typedef struct {
+	cvmx_bgxx_cmrx_config_t 		cmr_config;
+	cvmx_bgxx_smux_rx_frm_chk_t	rx_frm_chk;
+	cvmx_bgxx_smux_rx_frm_ctl_t	rx_frm_ctl;
+	cvmx_bgxx_smux_rx_int_t		smu_rx_int;
+	cvmx_bgxx_smux_hg2_control_t	hg2_control;
+	cvmx_bgxx_spux_int_t		spu_int;
+	cvmx_bgxx_cmrx_int_t		cmr_int;
+} lmac_spu_rx_errors_t;
+
+
+/*=============== SPU (xXAUI, 10G, 40G mode) help funcs ==================*/
+/* The following funcs are implemented in this section */
+int cvmx_helper_bgx_spu_rx_errors(int node, int bgx, unsigned N);
+int cvmx_helper_bgx_spu_tx_errors(int node, int bgx, unsigned N);
+int cvmx_helper_bgx_spu_loopback(int node, int bgx, unsigned N);
+
+/* CN78xx-HM-0.992E 33.5.1 (SPU) Receive Errors/Exceptions Checks */
+int cvmx_helper_bgx_spu_rx_errors(int node, int bgx, unsigned N)
+{
+	lmac_spu_rx_errors_t lmac[4];
+	unsigned ind;
+	uint8_t lmac_type, mask_xaui = 0;
+
+	for (ind = 0; ind < N; ind++) {
+		lmac[ind].rx_frm_chk.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SMUX_RX_FRM_CHK(ind, bgx));
+		lmac[ind].rx_frm_ctl.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SMUX_RX_FRM_CTL(ind, bgx));
+		lmac[ind].smu_rx_int.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SMUX_RX_INT(ind, bgx));
+		lmac[ind].spu_int.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SPUX_INT(ind, bgx));
+		lmac[ind].cmr_int.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_INT(ind, bgx));
+		lmac[ind].cmr_config.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_CONFIG(ind, bgx));
+		lmac_type = lmac[ind].cmr_config.s.lmac_type;
+		mask_xaui |= ((lmac_type != 0) << ind);
+	}
+	PRMcd("SPU:RXERR: Too long packet(jabber)", mask_xaui,
+		/*cond*/lmac[ind].rx_frm_chk.s.jabber,
+		"       %d       ", /*data*/lmac[ind].smu_rx_int.s.jabber);
+	PRMcd("SPU:RXERR: Err Ctrl Char (rcverr)", mask_xaui,
+		/*cond*/lmac[ind].rx_frm_chk.s.rcverr,
+		"       %d       ", /*data*/lmac[ind].smu_rx_int.s.rcverr);
+	PRMcd("SPU:RXERR: Ctrl Frame FCS/CRC(fcserr_c)", mask_xaui,
+		/*cond*/lmac[ind].rx_frm_chk.s.fcserr_c,
+		"       %d       ", /*data*/lmac[ind].smu_rx_int.s.fcserr);
+	PRMcd("SPU:RXERR: Data Frame FCS/CRC(fcserr_d)", mask_xaui,
+		/*cond*/lmac[ind].rx_frm_chk.s.fcserr_d,
+		"       %d       ", /*data*/lmac[ind].smu_rx_int.s.fcserr);
+	PRMcd("SPU:RXERR: Not-enough-data(skperr)", mask_xaui,
+		/*cond*/lmac[ind].rx_frm_chk.s.skperr,
+		"       %d       ", /*data*/lmac[ind].smu_rx_int.s.skperr);
+	PRMcd("SPU:RXERR: Bad Preamble(pcterr)", mask_xaui,
+		/*cond*/lmac[ind].rx_frm_ctl.s.pre_chk,
+		"       %d       ", /*data*/lmac[ind].smu_rx_int.s.pcterr);
+	PRMcd("SPU:RXERR: Rx a reserv ctrl code group(rsverr)", mask_xaui, /*cond*/1,
+		"       %d       ", /*data*/lmac[ind].smu_rx_int.s.rsverr);
+	PRMcd("SPU:RXERR: PAUSE RxFIFO FULL DROP(pause_drp)", mask_xaui, /*cond*/1,
+		"       %d       ", /*data*/lmac[ind].cmr_int.s.pause_drp);
+	PRMcd("SPU:RXERR: Code group not aligned(loc_fault)", mask_xaui, /*cond*/1,
+		"       %d       ", /*data*/lmac[ind].smu_rx_int.s.loc_fault);
+	PRMcd("SPU:RXERR: 'loc_fault' on remote node(rem_fault)", mask_xaui, /*cond*/1,
+		"       %d       ", /*data*/lmac[ind].smu_rx_int.s.rem_fault);
+	PRMcd("SPU:RXERR: A reserved sequence(bad_seq)", mask_xaui, /*cond*/1,
+		"       %d       ", /*data*/lmac[ind].smu_rx_int.s.bad_seq);
+	PRMcd("SPU:RXERR: Bad termination symbol(bad_term)", mask_xaui, /*cond*/1,
+		"       %d       ", /*data*/lmac[ind].smu_rx_int.s.bad_term);
+	PRMcd("SPU:RXERR: Code group sync lost(synlos)", mask_xaui, /*cond*/1,
+		"       %d       ", /*data*/lmac[ind].spu_int.s.synlos);
+	PRMcd("SPU:RXERR: Bit lock is lost(bitlcklos)", mask_xaui, /*cond*/1,
+		"       %d       ", /*data*/lmac[ind].spu_int.s.bitlckls);
+	PRMcd("SPU:RXERR: Bad (CRC) HiGig2 message(hg2cc)", mask_xaui,
+		/*cond*/lmac[ind].hg2_control.s.hg2rx_en,
+		"       %d       ", /*data*/lmac[ind].smu_rx_int.s.hg2cc);
+	PRMcd("SPU:RXERR: HiGig2 isn't phy/log link msg(hg2fld)", mask_xaui,
+		/*cond*/lmac[ind].hg2_control.s.hg2rx_en,
+		"       %d       ", /*data*/lmac[ind].smu_rx_int.s.hg2fld);
+	return 0;
+}
+
+typedef struct {
+	cvmx_bgxx_cmrx_config_t 		cmr_config;
+	cvmx_bgxx_smux_tx_int_t		smu_tx_int;
+} lmac_spu_tx_errors_t;
+
+/* CN78xx-HM-0.992E 33.5.2 (SPU) Transmit Errors/Exceptions Checks */
+int cvmx_helper_bgx_spu_tx_errors(int node, int bgx, unsigned N)
+{
+	lmac_spu_tx_errors_t lmac[4];
+	unsigned ind;
+	uint8_t lmac_type, mask_xaui = 0;
+
+	for (ind = 0; ind < N; ind++) {
+		lmac[ind].smu_tx_int.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SMUX_TX_INT(ind, bgx));
+		lmac[ind].cmr_config.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_CONFIG(ind, bgx));
+		lmac_type = lmac[ind].cmr_config.s.lmac_type;
+		mask_xaui |= ((lmac_type != 0) << ind);
+	}
+	PRMcd("SPU:TXERR: Packet Transfer Underflow(undflw)", mask_xaui, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].smu_tx_int.s.undflw);
+	PRMcd("SPU:TXERR: PTP pkt when link down(fake_commit)", mask_xaui, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].smu_tx_int.s.fake_commit);
+	return 0;
+}
+
+typedef struct {
+	cvmx_bgxx_cmrx_config_t 		cmr_config;
+	cvmx_bgxx_spux_control1_t		spu_control1;
+	cvmx_bgxx_smux_ext_loopback_t		smu_ext_loopback;
+} lmac_spu_loopback_t;
+
+/* CN78xx-HM-0.992E 33.8 (SPU, SMU) Loopback */
+int cvmx_helper_bgx_spu_loopback(int node, int bgx, unsigned N)
+{
+	lmac_spu_loopback_t lmac[4];
+	unsigned ind;
+	bool lb1[4]/*internal loopback*/, lb2[4]/* external loopback*/;
+	uint8_t lmac_type, mask_xaui = 0;
+
+	for (ind = 0; ind < N; ind++) {
+		lmac[ind].cmr_config.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_CONFIG(ind, bgx));
+		lmac[ind].spu_control1.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SPUX_CONTROL1(ind, bgx));
+		lmac[ind].smu_ext_loopback.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SMUX_EXT_LOOPBACK(ind, bgx));
+
+		lb1[ind] = (lmac[ind].spu_control1.s.loopbck == 1)/* int lb enabled */
+			&& (lmac[ind].cmr_config.s.enable == 1);  /* Logical MAC/PCS enabled */
+		lb2[ind] = (lmac[ind].smu_ext_loopback.s.en == 1);/* ext lb enabled */
+		lmac_type = lmac[ind].cmr_config.s.lmac_type;
+		mask_xaui |= ((lmac_type != 0) << ind);
+	}
+	PRMcs("SPU: Internal Loopback(loopback1)", mask_xaui, /*cond*/lb1[ind],
+	       "   %8s    ", /*data*/lb1[ind] ? " Enabled" : "Disabled");
+	PRMcs("SMU: External Loopback(en)", mask_xaui, /*cond*/lb2[ind],
+	       "   %8s    ", /*data*/lb2[ind] ? " Enabled" : "Disabled");
+	return 0;
+}
+
+
+/*===================== SGMII mode help funcs =====================*/
+/* The following funcs are implemented in this section */
+int cvmx_helper_bgx_gmp_rx_errors(int node, int bgx, unsigned N);
+int cvmx_helper_bgx_gmp_tx_errors(int node, int bgx, unsigned N);
+int cvmx_helper_bgx_gmp_pcs_errors(int node, int bgx, unsigned N);
+int cvmx_helper_bgx_gmp_loopback(int node, int bgx, unsigned N);
+int cvmx_helper_bgx_gmp_frm_ctl(int node, int bgx, unsigned N);
+
+typedef struct {
+	cvmx_bgxx_cmrx_config_t 		cmr_config;
+	cvmx_bgxx_gmp_gmi_rxx_frm_chk_t		gmp_gmi_rx_frm_chk;
+	cvmx_bgxx_gmp_gmi_rxx_frm_ctl_t		gmp_gmi_rx_frm_ctl;
+	cvmx_bgxx_gmp_gmi_rxx_int_t		gmp_gmi_rx_int;
+	cvmx_bgxx_gmp_pcs_intx_t		gmp_pcs_int;
+} lmac_gmi_rx_errors_t;
+
+/* CN78xx-HM-0.992E 34.4.1 (GMI) Receive Error/Exception Checks */
+int cvmx_helper_bgx_gmp_rx_errors(int node, int bgx, unsigned N)
+{
+	lmac_gmi_rx_errors_t lmac[4];
+	unsigned ind;
+	uint8_t lmac_type, mask_gmii, mask_sgmii = 0, mask_rgmii = 0;;
+
+	for (ind = 0; ind < N; ind++) {
+		lmac[ind].gmp_gmi_rx_frm_chk.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_GMI_RXX_FRM_CHK(ind, bgx));
+		lmac[ind].gmp_gmi_rx_frm_ctl.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_GMI_RXX_FRM_CTL(ind, bgx));
+		lmac[ind].gmp_gmi_rx_int.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_GMI_RXX_INT(ind, bgx));
+		lmac[ind].gmp_pcs_int.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_PCS_INTX(ind, bgx));
+		lmac[ind].cmr_config.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_CONFIG(ind, bgx));
+		lmac_type = lmac[ind].cmr_config.s.lmac_type;
+		mask_sgmii |= ((lmac_type == 0) << ind);
+		mask_rgmii |= ((lmac_type == 5) << ind);
+	}
+	mask_gmii = mask_sgmii || mask_rgmii;
+	PRMcd("GMP:RXERR: Carrier Extend errors(carext)", mask_sgmii,
+		/*cond*/lmac[ind].gmp_gmi_rx_frm_chk.s.carext,
+		"       %d       ", /*data*/lmac[ind].gmp_gmi_rx_int.s.carext);
+	PRMcd("GMP:RXERR: Too long packet(jabber)", mask_gmii,
+		/*cond*/lmac[ind].gmp_gmi_rx_frm_chk.s.jabber,
+		"       %d       ", /*data*/lmac[ind].gmp_gmi_rx_int.s.jabber);
+	PRMcd("GMP:RXERR: Ctrl Frame FCS/CRC(fcserr)", mask_gmii,
+		/*cond*/lmac[ind].gmp_gmi_rx_frm_chk.s.fcserr,
+		"       %d       ", /*data*/lmac[ind].gmp_gmi_rx_int.s.fcserr);
+	PRMcd("GMP:RXERR: Too short packet(minerr)", mask_gmii,
+		/*cond*/lmac[ind].gmp_gmi_rx_frm_chk.s.minerr,
+		"       %d       ", /*data*/lmac[ind].gmp_gmi_rx_int.s.minerr);
+	PRMcd("GMP:RXERR: Data-reception error(rcverr)", mask_gmii,
+		/*cond*/lmac[ind].gmp_gmi_rx_frm_chk.s.rcverr,
+		"       %d       ", /*data*/lmac[ind].gmp_gmi_rx_int.s.rcverr);
+	PRMcd("GMP:RXERR: Not-enough-data(skperr)", mask_gmii,
+		/*cond*/lmac[ind].gmp_gmi_rx_frm_chk.s.skperr,
+		"       %d       ", /*data*/lmac[ind].gmp_gmi_rx_int.s.skperr);
+	PRMcd("GMP:RXERR: Overrun (data come too fast)(ovrerr)", mask_sgmii, /*cond*/1,
+		"       %d       ", /*data*/lmac[ind].gmp_gmi_rx_int.s.ovrerr);
+	PRMcd("GMP:RXERR: Bad Preamble(pcterr)", mask_gmii,
+		/*cond*/lmac[ind].gmp_gmi_rx_frm_ctl.s.pre_chk,
+		"       %d       ", /*data*/lmac[ind].gmp_gmi_rx_int.s.pcterr);
+	PRMcd("GMP:RXERR: Rx a reserv ctrl code group(rsverr)", mask_gmii, /*cond*/1,
+		"       %d       ", /*data*/lmac[ind].gmp_gmi_rx_int.s.rsverr);
+	PRMcd("GMP:RXERR: False carrier detected(falerr)", mask_sgmii, /*cond*/1,
+		"       %d       ", /*data*/lmac[ind].gmp_gmi_rx_int.s.falerr);
+	PRMcd("GMP:RXERR: Collision detect (Half Dupl)(coldet)", mask_gmii, /*cond*/1,
+		"       %d       ", /*data*/lmac[ind].gmp_gmi_rx_int.s.coldet);
+	PRMcd("GMP:RXERR: Small Interframe Gap(ifgerr)", mask_sgmii, /*cond*/1,
+		"       %d       ", /*data*/lmac[ind].gmp_gmi_rx_int.s.ifgerr);
+/* the following field (pause_drp) doesn't exists, but it is listed in 34.4.1 Table34-3 */
+/*	PRMcd("GMP:RXERR: PAUSE RxFIFO FULL DROP(pause_drp)", mask_gmii, 1, */
+/*	       "       %d       ", lmac[ind].gmp_gmi_rx_int.s.pause_drp); */
+	PRMcd("GMP:RXERR: A bad (10-bit) code group(rxerr)", mask_gmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_pcs_int.s.rxerr);
+	PRMcd("GMP:RXERR: Lost sync lock(bit or group)(rxlock)", mask_gmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_pcs_int.s.rxlock);
+	PRMcd("GMP:RXERR: Bad Sync (shouldn't occur)(sync_bad)", mask_gmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_pcs_int.s.sync_bad);
+	PRMcd("GMP:RXERR: Internal Receive error(rxbad)", mask_gmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_pcs_int.s.rxbad);
+	return 0;
+}
+
+
+typedef struct {
+	cvmx_bgxx_cmrx_config_t 		cmr_config;
+	cvmx_bgxx_gmp_gmi_txx_int_t		gmp_gmi_tx_int;
+	cvmx_bgxx_gmp_pcs_intx_t		gmp_pcs_int;
+	cvmx_bgxx_cmrx_int_t			cmr_int;
+	cvmx_bgxx_cmr_bad_t			cmr_bad;
+} lmac_gmp_tx_errors_t;
+
+/* CN78xx-HM-0.992E 34.4.2 (GMP) Transmit Error/Exception Checks */
+int cvmx_helper_bgx_gmp_tx_errors(int node, int bgx, unsigned N)
+{
+	lmac_gmp_tx_errors_t lmac[4];
+	unsigned ind;
+	uint8_t lmac_type, mask_sgmii = 0;
+
+	for (ind = 0; ind < N; ind++) {
+		lmac[ind].gmp_gmi_tx_int.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_GMI_TXX_INT(ind, bgx));
+		lmac[ind].gmp_pcs_int.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_PCS_INTX(ind, bgx));
+		lmac[ind].cmr_int.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_INT(ind, bgx));
+		lmac[ind].cmr_int.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMR_BAD(bgx));
+		lmac[ind].cmr_config.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_CONFIG(ind, bgx));
+		lmac_type = lmac[ind].cmr_config.s.lmac_type;
+		mask_sgmii |= ((lmac_type == 0) << ind);
+	}
+	PRMcd("GMP:TXERR: Wrong channel value(pko_nxc)", mask_sgmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].cmr_int.s.pko_nxc);
+	PRMcd("GMP:TXERR: Packet Transfer Underflow(undflw)", mask_sgmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_gmi_tx_int.s.undflw);
+	PRMcd("GMP:TXERR: Late Collision in Half Dplx(late_col)", mask_sgmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_gmi_tx_int.s.late_col);
+	PRMcd("GMP:TXERR: Excessive Collision Half Dplx(xscol)", mask_sgmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_gmi_tx_int.s.xscol);
+	PRMcd("GMP:TXERR: Excessive Deferral Half Dplx(xsdef)", mask_sgmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_gmi_tx_int.s.xsdef);
+/* the following field (out_ovr) doesn't exists, but it is listed in 34.4.2 Table34-4 */
+/* the following field (loststat) doesn't exists, but it is listed in 34.4.2 Table34-4 */
+/* the following field (statovr) doesn't exists, but it is listed in 34.4.2 Table34-4 */
+/* the following field (inb_nxa) doesn't exists, but it is listed in 34.4.2 Table34-4 */
+	PRMcd("GMP:TXERR: Tx Internal Error(txbad)", mask_sgmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_pcs_int.s.txbad);
+	PRMcd("GMP:TXERR: Tx Internal Error(txfifo)", mask_sgmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_pcs_int.s.txfifo);
+	PRMcd("GMP:TXERR: Tx Internal Error(txfifu)", mask_sgmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_pcs_int.s.txfifu);
+	return 0;
+}
+
+
+typedef struct {
+	cvmx_bgxx_cmrx_config_t 		cmr_config;
+	cvmx_bgxx_gmp_pcs_intx_t		gmp_pcs_int;
+	cvmx_bgxx_gmp_pcs_mrx_status_t		gmp_pcs_mr_status;
+} lmac_gmp_pcs_errors_t;
+
+/* CN78xx-HM-0.992E 34.4.4 (GMP) PCS Error/Exception Checks */
+int cvmx_helper_bgx_gmp_pcs_errors(int node, int bgx, unsigned N)
+{
+	lmac_gmp_pcs_errors_t lmac[4];
+	unsigned ind;
+	uint8_t lmac_type, mask_sgmii = 0;
+
+	for (ind = 0; ind < N; ind++) {
+		lmac[ind].gmp_pcs_int.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_PCS_INTX(ind, bgx));
+		lmac[ind].gmp_pcs_mr_status.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_PCS_MRX_STATUS(ind, bgx));
+		lmac[ind].cmr_config.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_CONFIG(ind, bgx));
+		lmac_type = lmac[ind].cmr_config.s.lmac_type;
+		mask_sgmii |= ((lmac_type == 0) << ind);
+	}
+	PRMcd("GMP:PCSERR: ANEG didn't found match mode(an_err)", mask_sgmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_pcs_int.s.an_err);
+	PRMcd("GMP:PCSERR: Remote fault received(rm_flt)", mask_sgmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_pcs_mr_status.s.rm_flt);
+	PRMcd("GMP:PCSERR: The xmit variable changes(xmit)", mask_sgmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_pcs_int.s.xmit);
+	PRMcd("GMP:PCSERR: Link Speed mode change(lnkspd)", mask_sgmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_pcs_int.s.lnkspd);
+	PRMcd("GMP:PCSERR: Duplex mode changes(dup)", mask_sgmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_pcs_int.s.dup);
+	PRMcd("GMP:PCSERR: ANEG state machine fault(an_bad)", mask_sgmii, /*cond*/1,
+	       "       %d       ", /*data*/lmac[ind].gmp_pcs_int.s.an_bad);
+	return 0;
+}
+
+
+typedef struct {
+	cvmx_bgxx_cmrx_config_t 		cmr_config;
+	cvmx_bgxx_gmp_pcs_mrx_control_t		gmp_pcs_mr_control;
+	cvmx_bgxx_gmp_pcs_miscx_ctl_t		gmp_pcs_misc_ctl;
+} lmac_gmp_loopback_t;
+
+/* CN78xx-HM-0.992E 34.7 (GMP) Loopback */
+int cvmx_helper_bgx_gmp_loopback(int node, int bgx, unsigned N)
+{
+	lmac_gmp_loopback_t lmac[4];
+	unsigned ind;
+	bool lb1[4]/*internal loopback*/, lb2[4]/* external loopback*/;
+	uint8_t lmac_type, mask_sgmii = 0;
+
+	for (ind = 0; ind < N; ind++) {
+		lmac[ind].cmr_config.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_CONFIG(ind, bgx));
+		lmac[ind].gmp_pcs_mr_control.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_PCS_MRX_CONTROL(ind, bgx));
+		lmac[ind].gmp_pcs_misc_ctl.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_PCS_MISCX_CTL(ind, bgx));
+
+		lb1[ind] = (lmac[ind].gmp_pcs_mr_control.s.loopbck1 == 1)/* int lb enabled */
+			&& (lmac[ind].cmr_config.s.enable == 1) 	/* Logical MAC/PCS enabled */
+			&& (lmac[ind].gmp_pcs_misc_ctl.s.gmxeno == 0);	/* GMI enable override */
+		lb2[ind] = (lmac[ind].gmp_pcs_misc_ctl.s.loopbck2 == 1);/* ext lb enabled */
+		lmac_type = lmac[ind].cmr_config.s.lmac_type;
+		mask_sgmii |= ((lmac_type == 0) << ind);
+	}
+	PRMcs("GMP: Internal Loopback(loopbck1)", mask_sgmii, /*cond*/lb1[ind],
+	       "   %8s    ", /*data*/lb1[ind] ? " Enabled" : "Disabled");
+	PRMcs("GMP: External Loopback(loopbck2)", mask_sgmii, /*cond*/lb2[ind],
+	       "   %8s    ", /*data*/lb2[ind] ? " Enabled" : "Disabled");
+	return 0;
+}
+
+typedef struct {
+	cvmx_bgxx_cmrx_config_t 		cmr_config;
+	cvmx_bgxx_gmp_gmi_rxx_frm_ctl_t		gmp_gmi_rx_frm_ctl;
+} lmac_gmp_frm_ctl_t;
+
+/* Print out the BGX Frame Control Configuration (which frame checvhs are enabled */
+int cvmx_helper_bgx_gmp_frm_ctl(int node, int bgx, unsigned N)
+{
+	lmac_gmp_frm_ctl_t lmac[4];
+	unsigned ind;
+	uint8_t lmac_type, mask_sgmii = 0;
+
+	for (ind = 0; ind < N; ind++) {
+		lmac[ind].cmr_config.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_CONFIG(ind, bgx));
+		lmac[ind].gmp_gmi_rx_frm_ctl.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_GMI_RXX_FRM_CTL(ind, bgx));
+
+		lmac_type = lmac[ind].cmr_config.s.lmac_type;
+		mask_sgmii |= ((lmac_type == 0) << ind);
+	}
+	PRMns("GMP:FRM_CTL: Timestamp Enable(ptp_mode)",
+		mask_sgmii, "   %8s    ",
+		lmac[ind].gmp_gmi_rx_frm_ctl.s.ptp_mode ? " Enabled" : "Disabled");
+	PRMns("GMP:FRM_CTL: Don't modify MODbits(null_dis)",
+		mask_sgmii, "   %8s    ",
+		lmac[ind].gmp_gmi_rx_frm_ctl.s.null_dis ? " Enabled" : "Disabled");
+	PRMns("GMP:FRM_CTL: Aligns the SFD byte(pre_align)",
+		mask_sgmii, "   %8s    ",
+		lmac[ind].gmp_gmi_rx_frm_ctl.s.pre_align ? " Enabled" : "Disabled");
+	PRMns("GMP:FRM_CTL: Begin frame at first SFD(pre_free)",
+		mask_sgmii, "   %8s    ",
+		lmac[ind].gmp_gmi_rx_frm_ctl.s.pre_free ? " Enabled" : "Disabled");
+	PRMns("GMP:FRM_CTL: PAUSE frm can match SMAC(ctl_smac)",
+		mask_sgmii, "   %8s    ",
+		lmac[ind].gmp_gmi_rx_frm_ctl.s.ctl_smac ? " Enabled" : "Disabled");
+	PRMns("GMP:FRM_CTL: PAUSE frm can match MCAST(ctl_mcst)",
+		mask_sgmii, "   %8s    ",
+		lmac[ind].gmp_gmi_rx_frm_ctl.s.ctl_mcst ? " Enabled" : "Disabled");
+	PRMns("GMP:FRM_CTL: Fwd PAUSE info to TX block(ctl_bck)",
+		mask_sgmii, "   %8s    ",
+		lmac[ind].gmp_gmi_rx_frm_ctl.s.ctl_bck ? " Enabled" : "Disabled");
+	PRMns("GMP:FRM_CTL: Drop control-PAUSE frames(ctl_drp)",
+		mask_sgmii, "   %8s    ",
+		lmac[ind].gmp_gmi_rx_frm_ctl.s.ctl_drp ? " Enabled" : "Disabled");
+	PRMns("GMP:FRM_CTL: Strip off the preamble (pre_strp)",
+		mask_sgmii, "   %8s    ",
+		lmac[ind].gmp_gmi_rx_frm_ctl.s.pre_strp ? " Enabled" : "Disabled");
+	PRMns("GMP:FRM_CTL: Check preamble is correct(pre_chk)",
+		mask_sgmii, "   %8s    ",
+		lmac[ind].gmp_gmi_rx_frm_ctl.s.pre_chk ? " Enabled" : "Disabled");
+	return 0;
+}
+
+
+
+/*===================== BGX CONFIG Dump func =====================*/
+/* The following funcs are implemented in this section */
+int cvmx_dump_bgx_config(unsigned bgx);
+int cvmx_dump_bgx_config_node(unsigned node, unsigned bgx);
+
+typedef struct {
+	cvmx_bgxx_cmrx_config_t 	cmr_config;
+	cvmx_bgxx_cmrx_rx_id_map_t 	cmr_rx_id_map;
+	cvmx_bgxx_cmrx_rx_bp_drop_t 	cmr_rx_bp_drop;
+	cvmx_bgxx_cmrx_rx_bp_on_t 	cmr_rx_bp_on;
+	cvmx_bgxx_cmrx_rx_bp_off_t 	cmr_rx_bp_off;
+	cvmx_bgxx_cmrx_rx_weight_t 	cmr_rx_weight;
+	cvmx_bgxx_cmrx_rx_adr_ctl_t 	cmr_rx_adr_ctl;
+	cvmx_bgxx_cmrx_rx_logl_xoff_t 	cmr_rx_logl_xoff;
+	cvmx_bgxx_cmrx_rx_logl_xon_t 	cmr_rx_logl_xon;
+	cvmx_bgxx_cmrx_tx_channel_t 	cmr_tx_channel;
+	cvmx_bgxx_spux_an_control_t 	spu_an_control;
+	cvmx_bgxx_spux_an_adv_t 	spu_an_adv;
+	cvmx_bgxx_smux_cbfc_ctl_t 	smu_cbfc_ctl;
+	cvmx_bgxx_smux_tx_ctl_t 	smu_tx_ctl;
+	cvmx_bgxx_smux_rx_udd_skp_t	smu_rx_udd_skp;
+	cvmx_bgxx_smux_tx_thresh_t 	smu_tx_thresh;
+	cvmx_bgxx_smux_hg2_control_t 	smu_hg2_control;
+	cvmx_helper_link_info_t 	link_info;
+	cvmx_bgxx_gmp_pcs_miscx_ctl_t	gmp_pcs_misc_ctl;
+	cvmx_bgxx_gmp_pcs_mrx_control_t	gmp_pcs_mr_control;
+	cvmx_bgxx_gmp_pcs_anx_results_t gmp_pcs_an_results;
+	cvmx_bgxx_gmp_pcs_mrx_status_t	gmp_pcs_mr_status;
+	cvmx_bgxx_gmp_pcs_rxx_sync_t	gmp_pcs_rx_sync;
+	cvmx_bgxx_gmp_gmi_rxx_frm_ctl_t	gmp_gmi_rx_frm_ctl;
+	cvmx_bgxx_gmp_gmi_prtx_cfg_t	gmp_gmi_prt_cfg;
+} lmac_config_t;
+
+int cvmx_dump_bgx_config_node(unsigned node, unsigned bgx)
+{
+	lmac_config_t lmac[4];
+	cvmx_bgxx_cmr_global_config_t global_config;
+	cvmx_bgxx_cmr_rx_ovr_bp_t cmr_rx_ovr_bp;
+	unsigned ind, N;
+	uint8_t mask_aneg_ovrd, mask_aneg, lmac_type, lmac_gmii;
+	uint8_t lmac_sgmii, lmac_rgmii, lmac_xaui, lmac_rxaui, lmac_10g_r, lmac_40g_r;
+	int ipd_port, qlm, gbaud_mhz;
+
+	lmac_sgmii = lmac_rgmii = lmac_xaui = lmac_rxaui = lmac_10g_r  = lmac_40g_r = 0;
+	mask_aneg_ovrd = mask_aneg = 0;
+
+	global_config.u64 = cvmx_read_csr_node(node,
+		CVMX_BGXX_CMR_GLOBAL_CONFIG(bgx));
+
+	cvmx_dprintf("\n/*===== BGX CONFIG Parameters			BGX%d =====*/\n", bgx);
+	/* just report configured RX/Tx LMACS - don't check return */
+	cvmx_helper_bgx_number_rx_tx_lmacs(node, bgx, &N);
+
+	qlm = bgx<2 ? (global_config.s.pmux_sds_sel==1 ? bgx+2 : bgx) : bgx+2;
+	cvmx_dprintf("NODE%d: BGX%d/lmac[0..%d] connected to QLM%d\n",
+		node, bgx, N - 1, qlm);
+
+	for (ind = 0; ind < N; ind++) {
+
+		lmac[ind].cmr_config.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_CONFIG(ind, bgx));
+		lmac[ind].cmr_rx_id_map.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_ID_MAP(ind, bgx));
+		ipd_port = cvmx_helper_get_ipd_port(bgx, ind);
+		lmac[ind].link_info = cvmx_helper_link_get(ipd_port);
+		lmac[ind].spu_an_control.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SPUX_AN_CONTROL(ind, bgx));
+		lmac[ind].spu_an_adv.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SPUX_AN_ADV(ind, bgx));
+		lmac[ind].cmr_rx_bp_drop.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_BP_DROP(ind, bgx));
+		lmac[ind].cmr_rx_bp_on.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_BP_ON(ind, bgx));
+		lmac[ind].cmr_rx_bp_off.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_BP_OFF(ind, bgx));
+		lmac[ind].cmr_rx_weight.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_WEIGHT(ind, bgx));
+		lmac[ind].cmr_rx_adr_ctl.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_ADR_CTL(ind, bgx));
+		lmac[ind].cmr_rx_logl_xoff.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_LOGL_XOFF(ind, bgx));
+		lmac[ind].cmr_rx_logl_xon.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_LOGL_XON(ind, bgx));
+		lmac[ind].cmr_tx_channel.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_CHANNEL(ind, bgx));
+		lmac[ind].gmp_pcs_misc_ctl.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_PCS_MISCX_CTL(ind, bgx));
+		lmac[ind].gmp_pcs_mr_control.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_PCS_MRX_CONTROL(ind, bgx));
+		lmac[ind].gmp_pcs_an_results.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_PCS_ANX_RESULTS(ind, bgx));
+		lmac[ind].gmp_pcs_mr_status.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_PCS_MRX_STATUS(ind, bgx));
+		lmac[ind].gmp_gmi_rx_frm_ctl.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_GMI_RXX_FRM_CTL(ind, bgx));
+		lmac[ind].gmp_pcs_rx_sync.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_PCS_RXX_SYNC(ind, bgx));
+		lmac[ind].smu_cbfc_ctl.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SMUX_CBFC_CTL(ind, bgx));
+		lmac[ind].smu_tx_ctl.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SMUX_TX_CTL(ind, bgx));
+		lmac[ind].smu_rx_udd_skp.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SMUX_RX_UDD_SKP(ind, bgx));
+		lmac[ind].smu_tx_thresh.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SMUX_TX_THRESH(ind, bgx));
+		lmac[ind].smu_hg2_control.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_SMUX_HG2_CONTROL(ind, bgx));
+		lmac[ind].gmp_gmi_prt_cfg.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_GMP_GMI_PRTX_CFG(ind, bgx));
+
+		lmac_type = lmac[ind].cmr_config.s.lmac_type;
+		lmac_sgmii |= ((lmac_type == 0) << ind);
+		lmac_xaui |= ((lmac_type == 1) << ind);
+		lmac_rxaui |= ((lmac_type == 2) << ind);
+		lmac_10g_r |= ((lmac_type == 3) << ind);
+		lmac_40g_r |= ((lmac_type == 4) << ind);
+		lmac_rgmii |= ((lmac_type == 5) << ind);
+
+		if (lmac[ind].gmp_pcs_misc_ctl.s.an_ovrd /* ANEG Overrided */
+		    || (lmac[ind].gmp_pcs_mr_control.s.an_en == 0) ) /*ANEG disabled */
+			mask_aneg_ovrd |= (1 << ind);
+		if ( (lmac[ind].gmp_pcs_mr_control.s.an_en == 1) /* ANEG Enabled */
+/* NOTE: Do not check 'ANEG complete', just report what is when (ANEG_Enabled && !Overrided) */
+/*			&& (lmac[ind].gmp_pcs_mr_status.s.an_cpt == 1)*/ /* ANEG complete */
+			&& (lmac[ind].gmp_pcs_misc_ctl.s.an_ovrd == 0) )/* not overrided */
+			mask_aneg |= (1 << ind);
+	}
+
+	PRn("LMAC:", N, "     lmac%d     ", ind);
+	PRns("Type", N, "   %-10s  ",
+		get_lmac_type_name(lmac[ind].cmr_config.s.lmac_type,
+				   lmac[ind].gmp_pcs_misc_ctl.s.mode) );
+	gbaud_mhz = cvmx_qlm_get_gbaud_mhz(qlm);
+	PRn("QLM Baud Rate[MHz]", N, "     %5d     ",
+	       gbaud_mhz);/* NOTE: OK - same rate for all LMACs */
+	PRns("Master LMAC (Logical MAC/PCS) Enabled?(enable)", N, "   %8s    ",
+	       lmac[ind].cmr_config.s.enable ? " Enabled" : "Disabled");
+	PRn("Num PCS Lanes", N, "       %d       ",
+	       get_num_pcs_lanes(lmac[ind].cmr_config.s.lmac_type));
+	PRns("Binded to QLM lane(s)", N, "   %9s   ",
+		get_bind_lanes_per_lmac(ind, lmac[ind].cmr_config.s.lmac_type,
+				lmac[ind].cmr_config.s.lane_to_sds) );
+	PRns("Mix Enabled?(mix_en)", N, "   %8s   ",
+	       lmac[ind].cmr_config.s.mix_en ? " Enabled" : "Disabled");
+	PRn("Port kind (pknd)", N, "       %d       ",
+	       (lmac[ind].cmr_rx_id_map.s.pknd & 0x3F));
+
+	cvmx_dprintf("/*=== Frame Check Control, Loopback, ANEG Parameters ===*/\n");
+	lmac_gmii = lmac_sgmii || lmac_rgmii;
+	if (lmac_sgmii/*SGMII/1000BASE-X*/ || lmac_rgmii/*RGMII*/) {
+		PRcs("GMP: Power Down?(pwr_dn)", /* display only if in 'Power Down' */
+			N, /*cond*/lmac[ind].gmp_pcs_mr_control.s.pwr_dn,
+			"  %10s   ",
+			lmac[ind].gmp_pcs_mr_control.s.pwr_dn
+				? "Power Down" : " Power On "/*never displayed*/);
+		PRMcs("GMP: PCS acts as (mac_phy)",	      lmac_sgmii,
+		       (lmac[ind].gmp_pcs_misc_ctl.s.mode == 0) /* SGMII */,
+		       "      %3s      ",
+		       lmac[ind].gmp_pcs_misc_ctl.s.mac_phy == 0 ? "MAC" : "PHY");
+		cvmx_helper_bgx_gmp_frm_ctl(node, bgx, N);
+		if (mask_aneg_ovrd != 0) { /* ANEG is overrided */
+#undef 			spd
+#define 		spd  (lmac[ind].gmp_pcs_mr_control.s.spdmsb << 1 \
+				| lmac[ind].gmp_pcs_mr_control.s.spdlsb)
+			PRMns("GMP: SPEED  (Overrided)", mask_aneg_ovrd, "   %9s   ",
+				spd == 0 ? "  10 Mb/s" :
+				spd == 1 ? " 100 Mb/s" :
+				spd == 2 ? "1000 Mb/s" : "   ???   ");
+			PRMns("GMP: DUPLEX (Overrided)", mask_aneg_ovrd,
+				"      %4s     ",
+				lmac[ind].gmp_pcs_mr_control.s.dup ? "Full" : "Half");
+		}
+		if (mask_aneg != 0) { /* SPEED and DUPLEX from ANEG */
+			//mask_aneg = 0xf;
+			PRMns("GMP: ANEG Enabled?(an_en)",
+				mask_aneg, "   %8s    ",
+				lmac[ind].gmp_pcs_mr_control.s.an_en
+					? " Enabled" : "Disabled");
+			PRMcs("GMP: ANEG(Enabled) Completed?(an_cpt)",
+				mask_aneg,
+				/*cond*/lmac[ind].gmp_pcs_mr_control.s.an_en,
+				"      %3s      ",
+				lmac[ind].gmp_pcs_mr_status.s.an_cpt ? "Yes" : " No");
+#undef 			spd
+#define			spd  (lmac[ind].gmp_pcs_an_results.s.spd)
+			PRMcs("GMP: SPEED  -from ANEG(Enabled && !Overrided)",
+				mask_aneg,
+				/*cond*/lmac[ind].gmp_pcs_mr_control.s.an_en,
+				"   %9s   ",
+				spd == 0 ? "  10 Mb/s" :
+				spd == 1 ? " 100 Mb/s" :
+				spd == 2 ? "1000 Mb/s" : "   ???   ");
+			PRMcs("GMP: DUPLEX -from ANEG(Enabled && !Overrided)",
+				mask_aneg,
+				/*cond*/lmac[ind].gmp_pcs_mr_control.s.an_en,
+				"      %4s     ",
+				lmac[ind].gmp_pcs_an_results.s.dup ? "Full" : "Half");
+		}
+		PRMns("GMP: PCS_Link Status(lnk_st)",lmac_gmii, "      %4s     ",
+			 lmac[ind].gmp_pcs_mr_status.s.lnk_st ? " Up " : "Down");
+		PRMns("GMP: PCS_RX_SYNC Status(sync)",lmac_gmii, "    %8s   ",
+			 lmac[ind].gmp_pcs_rx_sync.s.sync ? "  Sync  " : "NOT Sync");
+		/* skip reporting polarity of serdes lines for now - low priority */
+		/* 34.3.1.1 Receive Flow Control */
+#undef 		rx_flow_ctl
+#define		rx_flow_ctl (lmac[ind].gmp_gmi_rx_frm_ctl.s.ctl_bck << 1\
+				| lmac[ind].gmp_gmi_rx_frm_ctl.s.ctl_drp)
+		PRMns("GMP: Receive_Flow_Control (PAUSE pkt)", lmac_gmii, "  %12s ",
+		        rx_flow_ctl == 1 /* bck=0 & drp=1*/ ? "  Ingnored  " :
+		        rx_flow_ctl == 0 /* bck=0 & drp=0*/ ? " SW process " :
+		        rx_flow_ctl == 2 /* bck=1 & drp=0*/ ? " send PAUSE " :
+		        rx_flow_ctl == 3 /* bck=1 & drp=1*/ ? " HW process " :
+		        "            ");
+	}
+
+	if (lmac_sgmii != 0/*SGMII/1000BASE-X*/) {
+
+		PRMns("GMP: PCS_MODE(mode)",	      lmac_sgmii, "  %10s  ",
+		       lmac[ind].gmp_pcs_misc_ctl.s.mode == 0
+			? "   SGMII   " : "1000BASE-X");
+		cvmx_helper_bgx_gmp_loopback(node, bgx, N);
+
+		PRMd("GMP: Unidirectional (overwrite ANEG)", lmac_sgmii,
+			"       %d       ", lmac[ind].gmp_pcs_mr_control.s.uni);
+
+		PRMns("GMP:Duplex mode (SGMII/1000Base-X only)(duplex)", lmac_sgmii,
+			"  %11s  ",
+			lmac[ind].gmp_gmi_prt_cfg.s.duplex ? "Full-duplex" : "Half-duplex");
+#undef		spd
+#define		spd 	(lmac[ind].gmp_gmi_prt_cfg.s.speed_msb << 1 \
+			| lmac[ind].gmp_gmi_prt_cfg.s.speed)
+		PRMns("GMP:Speed (SGMII/1000Base-X only)(duplex)", lmac_sgmii,
+			"   %9s   ",
+			spd == 0 ? " 100 Mb/s" :
+			spd == 1 ? "1000 Mb/s" :
+			spd == 2 ? "  10 Mb/s" : "Reserved ");
+	}
+
+	if (lmac_rxaui != 0/*RXAUI*/) { /*ANEG not defined for RXAUI */
+		cvmx_helper_bgx_spu_loopback(node, bgx, N);
+
+		PRns("ANEG (not defined for RXAUI)", N, "  %11s  ", "not defined");
+	}
+	if (lmac_xaui || lmac_10g_r || lmac_40g_r) {
+		/* for all modes, but [sgmii, rxaui] */
+		uint8_t mask_134 = lmac_xaui | lmac_10g_r | lmac_40g_r;
+
+		cvmx_helper_bgx_spu_loopback(node, bgx, N);
+
+		PRMns("ANEG Enabled?(an_en)",	       mask_134,"   %8s    ",
+		       lmac[ind].spu_an_control.s.an_en  ? " Enabled" : "Disabled");
+		PRMd("ANEG_ADV: FEC req(fec_req)",    mask_134,"       %d       ",
+		       lmac[ind].spu_an_adv.s.fec_req);
+		PRMd("ANEG_ADV: FEC able(fec_able)",   mask_134,"       %d       ",
+		       lmac[ind].spu_an_adv.s.fec_able);
+		PRMd("ANEG_ADV: 40GBASE_CR4(a40g_cr4)",mask_134,"       %d       ",
+		       lmac[ind].spu_an_adv.s.a40g_cr4);
+		PRMd("ANEG_ADV: 40GBASE_KR4(a40g_kr4)",mask_134,"       %d       ",
+		       lmac[ind].spu_an_adv.s.a40g_kr4);
+		PRMd("ANEG_ADV: 10GBASE_KR(a10g_kr)", mask_134,"       %d       ",
+		       lmac[ind].spu_an_adv.s.a10g_kr);
+		PRMd("ANEG_ADV: 10GBASE_KX4(a10g_kx4)",mask_134,"       %d       ",
+		       lmac[ind].spu_an_adv.s.a10g_kx4);
+		PRMd("ANEG_ADV: 1000BASE_KX(a1g_kx)",mask_134,"       %d       ",
+		       lmac[ind].spu_an_adv.s.a1g_kx);
+		PRMd("ANEG_ADV: Asym PAUSE(asm_dir)", mask_134,"       %d       ",
+		       lmac[ind].spu_an_adv.s.asm_dir);
+		PRMd("ANEG_ADV: PAUSE able(pause)", mask_134,"       %d       ",
+		       lmac[ind].spu_an_adv.s.pause);
+	}
+
+
+	cvmx_dprintf("/*=== BGX MISC CONFIG Parameters ===*/\n");
+	PRns("Link Status (from (external) PHY ANEG)[SE API]", N,
+	     "      %4s     ", lmac[ind].link_info.s.link_up ? " Up " : "Down");
+	PRns("Link Duplex [SE API]",		N, "      %4s     ",
+		lmac[ind].link_info.s.full_duplex ? "Full" : "Half");
+	PRn("Link Speed (MBps) [SE API]",	N, "      %5d    ",
+	       lmac[ind].link_info.s.speed);
+	PRns("Data Packet Receive (data_pkt_rx_en)", N, "   %8s    ",
+	       lmac[ind].cmr_config.s.data_pkt_rx_en ? " Enabled" : "Disabled");
+	PRns("Data Packet Transmit(data_pkt_tx_en)", N, "   %8s    ",
+	       lmac[ind].cmr_config.s.data_pkt_tx_en ? " Enabled" : "Disabled");
+
+	cvmx_dprintf("/*=== Flow Control ===*/\n");
+	cmr_rx_ovr_bp.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMR_RX_OVR_BP(bgx));
+	PRns("Backpressure Override Enabled(en)", N,
+		"       %3s     ",
+		(cmr_rx_ovr_bp.s.en & (1<<ind)) ? "Yes" : " No");
+	PRn("Transmit BP channel mask(msk:16bits)", N,
+	"     0x%04x    ", lmac[ind].cmr_tx_channel.s.msk);
+	PRn("Transmit Credit return BP ch. mask(dis:16bits)", N,
+	"     0x%04x    ", lmac[ind].cmr_tx_channel.s.dis);
+	PRn("BP Status[(O)/LMAC avail, (1)/should be BP](bp)", N,
+		"        %d      ", (cmr_rx_ovr_bp.s.bp & (1<<ind)) ? 1 : 0);
+	PRns("Ignore RX FIFO BP_ON signal for BP(ign_fifo_bp)", N,
+		"       %3s     ",
+		(cmr_rx_ovr_bp.s.ign_fifo_bp & (1<<ind)) ? "Yes" : " No");
+	PRn("Rx BP Drop level[8 byte cycles](mark)", N, "      %3d      ",
+		lmac[ind].cmr_rx_bp_drop.s.mark);
+	PRn("Rx BP ON(high) level[bytes](mark)", 	N, "     %5d     ",
+		lmac[ind].cmr_rx_bp_on.s.mark<<4);
+	PRn("Rx BP OFF(low) level[bytes](mark)", 	N, "     %5d     ",
+		lmac[ind].cmr_rx_bp_off.s.mark<<4);
+	PRn("Rx RoundRobin WEIGHT(weight)", 	N, "      %3d      ",
+		lmac[ind].cmr_rx_weight.s.weight);
+	PRns("ADR_CTL: On DMAC CAM address match(cam_accept)",
+		N, "    %7s    ",
+		lmac[ind].cmr_rx_adr_ctl.s.cam_accept == 0 ? " Reject" : " Accept");
+	PRns("ADR_CTL: MCST_MODE(mcst_mode)", N, " %13s ",
+		lmac[ind].cmr_rx_adr_ctl.s.mcst_mode == 0 ? "  Reject All " :
+		lmac[ind].cmr_rx_adr_ctl.s.mcst_mode == 1 ? "  Accept All " :
+		lmac[ind].cmr_rx_adr_ctl.s.mcst_mode == 2 ? "AddrFilterCAM" :
+		"     ???     ");
+	PRns("ADR_CTL: BCST_ACCEPT(bcst_accept)", N, "    %7s    ",
+		lmac[ind].cmr_rx_adr_ctl.s.bcst_accept == 0 ? " Reject" : " Accept");
+	PRn("Channel BP: XOFF(cmr_rx_logl_xoff:16bits)", N,
+	       "     0x%04x    ", lmac[ind].cmr_rx_logl_xoff.s.xoff);
+	PRn("Channel BP: XON (cmr_rx_logl_xon: 16bits)", N,
+	       "     0x%04x    ", lmac[ind].cmr_rx_logl_xon.s.xon);
+	if (lmac_xaui || lmac_rxaui || lmac_10g_r || lmac_40g_r) {
+		/* for all modes, but sgmii, rgmii */
+		uint8_t mask_1234 = lmac_xaui | lmac_rxaui | lmac_10g_r | lmac_40g_r;
+
+		PRMn("SMU: Physical BP Enable(phys_en: 16bits)", mask_1234,
+		"     0x%04x    ", lmac[ind].smu_cbfc_ctl.s.phys_en);
+		PRMn("SMU: Logical  BP Enable(logl_en: 16bits)", mask_1234,
+		"     0x%04x    ", lmac[ind].smu_cbfc_ctl.s.logl_en);
+		PRMns("SMU: Forward PFC/CBFC PAUSE to BP block(bck_en)", mask_1234,
+			"      %3s      ",
+			lmac[ind].smu_cbfc_ctl.s.bck_en ? "Yes" : " No");
+		PRMns("SMU: Drop PFC/CBFC PAUSE frames(drp_en)", mask_1234,
+			"      %3s      ",
+			lmac[ind].smu_cbfc_ctl.s.drp_en ? "Yes" : " No");
+		PRMns("SMU: Transmit PFC/CBFC PAUSE packets (tx_en)", mask_1234,
+			"      %3s      ",
+			lmac[ind].smu_cbfc_ctl.s.tx_en ? "Yes" : " No");
+		PRMns("SMU: Receive  PFC/CBFC PAUSE packets (rx_en)", mask_1234,
+			"      %3s      ",
+			lmac[ind].smu_cbfc_ctl.s.rx_en ? "Yes" : " No");
+		PRMn("SMU: 40GBASE-R TX marker interval cnt(spu_mrk_cnt)", mask_1234,
+			"    %7d    ", lmac[ind].smu_tx_ctl.s.spu_mrk_cnt);
+
+		PRMns("SMU: Gen. PAUSE when CMR do XOFF(l2p_bp_conv)", mask_1234,
+			"      %3s      ",
+			lmac[ind].smu_tx_ctl.s.l2p_bp_conv ? "Yes" : " No");
+		PRMns("SMU: Bypass RX status (set to LS)(ls_byp)", mask_1234,
+			"      %3s      ",
+			lmac[ind].smu_tx_ctl.s.ls_byp ? "Yes" : " No");
+		PRMns("SMU: Unidirectional Mode Enable(uni_en)", mask_1234,
+			"   %8s    ",
+			lmac[ind].smu_tx_ctl.s.uni_en ? " Enabled" : "Disabled");
+		PRMns("SMU: Deficit Idle Counter Enable(dic_en)", mask_1234,
+			"   %8s    ",
+			lmac[ind].smu_tx_ctl.s.dic_en ? " Enabled" : "Disabled");
+		PRMn("SMU: TX Thresh level[128-bits word](cnt)", mask_1234,
+			"      %4d     ", lmac[ind].smu_tx_thresh.s.cnt);
+		/* HiGig2 config - begin */
+		/* it is enabled for all modes now (mask_1234),
+		 * but probably must be XAUI only? (lmac_xaui) - wikipedia
+		 * NOTE: HiGig2 configuration will be printed only if 'Enabled'
+		 */
+		PRMns("SMU: HiGig/HiGig2 mode Enabled?(hg_en)", mask_1234,
+			"     %6s    ",
+			lmac[ind].smu_tx_ctl.s.hg_en == 0 ? "  No   " :
+			lmac[ind].smu_rx_udd_skp.s.len == 12 ? "HiGig " : "HiGig2");
+		PRMcs("SMU: HiGig2 msg transmission(hg2tx_en)", mask_1234,
+			/*cond*/(lmac[ind].smu_tx_ctl.s.hg_en
+				&& lmac[ind].smu_rx_udd_skp.s.len == 16),
+			"   %8s    ",
+			lmac[ind].smu_hg2_control.s.hg2tx_en ? " Enabled" : "Disabled");
+		PRMcs("SMU: HiGig2 msg receive/process(hg2rx_en)", mask_1234,
+			/*cond*/(lmac[ind].smu_tx_ctl.s.hg_en
+				&& lmac[ind].smu_rx_udd_skp.s.len == 16),
+			"   %8s    ",
+			lmac[ind].smu_hg2_control.s.hg2rx_en ? " Enabled" : "Disabled");
+		PRMcs("SMU: HiGig2 Physical-link PAUSE(phys_en)", mask_1234,
+			/*cond*/(lmac[ind].smu_tx_ctl.s.hg_en
+				&& lmac[ind].smu_rx_udd_skp.s.len == 16),
+			"   %8s    ",
+			lmac[ind].smu_hg2_control.s.phys_en ? " Enabled" : "Disabled");
+		PRMc("SMU: 16-bit XOF enables(logl_en)", mask_1234,
+			/*cond*/(lmac[ind].smu_tx_ctl.s.hg_en
+				&& lmac[ind].smu_rx_udd_skp.s.len == 16),
+			"     0x%04x    ", lmac[ind].smu_hg2_control.s.logl_en);
+		/* HiGig2 config - end */
+	}
+	if (lmac[0/*LMAC0*/].cmr_config.s.lmac_type == 5/*RGMII*/)
+		cvmx_helper_bgx_rgmii_config(node, bgx, 1);
+
+	return 0;
 }
+
+
+/*===================== BGX CONFIG Dump func =====================*/
+/* The following funcs are implemented in this section */
+int cvmx_dump_bgx_status(unsigned bgx);
+int cvmx_dump_bgx_status_node(unsigned node, unsigned bgx);
+
+/**
+ * Dump status of BGX ports
+ */
+typedef struct {
+	cvmx_bgxx_cmrx_config_t 	cmr_config;
+	cvmx_bgxx_cmrx_rx_stat0_t 	rx_stat0;
+	cvmx_bgxx_cmrx_rx_stat1_t 	rx_stat1;
+	cvmx_bgxx_cmrx_rx_stat2_t 	rx_stat2;
+	cvmx_bgxx_cmrx_rx_stat2_t 	rx_stat3;
+	cvmx_bgxx_cmrx_rx_stat2_t 	rx_stat4;
+	cvmx_bgxx_cmrx_rx_stat2_t 	rx_stat5;
+	cvmx_bgxx_cmrx_rx_stat2_t 	rx_stat6;
+	cvmx_bgxx_cmrx_rx_stat2_t 	rx_stat7;
+	cvmx_bgxx_cmrx_rx_stat8_t 	rx_stat8;
+	cvmx_bgxx_cmrx_rx_bp_status_t 	rx_bp_status;
+	cvmx_helper_link_info_t 	link_info;
+	cvmx_bgxx_cmrx_tx_stat0_t 	tx_stat0;
+	cvmx_bgxx_cmrx_tx_stat1_t 	tx_stat1;
+	cvmx_bgxx_cmrx_tx_stat2_t 	tx_stat2;
+	cvmx_bgxx_cmrx_tx_stat3_t 	tx_stat3;
+	cvmx_bgxx_cmrx_tx_stat4_t 	tx_stat4;
+	cvmx_bgxx_cmrx_tx_stat5_t 	tx_stat5;
+	cvmx_bgxx_cmrx_tx_stat6_t 	tx_stat6;
+	cvmx_bgxx_cmrx_tx_stat7_t 	tx_stat7;
+	cvmx_bgxx_cmrx_tx_stat8_t 	tx_stat8;
+	cvmx_bgxx_cmrx_tx_stat9_t 	tx_stat9;
+	cvmx_bgxx_cmrx_tx_stat10_t 	tx_stat10;
+	cvmx_bgxx_cmrx_tx_stat11_t 	tx_stat11;
+	cvmx_bgxx_cmrx_tx_stat12_t 	tx_stat12;
+	cvmx_bgxx_cmrx_tx_stat13_t 	tx_stat13;
+	cvmx_bgxx_cmrx_tx_stat14_t 	tx_stat14;
+	cvmx_bgxx_cmrx_tx_stat15_t 	tx_stat15;
+	cvmx_bgxx_cmrx_tx_stat16_t 	tx_stat16;
+	cvmx_bgxx_cmrx_tx_stat17_t 	tx_stat17;
+	cvmx_bgxx_cmrx_rx_fifo_len_t 	rx_fifo_len;
+	cvmx_bgxx_cmrx_tx_fifo_len_t 	tx_fifo_len;
+} lmac_status_t;
+
+int cvmx_dump_bgx_status_node(unsigned node, unsigned bgx)
+{
+	lmac_status_t lmac[4];
+	cvmx_bgxx_cmr_global_config_t global_config;
+	unsigned ind, N;
+	uint8_t lmac_type;
+	uint8_t lmac_sgmii, lmac_rgmii, lmac_xaui, lmac_rxaui, lmac_10g_r, lmac_40g_r;
+	int ipd_port, qlm;
+
+	lmac_sgmii = lmac_rgmii = lmac_xaui = lmac_rxaui = lmac_10g_r  = lmac_40g_r = 0;
+
+	global_config.u64 = cvmx_read_csr_node(node,
+		CVMX_BGXX_CMR_GLOBAL_CONFIG(bgx));
+
+	cvmx_dprintf("\n/*===== BGX Status report			BGX%d =====*/\n", bgx);
+
+	/* just report configured RX/Tx LMACS - don't check return */
+	cvmx_helper_bgx_number_rx_tx_lmacs(node, bgx, &N);
+
+	qlm = bgx<2 ? (global_config.s.pmux_sds_sel==1 ? bgx+2 : bgx) : bgx+2;
+	cvmx_dprintf("NODE%d: BGX%d/lmac[0..%d] connected to QLM%d\n",
+		node, bgx, N - 1,qlm);
+
+	for (ind = 0; ind < N; ind++) {
+		lmac[ind].cmr_config.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_CONFIG(ind, bgx));
+		lmac[ind].rx_bp_status.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_BP_STATUS(ind, bgx));
+		lmac[ind].rx_stat0.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_STAT0(ind, bgx));
+		lmac[ind].rx_stat1.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_STAT1(ind, bgx));
+		lmac[ind].rx_stat2.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_STAT2(ind, bgx));
+		lmac[ind].rx_stat3.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_STAT3(ind, bgx));
+		lmac[ind].rx_stat4.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_STAT4(ind, bgx));
+		lmac[ind].rx_stat5.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_STAT5(ind, bgx));
+		lmac[ind].rx_stat6.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_STAT6(ind, bgx));
+		lmac[ind].rx_stat7.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_STAT7(ind, bgx));
+		lmac[ind].rx_stat8.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_STAT8(ind, bgx));
+		ipd_port = cvmx_helper_get_ipd_port(bgx, ind);
+		lmac[ind].link_info = cvmx_helper_link_get(ipd_port);
+		lmac[ind].tx_stat0.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT0(ind, bgx));
+		lmac[ind].tx_stat1.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT1(ind, bgx));
+		lmac[ind].tx_stat2.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT2(ind, bgx));
+		lmac[ind].tx_stat3.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT3(ind, bgx));
+		lmac[ind].tx_stat4.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT4(ind, bgx));
+		lmac[ind].tx_stat5.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT5(ind, bgx));
+		lmac[ind].tx_stat6.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT6(ind, bgx));
+		lmac[ind].tx_stat7.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT7(ind, bgx));
+		lmac[ind].tx_stat8.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT8(ind, bgx));
+		lmac[ind].tx_stat9.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT9(ind, bgx));
+		lmac[ind].tx_stat10.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT10(ind, bgx));
+		lmac[ind].tx_stat11.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT11(ind, bgx));
+		lmac[ind].tx_stat12.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT12(ind, bgx));
+		lmac[ind].tx_stat13.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT13(ind, bgx));
+		lmac[ind].tx_stat14.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT14(ind, bgx));
+		lmac[ind].tx_stat15.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT15(ind, bgx));
+		lmac[ind].tx_stat16.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT16(ind, bgx));
+		lmac[ind].tx_stat17.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_STAT17(ind, bgx));
+		lmac[ind].rx_fifo_len.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_RX_FIFO_LEN(ind, bgx));
+		lmac[ind].tx_fifo_len.u64 = cvmx_read_csr_node(node,
+			CVMX_BGXX_CMRX_TX_FIFO_LEN(ind, bgx));
+
+		lmac_type = lmac[ind].cmr_config.s.lmac_type;
+		lmac_sgmii |= ((lmac_type == 0/*SGMII*/) << ind);
+		lmac_xaui  |= ((lmac_type == 1/*XAUI*/)  << ind);
+		lmac_rxaui |= ((lmac_type == 2/*RXAUI*/) << ind);
+		lmac_10g_r |= ((lmac_type == 3/*10G_R*/) << ind);
+		lmac_40g_r |= ((lmac_type == 4/*40G_R*/) << ind);
+		lmac_rgmii |= ((lmac_type == 5/*RGMII*/) << ind);
+	}
+
+	PRn("LMAC:",			N, "     lmac%d     ", ind);
+	PRns("Link Status (from ANEG)[SE API]",N,"      %4s     ",
+		lmac[ind].link_info.s.link_up ? " Up " : "Down");
+	PRns("Link Duplex [SE API]",		N, "      %4s     ",
+		lmac[ind].link_info.s.full_duplex ? "Full" : "Half");
+	PRn("Link Speed (MBps) [SE API]",	N, "     %5d     ",
+		lmac[ind].link_info.s.speed);
+	cvmx_helper_bgx_link_status(node, bgx, N);
+	PRn("RX FIFO LEN(fifo_len)", N, "     %5d     ",
+	       lmac[ind].rx_fifo_len.s.fifo_len);
+	PRn("TX FIFO LEN(fifo_len)", N, "     %5d     ",
+	       lmac[ind].tx_fifo_len.s.fifo_len);
+
+	cvmx_dprintf("/*=== Loopback, RX, TX, (PCS) Errors ===*/\n");
+	if (lmac_sgmii) { /* SGMII mode */
+		cvmx_helper_bgx_gmp_loopback(node, bgx, N);
+	}
+	if (lmac_sgmii || lmac_rgmii) { /* SGMII or RGMII mode */
+		cvmx_helper_bgx_gmp_rx_errors(node, bgx, N);
+		cvmx_helper_bgx_gmp_tx_errors(node, bgx, N);
+		cvmx_helper_bgx_gmp_pcs_errors(node, bgx, N);
+	}
+	if (lmac_xaui || lmac_rxaui || lmac_10g_r || lmac_40g_r) {
+		/* for all modes, but sgmii, rgmii */
+		cvmx_helper_bgx_spu_loopback(node, bgx, N);
+		cvmx_helper_bgx_spu_rx_errors(node, bgx, N);
+		cvmx_helper_bgx_spu_tx_errors(node, bgx, N);
+	}
+
+	if (lmac[0/*LMAC0*/].cmr_config.s.lmac_type == 5/*RGMII*/)
+		cvmx_helper_bgx_rgmii_status(node, bgx, 1);
+
+	/*
+	 * NOTE: Most of the following lines will be printed only if 'data'!=0
+	 * i.e. 'counters' with 'zero' value will be skipped/hiden entirely
+	 */
+	cvmx_dprintf ("/*=== Rx and TX Statistics - common for all modes ===*/\n");
+	PRd("RX0: Received Pakets(cnt)", N, "%14lld ",
+	       CAST_ULL(lmac[ind].rx_stat0.s.cnt));
+	PRd("RX1: Octets of received packets(cnt)", N, "%14lld ",
+	       CAST_ULL(lmac[ind].rx_stat1.s.cnt));
+	PRd("RX2: Received FC or PAUSE packets(cnt)", N, "%14lld ",
+	       CAST_ULL(lmac[ind].rx_stat2.s.cnt));
+	PRd("RX3: FC and PAUSE octs(cnt)",N, "%14lld ",
+	       CAST_ULL(lmac[ind].rx_stat3.s.cnt));
+	PRd("RX4: Total DMAC pkts(cnt)", 	N, "%14lld ",
+	       CAST_ULL(lmac[ind].rx_stat4.s.cnt));
+	PRd("RX5: DMAC filter octs(cnt)", N, "%14lld ",
+	       CAST_ULL(lmac[ind].rx_stat5.s.cnt));
+	PRd("RX6: Full FIFO drop pkts(cnt)",N,"%14lld ",
+	       CAST_ULL(lmac[ind].rx_stat6.s.cnt));
+	PRd("RX7: Full FIFO drop octs(cnt)",N,"%14lld ",
+	       CAST_ULL(lmac[ind].rx_stat7.s.cnt));
+	PRn("RX8: Total Errors(cnt)",	N, "%14lld ",
+	       CAST_ULL(lmac[ind].rx_stat8.s.cnt));
+	PRn("RX Backpressure[cnt](bp)",	N, "%14lld ",
+	       CAST_ULL(lmac[ind].rx_bp_status.s.bp));
+	PRd("TX0: Packets dropped - excess. collisions(xscol)", N, "%14lld ",
+	       CAST_ULL(lmac[ind].tx_stat0.s.xscol));
+	PRd("TX1: Packets dropped - excessive deferral(xsdef)", N, "%14lld ",
+	       CAST_ULL(lmac[ind].tx_stat1.s.xsdef));
+	PRd("TX2: Multi Colissions packets(mcol)", N,"%14lld ",
+	       CAST_ULL(lmac[ind].tx_stat2.s.mcol));
+	PRd("TX3: Single Collision packets(scol)", N, "%14lld ",
+	       CAST_ULL(lmac[ind].tx_stat3.s.scol));
+	PRd("TX4: Total sent octets(octs)", N, "%14lld ",
+	       CAST_ULL(lmac[ind].tx_stat4.s.octs));
+	PRd("TX5: Total sent packets(pkts)", N, "%14lld ",
+	       CAST_ULL(lmac[ind].tx_stat5.s.pkts));
+	PRd("TX6: Packets sent with octet count < 64", N, "%14lld ",
+		CAST_ULL(lmac[ind].tx_stat6.s.hist0));
+	PRd("TX7: Packets sent with octet count == 64", N, "%14lld ",
+		CAST_ULL(lmac[ind].tx_stat7.s.hist1));
+	PRd("TX8: Packets sent with octet count 64-127", N, "%14lld ",
+		CAST_ULL(lmac[ind].tx_stat8.s.hist2));
+	PRd("TX9: Packets sent with octet count 128-255", N, "%14lld ",
+		CAST_ULL(lmac[ind].tx_stat9.s.hist3));
+	PRd("TX10: Packets sent with octet count 256-511", N, "%14lld ",
+		CAST_ULL(lmac[ind].tx_stat10.s.hist4));
+	PRd("TX11: Packets sent with octet count 512-1023", N, "%14lld ",
+		CAST_ULL(lmac[ind].tx_stat11.s.hist5));
+	PRd("TX12: Packets sent with octet count 1024-1518", N, "%14lld ",
+		CAST_ULL(lmac[ind].tx_stat12.s.hist6));
+	PRd("TX13: Packets sent with octet count > 1518", N, "%14lld ",
+		CAST_ULL(lmac[ind].tx_stat13.s.hist7));
+	PRd("TX14: Packets sent to broadcast DMAC(bcst)", N, "%14lld ",
+		CAST_ULL(lmac[ind].tx_stat14.s.bcst));
+	PRd("TX15: Packets sent to multicast DMAC(mcst)", N, "%14lld ",
+		CAST_ULL(lmac[ind].tx_stat15.s.mcst));
+	PRd("TX16: Number of underflow packets (undflw)", N, "%14lld ",
+		CAST_ULL(lmac[ind].tx_stat16.s.undflw));
+	PRd("TX17: Number of BGX gen. PAUSE/PFC ctl pkts(ctl)", N, "%14lld ",
+		CAST_ULL(lmac[ind].tx_stat17.s.ctl));
+
+	return 0;
+}
+
+int cvmx_dump_bgx_status(unsigned bgx)
+{
+	return cvmx_dump_bgx_status_node(0, bgx);
+}
+
+int cvmx_dump_bgx_config(unsigned bgx)
+{
+	return cvmx_dump_bgx_config_node(0, bgx);
+}
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-board.c b/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
index fed889a..7bfee97 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-board.c
@@ -87,7 +87,7 @@
  * this pointer to a function before calling any cvmx-helper
  * operations.
  */
-CVMX_SHARED cvmx_helper_link_info_t (*cvmx_override_board_link_get)(int ipd_port) = NULL;
+CVMX_SHARED cvmx_helper_link_info_t(*cvmx_override_board_link_get)(int ipd_port) = NULL;
 
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 /** Set this to 1 to enable lots of debugging output */
@@ -105,6 +105,14 @@ static const int device_tree_dbg = 0;
 static cvmx_helper_link_info_t
 __cvmx_get_cortina_phy_link_state(cvmx_phy_info_t *phy_info);
 
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+/**
+ * @INTERNAL
+ * Get link state of generic C22 compliant PHYs
+ */
+static cvmx_helper_link_info_t
+__cvmx_get_generic_8023_c22_phy_link_state(cvmx_phy_info_t *phy_info);
+
 /**
  * @INTERNAL
  * Get link state of generic C45 compliant PHYs
@@ -121,6 +129,21 @@ __get_marvell_phy_link_state(cvmx_phy_info_t *phy_info);
 
 /**
  * @INTERNAL
+ * Get link state of Aquantia PHY
+ */
+static cvmx_helper_link_info_t
+__get_aquantia_phy_link_state(cvmx_phy_info_t *phy_info);
+
+/**
+ * @INTERNAL
+ * Get link state of the Vitesse VSC8490 PHY
+ */
+static cvmx_helper_link_info_t
+__get_vitesse_vsc8490_phy_link_state(cvmx_phy_info_t *phy_info);
+#endif
+
+/**
+ * @INTERNAL
  * Get link state of broadcom PHY
  *
  * @param phy_info	PHY information
@@ -216,7 +239,7 @@ int __pip_eth_node(const void *fdt_addr, int aliases, int ipd_port)
 	snprintf(name_buffer, sizeof(name_buffer), "ethernet@%x", interface_index);
 	eth = fdt_subnode_offset(fdt_addr, iface, name_buffer);
 	if (dbg)
-		cvmx_dprintf("eth=%d\n", eth);
+		cvmx_dprintf("eth=%d \n", eth);
 	if (eth < 0) {
 		if (dbg)
 			cvmx_dprintf("ERROR : pip interface@%d ethernet@%d not "
@@ -260,6 +283,7 @@ static int __mdiobus_addr_to_unit(uint32_t addr)
 	return unit;
 }
 
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 /**
  * Parse the muxed MDIO interface information from the device tree
  *
@@ -395,6 +419,7 @@ static int __get_muxed_mdio_info_from_dt(cvmx_phy_info_t *phy_info,
 	}
 	return 0;
 }
+#endif
 
 /**
  * @INTERNAL
@@ -405,7 +430,7 @@ static int __get_muxed_mdio_info_from_dt(cvmx_phy_info_t *phy_info,
  * @return node, interface and port number, will be -1 for invalid address.
  */
 static struct cvmx_xiface
-__cvmx_78xx_bgx_reg_addr_to_xiface(uint64_t bgx_addr)
+__cvmx_bgx_reg_addr_to_xiface(uint64_t bgx_addr)
 {
 	struct cvmx_xiface xi = {-1, -1};
 
@@ -452,6 +477,20 @@ static void __cvmx_mdio_addr_to_node_bus(uint64_t addr, int *node, int *bus)
 				    __func__, (unsigned long long) addr);
 			break;
 		}
+	} else if (OCTEON_IS_MODEL(OCTEON_CN73XX)) {
+		switch (addr) {
+		case 0x0001180000003800:
+			*bus = 0;
+			break;
+		case 0x0001180000003880:
+			*bus = 1;
+			break;
+		default:
+			*bus = -1;
+			cvmx_printf("%s: Invalid SMI bus address 0x%llx\n",
+				    __func__, (unsigned long long) addr);
+			break;
+		}
 	} else {
 		switch (addr) {
 		case 0x0001180000001800:
@@ -577,6 +616,7 @@ int __cvmx_helper_78xx_parse_phy(struct cvmx_phy_info *phy_info, int ipd_port)
 	int phy_addr;
 	int index = cvmx_helper_get_interface_index_num(ipd_port);
 	int xiface = cvmx_helper_get_interface_num(ipd_port);
+	int compat_len = 0;
 
 	if (fdt_addr == NULL)
 		fdt_addr = __cvmx_phys_addr_to_ptr(cvmx_sysinfo_get()->fdt_addr,
@@ -592,21 +632,27 @@ int __cvmx_helper_78xx_parse_phy(struct cvmx_phy_info *phy_info, int ipd_port)
 		 * device tree to get the node offsets.
 		 */
 		if (device_tree_dbg)
-			cvmx_dprintf("No config present, calling __cvmx_helper_parse_78xx_bgx_dt\n");
-		if (__cvmx_helper_parse_78xx_bgx_dt(fdt_addr)) {
-			cvmx_printf("Error: could not parse 78xx BGX\n");
+			cvmx_dprintf("No config present, calling __cvmx_helper_parse_bgx_dt\n");
+		if (__cvmx_helper_parse_bgx_dt(fdt_addr)) {
+			cvmx_printf("Error: could not parse BGX device tree\n");
+			return -1;
+		}
+		if (octeon_has_feature(OCTEON_FEATURE_BGX_XCV) &&
+		    __cvmx_helper_parse_bgx_rgmii_dt(fdt_addr)) {
+			cvmx_printf("Error: could not parse BGX XCV device tree\n");
 			return -1;
 		}
 		phy = cvmx_helper_get_phy_fdt_node_offset(xiface, index);
 		if (phy < 0) {
 			if (device_tree_dbg)
-				cvmx_dprintf("%s: Could not get PHY node offset for IPD port 0x%x\n",
-					     __func__, ipd_port);
+				cvmx_dprintf("%s: Could not get PHY node offset for IPD port 0x%x, xiface: 0x%x, index: %d\n",
+					     __func__, ipd_port, xiface, index);
 			return -1;
 		}
 	}
 
-	compat = (const char *)fdt_getprop(fdt_addr, phy, "compatible", NULL);
+	compat = (const char *)fdt_getprop(fdt_addr, phy, "compatible",
+					   &compat_len);
 	if (!compat) {
 		cvmx_printf("ERROR: %d:%d:no compatible prop in phy\n", xiface, index);
 		return -1;
@@ -633,6 +679,18 @@ int __cvmx_helper_78xx_parse_phy(struct cvmx_phy_info *phy_info, int ipd_port)
 	} else if (!memcmp("cortina", compat, strlen("cortina"))) {
 		phy_info->phy_type = CORTINA_PHY;
 		phy_info->link_function = __cvmx_get_cortina_phy_link_state;
+	} else if (!strcmp("vitesse,vsc8490", compat)) {
+		phy_info->phy_type = VITESSE_VSC8490_PHY;
+		phy_info->link_function = __get_vitesse_vsc8490_phy_link_state;
+	} else if (fdt_stringlist_contains(compat, compat_len,
+					   "ethernet-phy-ieee802.3-c22")) {
+		phy_info->phy_type = GENERIC_8023_C22_PHY;
+		phy_info->link_function =
+				__cvmx_get_generic_8023_c22_phy_link_state;
+	} else if (fdt_stringlist_contains(compat, compat_len,
+					   "ethernet-phy-ieee802.3-c45")) {
+		phy_info->phy_type = GENERIC_8023_C22_PHY;
+		phy_info->link_function = __get_generic_8023_c45_phy_link_state;
 	}
 
 	phy_info->ipd_port = ipd_port;
@@ -709,7 +767,7 @@ int __cvmx_helper_78xx_parse_phy(struct cvmx_phy_info *phy_info, int ipd_port)
  *
  * @return 0 for success, -1 on error.
  */
-int __cvmx_helper_parse_78xx_bgx_dt(void *fdt_addr)
+int __cvmx_helper_parse_bgx_dt(void *fdt_addr)
 {
 	int port_index;
 	int dbg = device_tree_dbg;
@@ -757,7 +815,7 @@ int __cvmx_helper_parse_78xx_bgx_dt(void *fdt_addr)
 		reg_addr = cvmx_fdt_translate_address(fdt_addr,
 						      fdt_interface_node,
 						      (uint32_t *)&reg_addr);
-		xi = __cvmx_78xx_bgx_reg_addr_to_xiface(reg_addr);
+		xi = __cvmx_bgx_reg_addr_to_xiface(reg_addr);
 		if (xi.node < 0) {
 			cvmx_dprintf("Device tree BGX node has invalid address 0x%llx\n",
 				     (unsigned long long)reg_addr);
@@ -787,7 +845,6 @@ int __cvmx_helper_parse_78xx_bgx_dt(void *fdt_addr)
 							  fdt_phy_node, NULL));
 			}
 			cvmx_helper_set_port_phy_present(xiface, port_index, true);
-
 		} else {
 			cvmx_helper_set_phy_fdt_node_offset(xiface, port_index,
 							    -1);
@@ -800,6 +857,121 @@ int __cvmx_helper_parse_78xx_bgx_dt(void *fdt_addr)
 	return 0;
 }
 
+int __cvmx_helper_parse_bgx_rgmii_dt(const void *fdt_addr)
+{
+	uint64_t reg_addr;
+	struct cvmx_xiface xi;
+	int fdt_port_node = -1;
+	int fdt_interface_node;
+	int fdt_phy_node;
+	int port_index;
+	int xiface;
+	int dbg = device_tree_dbg;
+
+	/* There's only one xcv (RGMII) interface, so just search for the one
+	 * that's part of a BGX entry.
+	 */
+	while ((fdt_port_node = fdt_node_offset_by_compatible(fdt_addr,
+							      fdt_port_node,
+							      "cavium,octeon-7360-xcv")) >= 0) {
+		fdt_interface_node = fdt_parent_offset(fdt_addr, fdt_port_node);
+		if (fdt_interface_node < 0) {
+			cvmx_printf("Error: device tree corrupt!\n");
+			return -1;
+		}
+		if (dbg)
+			cvmx_dprintf("%s: XCV parent node compatible: %s\n",
+				     __func__,
+				     (char *)fdt_getprop(fdt_addr,
+							 fdt_interface_node,
+							 "compatible", NULL));
+		if (!fdt_node_check_compatible(fdt_addr, fdt_interface_node,
+					       "cavium,octeon-7890-bgx"))
+			break;
+	}
+	if (fdt_port_node == -FDT_ERR_NOTFOUND) {
+		if (dbg)
+			cvmx_dprintf("No XCV/RGMII interface found in device tree\n");
+		return 0;
+	} else if (fdt_port_node < 0) {
+		cvmx_dprintf("%s: Error %d parsing device tree\n",
+			     __func__, fdt_port_node);
+		return -1;
+	}
+	if (dbg) {
+		char path[256];
+		if (!fdt_get_path(fdt_addr, fdt_port_node, path, sizeof(path))) {
+			cvmx_dprintf("xcv path: %s\n", path);
+		}
+		if (!fdt_get_path(fdt_addr, fdt_interface_node, path,
+				  sizeof(path))) {
+			cvmx_dprintf("interface path: %s\n", path);
+		}
+	}
+	port_index = cvmx_fdt_get_int(fdt_addr, fdt_port_node, "reg", -1);
+	if (port_index != 0) {
+		cvmx_printf("%s: Error: port index (reg) must be 0, not %d.\n",
+			    __func__, port_index);
+		return -1;
+	}
+	reg_addr = cvmx_fdt_get_addr(fdt_addr, fdt_interface_node, "reg");
+	if (reg_addr == FDT_ADDR_T_NONE) {
+		cvmx_printf("%s: Error: could not get BGX interface address\n",
+			    __func__);
+		return -1;
+	}
+	/* We don't have to bother translating since only 78xx supports OCX and
+	 * doesn't support RGMII.
+	 */
+	xi = __cvmx_bgx_reg_addr_to_xiface(reg_addr);
+	if (dbg)
+		cvmx_dprintf("%s: xi.node: %d, xi.interface: 0x%x, addr: 0x%llx\n", __func__,
+			     xi.node, xi.interface,
+			     (unsigned long long)reg_addr);
+	if (xi.node < 0) {
+		cvmx_printf("%s: Device tree BGX node has invalid address 0x%llx\n",
+			    __func__, (unsigned long long)reg_addr);
+		return -1;
+	}
+	if (dbg) {
+		cvmx_dprintf("%s: Found XCV (RGMII) interface on interface %d\n",
+			     __func__, xi.interface);
+		cvmx_dprintf("  phy handle: 0x%x\n",
+			     cvmx_fdt_get_int(fdt_addr, fdt_port_node,
+					      "phy-handle", -1));
+	}
+	fdt_phy_node = cvmx_fdt_lookup_phandle(fdt_addr, fdt_port_node,
+					       "phy-handle");
+	if (dbg)
+		cvmx_dprintf("%s: phy-handle node: 0x%x\n", __func__,
+			     fdt_phy_node);
+	xiface = cvmx_helper_node_interface_to_xiface(xi.node, xi.interface);
+
+	cvmx_helper_set_port_fdt_node_offset(xiface, port_index,
+					     fdt_port_node);
+	if (fdt_phy_node >= 0) {
+		if (dbg) {
+			cvmx_dprintf("%s: Setting PHY fdt node offset for interface 0x%x, port %d to %d\n",
+				     __func__, xiface, port_index,
+				     fdt_phy_node);
+			cvmx_dprintf("%s: PHY node name: %s\n",
+				     __func__,
+				     fdt_get_name(fdt_addr,
+						  fdt_phy_node, NULL));
+		}
+		cvmx_helper_set_phy_fdt_node_offset(xiface, port_index,
+						    fdt_phy_node);
+		cvmx_helper_set_port_phy_present(xiface, port_index, true);
+	} else {
+		cvmx_helper_set_phy_fdt_node_offset(xiface, port_index, -1);
+		if (dbg)
+			cvmx_dprintf("%s: No PHY fdt node offset for interface 0x%x, port %d to %d\n",
+				     __func__, xiface, port_index, fdt_phy_node);
+		cvmx_helper_set_port_phy_present(xiface, port_index, false);
+	}
+	return 0;
+}
+
 /**
  * Returns if a port is present on an interface
  *
@@ -822,15 +994,18 @@ int __cvmx_helper_board_get_port_from_dt(void *fdt_addr, int ipd_port)
 	uint32_t *val;
 	int phy_node_offset;
 
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+	if (octeon_has_feature(OCTEON_FEATURE_BGX)) {
 		static int fdt_ports_initialized = 0;
 
 		port_index = cvmx_helper_get_interface_index_num(ipd_port);
 
 		if (!fdt_ports_initialized) {
-			if (!__cvmx_helper_parse_78xx_bgx_dt(fdt_addr))
+			if (octeon_has_feature(OCTEON_FEATURE_BGX_XCV))
+				if (!__cvmx_helper_parse_bgx_rgmii_dt(fdt_addr))
+					fdt_ports_initialized = 1;
+			if (!__cvmx_helper_parse_bgx_dt(fdt_addr)) {
 				fdt_ports_initialized = 1;
-			else {
+			} else {
 				cvmx_dprintf("%s: Error parsing FDT\n",
 					     __func__);
 				return -1;
@@ -842,15 +1017,15 @@ int __cvmx_helper_board_get_port_from_dt(void *fdt_addr, int ipd_port)
 
 	mode = cvmx_helper_interface_get_mode(xiface);
 
-	switch (mode) {
-	/* Device tree has information about the following mode types. */
-	case CVMX_HELPER_INTERFACE_MODE_RGMII:
-	case CVMX_HELPER_INTERFACE_MODE_GMII:
-	case CVMX_HELPER_INTERFACE_MODE_SPI:
-	case CVMX_HELPER_INTERFACE_MODE_XAUI:
-	case CVMX_HELPER_INTERFACE_MODE_SGMII:
+        switch (mode) {
+        /* Device tree has information about the following mode types. */
+        case CVMX_HELPER_INTERFACE_MODE_RGMII:
+        case CVMX_HELPER_INTERFACE_MODE_GMII:
+        case CVMX_HELPER_INTERFACE_MODE_SPI:
+        case CVMX_HELPER_INTERFACE_MODE_XAUI:
+        case CVMX_HELPER_INTERFACE_MODE_SGMII:
 	case CVMX_HELPER_INTERFACE_MODE_QSGMII:
-	case CVMX_HELPER_INTERFACE_MODE_RXAUI:
+        case CVMX_HELPER_INTERFACE_MODE_RXAUI:
 	case CVMX_HELPER_INTERFACE_MODE_AGL:
 	case CVMX_HELPER_INTERFACE_MODE_XLAUI:
 	case CVMX_HELPER_INTERFACE_MODE_XFI:
@@ -879,7 +1054,7 @@ int __cvmx_helper_board_get_port_from_dt(void *fdt_addr, int ipd_port)
 	if (!pip_path) {
 		cvmx_dprintf("%s: ERROR: "
 			"interface %x pip path not found in device tree\n",
-			__func__, xiface);
+		         __func__, xiface);
 		return -1;
 	}
 	pip = fdt_path_offset(fdt_addr, pip_path);
@@ -971,7 +1146,7 @@ int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 	if (dbg)
 		cvmx_dprintf("%s(%p, %d)\n", __func__, phy_info, ipd_port);
 
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+	if (octeon_has_feature(OCTEON_FEATURE_BGX))
 		return __cvmx_helper_78xx_parse_phy(phy_info, ipd_port);
 
 	if (fdt_addr == 0)
@@ -1061,8 +1236,13 @@ int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 		if (dbg)
 			cvmx_dprintf("Vitesse PHY detected for ipd_port %d\n",
 				     ipd_port);
-		if (!fdt_node_check_compatible(fdt_addr, phy,
-					       "ethernet-phy-ieee802.3-c22")) {
+		if (!fdt_node_check_compatible(fdt_addr, phy, "vitesse,vsc8490")) {
+			phy_info->phy_type = VITESSE_VSC8490_PHY;
+			if (dbg)
+				cvmx_dprintf("Vitesse VSC8490 detected\n");
+			phy_info->link_function = __get_vitesse_vsc8490_phy_link_state;
+		} else if (!fdt_node_check_compatible(fdt_addr, phy,
+						      "ethernet-phy-ieee802.3-c22")) {
 			phy_info->phy_type = GENERIC_8023_C22_PHY;
 			phy_info->link_function =
 					__cvmx_get_generic_8023_c22_phy_link_state;
@@ -1075,6 +1255,11 @@ int __get_phy_info_from_dt(cvmx_phy_info_t *phy_info, int ipd_port)
 			if (dbg)
 				cvmx_dprintf("Vitesse 802.3 c45 detected\n");
 		}
+	} else if (!memcmp("aquantia", phy_compatible_str, strlen("aquantia"))) {
+		phy_info->phy_type = AQUANTIA_PHY;
+		phy_info->link_function = __get_aquantia_phy_link_state;
+		if (dbg)
+			cvmx_dprintf("Aquantia c45 PHY detected\n");
 	} else if (!memcmp("cortina", phy_compatible_str, strlen("cortina"))) {
 		phy_info->phy_type = CORTINA_PHY;
 		phy_info->link_function = __cvmx_get_cortina_phy_link_state;
@@ -1565,15 +1750,24 @@ __cvmx_get_cortina_phy_link_state(cvmx_phy_info_t *phy_info)
 	uint32_t bus = phy_addr >> 8;
 	int value;
 	uint32_t id;
-	bool is_cs4223 = false;
 #define CS4XXX_GLOBAL_CHIPID_LSB			0x0000
 #define CS4XXX_GLOBAL_CHIPID_MSB			0x0001
 #define CS4321_GIGEPCS_LINE_STATUS			0xC01
 #define CS4321_GPIO_GPIO_INTS				0x16D
 #define CS4321_GLOBAL_GT_10KHZ_REF_CLK_CNT0		0x2E
 #define CS4223_PP_LINE_SDS_DSP_MSEQ_STATUS		0x1237
+#define CS4223_PP_LINE_SDS_DSP_MSEQ_DUPLEX_LOS_S	(1 << 0)
+#define CS4223_PP_LINE_SDS_DSP_MSEQ_LOCAL_LOS_S		(1 << 1)
+#define CS4223_PP_LINE_SDS_DSP_MSEQ_DUPLEX_LOCKD_S	(1 << 2)
+#define CS4223_PP_LINE_SDS_DSP_MSEQ_LOCAL_LOCD_S	(1 << 3)
+#define CS4223_PP_LINE_SDS_DSP_MSEQ_EXTERNAL_LOS_S	(1 << 4)
 #define CS4223_PP_LINE_SDS_DSP_MSEQ_EDC_CONVERGED	(1 << 5)
 #define CS4223_PP_HOST_SDS_DSP_MSEQ_STATUS		0x1A37
+#define CS4223_PP_HOST_SDS_DSP_MSEQ_DUPLEX_LOS_S	(1 << 0)
+#define CS4223_PP_HOST_SDS_DSP_MSEQ_LOCAL_LOS_S		(1 << 1)
+#define CS4223_PP_HOST_SDS_DSP_MSEQ_DUPLEX_LOCKD_S	(1 << 2)
+#define CS4223_PP_HOST_SDS_DSP_MSEQ_LOCAL_LOCD_S	(1 << 3)
+#define CS4223_PP_HOST_SDS_DSP_MSEQ_EXTERNAL_LOS_S	(1 << 4)
 #define CS4223_PP_HOST_SDS_DSP_MSEQ_EDC_CONVERGED	(1 << 5)
 
 	result.u64 = 0;
@@ -1581,16 +1775,47 @@ __cvmx_get_cortina_phy_link_state(cvmx_phy_info_t *phy_info)
 
 	phy_addr &= 0xff;
 
+	/* Figure out which Cortina/Inphi PHY we're working with */
 	value = cvmx_mdio_45_read(bus, phy_addr, 0, CS4XXX_GLOBAL_CHIPID_LSB);
 	id = value;
 
 	value = cvmx_mdio_45_read(bus, phy_addr, 0, CS4XXX_GLOBAL_CHIPID_MSB);
 	id |= value << 16;
 
-	is_cs4223 = ((id & 0x0FFFFFFF) == 0x000303e5);
+	/* Are we a CS4223/CS4343 PHY? */
+	if ((id & 0x0FFFFFFF) == 0x000303e5) {
+		/* For the CS4223 and CS4343 Cortina PHYs the link information
+		 * is contained in one of the slices.  We use sub-addresses in
+		 * order to calculate the appropriate register offset.
+		 */
+		int slice_off;
+		int xiface;
+		cvmx_helper_interface_mode_t mode;
 
-	if (is_cs4223) {
-		int slice_off = phy_info->phy_sub_addr * 0x1000;
+		/* For XLAUI/40G KR mode we need to check the link of all four
+		 * slices.  If any one of them is down then the entire link is
+		 * down.
+		 */
+		xiface = cvmx_helper_get_interface_num(phy_info->ipd_port);
+		mode = cvmx_helper_interface_get_mode(xiface);
+		if ((mode == CVMX_HELPER_INTERFACE_MODE_XLAUI) ||
+		    (mode == CVMX_HELPER_INTERFACE_MODE_40G_KR4)) {
+			result.s.link_up = 1;
+			for (slice_off = 0; slice_off < 0x4000;
+			     slice_off += 0x1000) {
+				value = cvmx_mdio_45_read(bus, phy_addr, 0,
+							  CS4223_PP_LINE_SDS_DSP_MSEQ_STATUS + slice_off);
+				if (!(value & CS4223_PP_LINE_SDS_DSP_MSEQ_EDC_CONVERGED))
+					result.s.link_up = 0;
+				value = cvmx_mdio_45_read(bus, phy_addr, 0,
+							  CS4223_PP_HOST_SDS_DSP_MSEQ_STATUS + slice_off);
+				if (!(value & CS4223_PP_HOST_SDS_DSP_MSEQ_EDC_CONVERGED))
+					result.s.link_up = 0;
+			}
+			return result;
+		}
+
+		slice_off = phy_info->phy_sub_addr * 0x1000;
 		/* For now we only support 40Gbps for this device. */
 		value = cvmx_mdio_45_read(bus, phy_addr, 0,
 					  CS4223_PP_LINE_SDS_DSP_MSEQ_STATUS + slice_off);
@@ -1603,7 +1828,9 @@ __cvmx_get_cortina_phy_link_state(cvmx_phy_info_t *phy_info)
 		return result;
 	}
 
-	/* Right now to determine the speed we look at the reference clock.
+	/* If we're here we are dealing with a Cortina CS4318 type PHY.
+	 *
+	 * Right now to determine the speed we look at the reference clock.
 	 * There's probably a better way but this should work.  For SGMII
 	 * we use a 100MHz reference clock, for XAUI/RXAUI we use a 156.25MHz
 	 * reference clock.
@@ -1658,6 +1885,91 @@ __get_generic_8023_c45_phy_link_state(cvmx_phy_info_t *phy_info)
 	return result;
 }
 
+#ifndef CVMX_BUILD_FOR_LINUX_KERNEL
+/**
+ * @INTERNAL
+ * Get link state of generic C45 compliant PHYs
+ */
+static cvmx_helper_link_info_t
+__get_vitesse_vsc8490_phy_link_state(cvmx_phy_info_t *phy_info)
+{
+	cvmx_helper_link_info_t result;
+	int phy_status;
+	uint32_t phy_addr = phy_info->phy_addr;
+	int xiface;
+	cvmx_helper_interface_mode_t mode;
+
+	xiface = cvmx_helper_get_interface_num(phy_info->ipd_port);
+	mode = cvmx_helper_interface_get_mode(xiface);
+
+	/* For 10G just use the generic 10G support */
+	if (mode == CVMX_HELPER_INTERFACE_MODE_XAUI ||
+	    mode == CVMX_HELPER_INTERFACE_MODE_RXAUI)
+		return __get_generic_8023_c45_phy_link_state(phy_info);
+
+	phy_status = cvmx_mdio_45_read(phy_addr >> 8, phy_addr & 0xff,
+				       3, 0xe10d);
+
+	result.u64 = 0;
+	if ((phy_status & 0x111) != 0x111)
+		return result;
+
+	result.s.speed = 1000;
+	result.s.full_duplex = 1;
+	result.s.link_up = 1;
+
+	return result;
+}
+
+/**
+ * @INTERNAL
+ * Get link state of Aquantia PHY
+ */
+static cvmx_helper_link_info_t
+__get_aquantia_phy_link_state(cvmx_phy_info_t *phy_info)
+{
+	cvmx_helper_link_info_t result;
+	uint16_t val;
+	uint32_t phy_addr = phy_info->phy_addr;
+	int bus = phy_info->mdio_unit;
+	uint8_t con_state;
+	static const uint16_t speeds[8] =
+		{ 10, 100, 1000, 10000, 2500, 5000, 0, 0 };
+
+	result.u64 = 0;
+
+	/* Read PMA Receive Vendor State 1, bit 0 indicates that the Rx link
+	 * is good
+	 */
+	val = cvmx_mdio_45_read(bus, phy_addr, 0x1, 0xe800);
+	result.s.link_up = !!(val & 1);
+
+	if (!result.s.link_up)
+		return result;
+
+	/* See if we're in autonegotiation training mode */
+
+	/* Read the Autonegation Reserved Vendor Status 1 register */
+	val = cvmx_mdio_45_read(bus, phy_addr, 7, 0xc810);
+	/* Get the connection state, State 4 = connected */
+	con_state = (val >> 9) & 0x1f;
+	if (con_state == 2 || con_state == 3 || con_state == 0xa) {
+		/* We're in autonegotiation mode so pretend the link is down */
+		result.s.link_up = 0;
+		return result;
+	}
+
+	result.s.full_duplex = 1;
+	if (con_state == 4) {
+		val = cvmx_mdio_45_read(bus, phy_addr, 7, 0xc800);
+		result.s.speed = speeds[(val >> 1) & 7];
+	} else {
+		result.s.speed = 1000;
+	}
+	return result;
+}
+#endif
+
 /**
  * @INTERNAL
  * Get link state of marvell PHY
@@ -1684,9 +1996,8 @@ __get_marvell_phy_link_state(cvmx_phy_info_t *phy_info)
 			phy_status |= 1 << 11;
 	}
 
-	/* Only return a link if the PHY has finished auto negotiation
-	   and set the resolved bit (bit 11) */
-	if (phy_status & (1 << 11)) {
+	/* Link is up = Speed/Duplex Resolved + RT-Link Up + G-Link Up. */
+	if ((phy_status & 0x0c08) == 0x0c08) {
 		result.s.link_up = 1;
 		result.s.full_duplex = ((phy_status >> 13) & 1);
 		switch ((phy_status >> 14) & 3) {
@@ -2018,7 +2329,7 @@ static int __switch_mdio_mux(const cvmx_phy_info_t *phy_info)
 		 */
 		if (phy_info->gpio_parent_mux_twsi >= 0) {
 			old_mux = __set_twsi_mux(phy_info->gpio_parent_mux_twsi,
-						1 << phy_info->gpio_parent_mux_select);
+					         1 << phy_info->gpio_parent_mux_select);
 			if (old_mux < 0) {
 				/*cvmx_dprintf("%s: Error: could not read old MUX value for port %d\n",
 					     __func__, ipd_port); */
@@ -2086,9 +2397,9 @@ cvmx_helper_link_info_t __cvmx_helper_board_link_get_from_dt(int ipd_port)
 			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
 			case CVMX_HELPER_INTERFACE_MODE_XAUI:
 			case CVMX_HELPER_INTERFACE_MODE_10G_KR:
+			case CVMX_HELPER_INTERFACE_MODE_XFI:
 				result.s.speed = 10000;
 				break;
-			case CVMX_HELPER_INTERFACE_MODE_XFI:
 			case CVMX_HELPER_INTERFACE_MODE_XLAUI:
 			case CVMX_HELPER_INTERFACE_MODE_40G_KR4:
 				result.s.speed = 40000;
@@ -2096,7 +2407,6 @@ cvmx_helper_link_info_t __cvmx_helper_board_link_get_from_dt(int ipd_port)
 			default:
 				break;
 			}
-
 			return result;
 		}
 		phy_info = cvmx_helper_get_port_phy_info(xiface, index);
@@ -2125,7 +2435,7 @@ cvmx_helper_link_info_t __cvmx_helper_board_link_get_from_dt(int ipd_port)
 	}
 
 	if (phy_info->phy_addr == -1) {
-		if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		if (octeon_has_feature(OCTEON_FEATURE_BGX)) {
 			if (__cvmx_helper_78xx_parse_phy(phy_info, ipd_port)) {
 				/*cvmx_dprintf("Error parsing PHY info for 78xx for ipd port %d\n",
 					       ipd_port); */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
index e829a41..3e13adb 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-cfg.c
@@ -52,6 +52,7 @@
 #include <asm/octeon/cvmx-helper-util.h>
 #include <asm/octeon/cvmx-helper-cfg.h>
 #include <asm/octeon/cvmx-helper-ilk.h>
+#include <asm/octeon/cvmx-helper-bgx.h>
 #include <asm/octeon/cvmx-ilk.h>
 #include <asm/octeon/cvmx-range.h>
 #include <asm/octeon/cvmx-global-resources.h>
@@ -66,6 +67,7 @@
 #include "cvmx-ilk.h"
 #include "cvmx-adma.h"
 #include "cvmx-helper-ilk.h"
+#include "cvmx-helper-bgx.h"
 #include "cvmx-pip.h"
 #include "cvmx-range.h"
 #include "cvmx-global-resources.h"
@@ -76,6 +78,8 @@
 # define min( a, b ) ( ( a ) < ( b ) ) ? ( a ) : ( b )
 #endif
 
+int cvmx_npi_max_pknds;
+
 CVMX_SHARED struct cvmx_cfg_port_param cvmx_cfg_port[CVMX_MAX_NODES][CVMX_HELPER_MAX_IFACE][CVMX_HELPER_CFG_MAX_PORT_PER_IFACE] =
 	{[0 ... CVMX_MAX_NODES - 1][0 ... CVMX_HELPER_MAX_IFACE - 1] =
 		{[0 ... CVMX_HELPER_CFG_MAX_PORT_PER_IFACE - 1] =
@@ -157,7 +161,10 @@ static const int dbg = 0;
 int __cvmx_helper_cfg_pknd(int xiface, int index)
 {
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	return cvmx_cfg_port[xi.node][xi.interface][index].ccpp_pknd;
+	int pkind;
+
+	pkind = cvmx_cfg_port[xi.node][xi.interface][index].ccpp_pknd;
+	return pkind;
 }
 
 int __cvmx_helper_cfg_bpid(int xiface, int index)
@@ -505,27 +512,22 @@ void cvmx_helper_cfg_init_pko_port_map(void)
 void cvmx_helper_cfg_set_jabber_and_frame_max()
 {
 	int interface, port;
+	/*Set the frame max size and jabber size to 65535. */
+	const unsigned max_frame = 65535;
 
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+	// FIXME: should support node argument for remote node init
+	if (octeon_has_feature(OCTEON_FEATURE_BGX)) {
 		int ipd_port;
 		int node = cvmx_get_node_num();
 
-		/*Set the frame max size and jabber size to 65535. */
 		for (interface = 0; interface < cvmx_helper_get_number_of_interfaces(); interface++) {
-			/* Set the frame max size and jabber size to 65535, as the defaults
-		   	are too small. */
 			int xiface = cvmx_helper_node_interface_to_xiface(node, interface);
 			cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(xiface);
-			int num_ports = cvmx_helper_ports_on_interface(interface);
-
+			int num_ports = cvmx_helper_ports_on_interface(xiface);
+			// FIXME: should be an easier way to determine
+			// that an interface is Ethernet/BGX
 			switch (imode) {
 			case CVMX_HELPER_INTERFACE_MODE_SGMII:
-				for (port = 0; port < num_ports; port++) {
-					ipd_port = cvmx_helper_get_ipd_port(xiface, port);
-					cvmx_pki_set_max_frm_len(ipd_port, -1);
-					cvmx_write_csr_node(node, CVMX_BGXX_GMP_GMI_RXX_JABBER(port, interface), 65535);
-				}
-				break;
 			case CVMX_HELPER_INTERFACE_MODE_XAUI:
 			case CVMX_HELPER_INTERFACE_MODE_RXAUI:
 			case CVMX_HELPER_INTERFACE_MODE_XLAUI:
@@ -534,8 +536,10 @@ void cvmx_helper_cfg_set_jabber_and_frame_max()
 			case CVMX_HELPER_INTERFACE_MODE_40G_KR4:
 				for (port = 0; port < num_ports; port++) {
 					ipd_port = cvmx_helper_get_ipd_port(xiface, port);
-					cvmx_pki_set_max_frm_len(ipd_port, -1);
-					cvmx_write_csr_node(node, CVMX_BGXX_SMUX_RX_JABBER(port, interface), 65535);
+					cvmx_pki_set_max_frm_len(
+						ipd_port, max_frame);
+					cvmx_helper_bgx_set_jabber(
+						xiface, port, max_frame);
 				}
 				break;
 			default:
@@ -546,10 +550,11 @@ void cvmx_helper_cfg_set_jabber_and_frame_max()
 
 		/*Set the frame max size and jabber size to 65535. */
 		for (interface = 0; interface < cvmx_helper_get_number_of_interfaces(); interface++) {
+			int xiface = cvmx_helper_node_interface_to_xiface(cvmx_get_node_num(), interface);
 			/* Set the frame max size and jabber size to 65535, as the defaults
 		   	are too small. */
-			cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(interface);
-			int num_ports = cvmx_helper_ports_on_interface(interface);
+			cvmx_helper_interface_mode_t imode = cvmx_helper_interface_get_mode(xiface);
+			int num_ports = cvmx_helper_ports_on_interface(xiface);
 
 			switch (imode) {
 			case CVMX_HELPER_INTERFACE_MODE_SGMII:
@@ -607,7 +612,7 @@ void cvmx_helper_cfg_store_short_packets_in_wqe()
 	cvmx_ipd_ctl_status_t ipd_ctl_status;
 	unsigned dyn_rs = 1;
 
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+	if (octeon_has_feature(OCTEON_FEATURE_PKI))
 		return;
 
 	/* NO_WPTR combines WQE with 1st MBUF, RS is redundant */
@@ -654,7 +659,7 @@ int __cvmx_helper_cfg_pko_port_eid(int pko_port)
 #define IPD2PKO_CACHE_Y(ipd_port)	(ipd_port) >> 8
 #define IPD2PKO_CACHE_X(ipd_port)	(ipd_port) & 0xff
 
-inline int __cvmx_helper_cfg_ipd2pko_cachex(int ipd_port)
+static inline int __cvmx_helper_cfg_ipd2pko_cachex(int ipd_port)
 {
 	int ipd_x = IPD2PKO_CACHE_X(ipd_port);
 	if (ipd_port & 0x800)
@@ -691,7 +696,7 @@ int cvmx_helper_cfg_ipd2pko_port_base(int ipd_port)
 	int ipd_y, ipd_x;
 
 	/* Internal PKO ports are not present in PKO3 */
-	if(OCTEON_IS_MODEL(OCTEON_CN78XX))
+	if(octeon_has_feature(OCTEON_FEATURE_PKI))
 		return ipd_port;
 
 	ipd_y = IPD2PKO_CACHE_Y(ipd_port);
@@ -902,12 +907,31 @@ int __cvmx_helper_init_port_valid(void)
 	int i, j, n;
 	bool valid;
 	static void *fdt_addr = 0;
+	int rc;
 
 	if (fdt_addr == 0)
 		fdt_addr = __cvmx_phys_addr_to_ptr(cvmx_sysinfo_get()->fdt_addr,
 						   (128*1024));
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
-		return __cvmx_helper_parse_78xx_bgx_dt(fdt_addr);
+	if (octeon_has_feature(OCTEON_FEATURE_BGX)) {
+		rc = __cvmx_helper_parse_bgx_dt(fdt_addr);
+		if (!rc && octeon_has_feature(OCTEON_FEATURE_BGX_XCV))
+			rc = __cvmx_helper_parse_bgx_rgmii_dt(fdt_addr);
+
+		/* Some ports are not in sequence, the device tree does not clear them */
+		for (i = 0; i < CVMX_HELPER_MAX_GMX; i++) {
+			int j;
+			for (j = 0; j < cvmx_helper_interface_enumerate(i); j++) {
+				cvmx_bgxx_cmrx_config_t cmr_config;
+				cmr_config.u64 = cvmx_read_csr(CVMX_BGXX_CMRX_CONFIG(j, i));
+				if (cmr_config.s.lane_to_sds == 0xe4
+				    && cmr_config.s.lmac_type != 4
+				    && cmr_config.s.lmac_type != 1
+				    && cmr_config.s.lmac_type != 5)
+					cvmx_helper_set_port_valid(i, j, false);
+			}
+		}
+		return rc;
+	}
 
 	/* TODO: Update this to behave more like 78XX */
 	for (i = 0; i < cvmx_helper_get_number_of_interfaces(); i++) {
@@ -1033,6 +1057,11 @@ int __cvmx_helper_init_port_config_data(int node)
 				}
 			} else {
 				for (j = 0; j < n; j++) {
+
+					if (j == n/cvmx_npi_max_pknds) {
+						pknd++;
+						bpid++;
+					}
 					cvmx_cfg_port[node][i][j].ccpp_pknd = pknd;
 					cvmx_cfg_port[node][i][j].ccpp_bpid = bpid;
 				}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-errata.c b/arch/mips/cavium-octeon/executive/cvmx-helper-errata.c
index 72f98a4..85dc3a0 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-errata.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-errata.c
@@ -83,7 +83,7 @@ int __cvmx_helper_errata_fix_ipd_ptr_alignment(void)
 #define FIX_IPD_FIRST_BUFF_PAYLOAD_BYTES     (cvmx_fpa_get_packet_pool_block_size() \
 						- 8 - cvmx_ipd_cfg.first_mbuf_skip )
 #define FIX_IPD_NON_FIRST_BUFF_PAYLOAD_BYTES (cvmx_fpa_get_packet_pool_block_size() \
-						- 8 - cvmx_ipd_cfg.not_first_mbuf_skip )
+                                              - 8 - cvmx_ipd_cfg.not_first_mbuf_skip )
 #define FIX_IPD_OUTPORT 0
 #define INTERFACE(port) (port >> 4)	/* Ports 0-15 are interface 0, 16-31 are interface 1 */
 #define INDEX(port) (port & 0xf)
@@ -248,7 +248,7 @@ fix_ipd_exit:
  * @param work   Work queue entry to fix
  * @return Zero on success. Negative on failure
  */
-int cvmx_helper_fix_ipd_packet_chain(cvmx_wqe_t *work)
+int cvmx_helper_fix_ipd_packet_chain(cvmx_wqe_t * work)
 {
 	uint64_t number_buffers = work->word2.s.bufs;
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c b/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
index 7ac0e5e..fcba07d 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-ilk.c
@@ -728,6 +728,18 @@ retry:
 	ilk_rxx_int.u64 = cvmx_read_csr_node(node, CVMX_ILK_RXX_INT(interface));
 
 	if (ilk_rxx_cfg1.s.rx_bdry_lock_ena == 0) {
+		/* (GSER-21957) GSER RX Equalization may make >= 5gbaud non-KR
+		   channel better */
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
+			int qlm, lane_mask;
+			for (qlm = 4; qlm < 8; qlm++) {
+				lane_mask = 1 << (qlm - 4) * 4;
+				if (lane_mask & cvmx_ilk_lane_mask[node][interface]) {
+					__cvmx_qlm_rx_equalization(node, qlm, -1);
+				}
+			}
+		}
+
 		/* Clear the boundary lock status bit */
 		ilk_rxx_int.u64 = 0;
 		ilk_rxx_int.s.word_sync_done = 1;
@@ -809,7 +821,7 @@ retry:
 	result.s.link_up = 1;
 	result.s.full_duplex = 1;
  	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
-		int qlm = cvmx_qlm_interface(xiface);
+		int qlm = cvmx_qlm_lmac(xiface, 0);
 		result.s.speed = cvmx_qlm_get_gbaud_mhz(qlm) * 64 / 67;
 	} else
 		result.s.speed = cvmx_qlm_get_gbaud_mhz(1 + interface) * 64 / 67;
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c b/arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c
index b4455df..2f23c75 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-ipd.c
@@ -120,12 +120,12 @@ static int cvmx_helper_fcs_op(int interface, int nports, int has_fcs)
 	int pknd;
 	union cvmx_pip_sub_pkind_fcsx pkind_fcsx;
 	union cvmx_pip_prt_cfgx port_cfg;
-	unsigned int node = cvmx_get_node_num();
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(interface);
 
 	if (!octeon_has_feature(OCTEON_FEATURE_PKND))
 		return 0;
 	if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
-		cvmx_helper_pki_set_fcs_op(node, interface, nports, has_fcs);
+		cvmx_helper_pki_set_fcs_op(xi.node, xi.interface, nports, has_fcs);
 		return 0;
 	}
 
@@ -232,45 +232,45 @@ static int __cvmx_helper_ipd_port_setup(int ipd_port)
  * interface. The number of ports on the interface must already
  * have been probed.
  *
- * @param interface Interface to setup IPD/PIP for
+ * @param xiface xiface to setup IPD/PIP for
  *
  * @return Zero on success, negative on failure
  */
-int __cvmx_helper_ipd_setup_interface(int interface)
+int __cvmx_helper_ipd_setup_interface(int xiface)
 {
 	cvmx_helper_interface_mode_t mode;
-	int ipd_port = cvmx_helper_get_ipd_port(interface, 0);
-	int num_ports = cvmx_helper_ports_on_interface(interface);
+	int num_ports = cvmx_helper_ports_on_interface(xiface);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int ipd_port = cvmx_helper_get_ipd_port(xiface, 0);
 	int delta;
-	unsigned int node = cvmx_get_node_num();
 
 	if (num_ports == CVMX_HELPER_CFG_INVALID_VALUE)
 		return 0;
 
 	delta = 1;
 	if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
-		if (interface < CVMX_HELPER_MAX_GMX)
+		if (xi.interface < CVMX_HELPER_MAX_GMX)
 			delta = 16;
 	}
 
 	while (num_ports--) {
-		if (!cvmx_helper_is_port_valid(interface, num_ports))
+		if (!cvmx_helper_is_port_valid(xiface, num_ports))
 			continue;
 		if (octeon_has_feature(OCTEON_FEATURE_PKI))
-			__cvmx_helper_pki_port_setup(node, ipd_port);
+			__cvmx_helper_pki_port_setup(xi.node, ipd_port);
 		else
 			__cvmx_helper_ipd_port_setup(ipd_port);
 		ipd_port += delta;
 	}
 	/* FCS settings */
-	cvmx_helper_fcs_op(interface,
-			   cvmx_helper_ports_on_interface(interface),
-			   __cvmx_helper_get_has_fcs(interface));
+	cvmx_helper_fcs_op(xiface,
+			   cvmx_helper_ports_on_interface(xiface),
+			   __cvmx_helper_get_has_fcs(xiface));
+
+	mode = cvmx_helper_interface_get_mode(xiface);
 
-	mode = cvmx_helper_interface_get_mode(interface);
-	
 	if (mode == CVMX_HELPER_INTERFACE_MODE_LOOP)
-			__cvmx_helper_loop_enable(interface);
+			__cvmx_helper_loop_enable(xiface);
 
 	return 0;
 }
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-loop.c b/arch/mips/cavium-octeon/executive/cvmx-helper-loop.c
index cc79833..c5eb1de 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-loop.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-loop.c
@@ -43,7 +43,7 @@
  * Functions for LOOP initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 97657 $<hr>
+ * <hr>$Revision: 115656 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -58,7 +58,7 @@
 #include "cvmx-pki.h"
 #endif
 
-int __cvmx_helper_loop_enumerate(int interface)
+int __cvmx_helper_loop_enumerate(int xiface)
 {
 	return OCTEON_IS_MODEL(OCTEON_CN68XX) ? 8 : (OCTEON_IS_MODEL(OCTEON_CNF71XX) ? 2 : 4);
 }
@@ -69,13 +69,13 @@ int __cvmx_helper_loop_enumerate(int interface)
  * connected to it. The LOOP interface should still be down
  * after this call.
  *
- * @param interface Interface to probe
+ * @param xiface Interface to probe
  *
  * @return Number of ports on the interface. Zero to disable.
  */
-int __cvmx_helper_loop_probe(int interface)
+int __cvmx_helper_loop_probe(int xiface)
 {
-	return __cvmx_helper_loop_enumerate(interface);
+	return __cvmx_helper_loop_enumerate(xiface);
 }
 
 /**
@@ -88,25 +88,24 @@ int __cvmx_helper_loop_probe(int interface)
  *
  * @return Zero on success, negative on failure
  */
-int __cvmx_helper_loop_enable(int interface)
+int __cvmx_helper_loop_enable(int xiface)
 {
 	cvmx_pip_prt_cfgx_t port_cfg;
 	int num_ports, index;
 	unsigned long offset;
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 
-	num_ports = __cvmx_helper_get_num_ipd_ports(interface);
-
+	num_ports = __cvmx_helper_get_num_ipd_ports(xiface);
 	/*
 	 * We need to disable length checking so packet < 64 bytes and jumbo
 	 * frames don't get errors
 	 */
 	for (index = 0; index < num_ports; index++) {
-		offset = ((octeon_has_feature(OCTEON_FEATURE_PKND)) ? cvmx_helper_get_pknd(interface, index) : cvmx_helper_get_ipd_port(interface, index));
+		offset = ((octeon_has_feature(OCTEON_FEATURE_PKND)) ? cvmx_helper_get_pknd(xiface, index) : cvmx_helper_get_ipd_port(xiface, index));
 
 		if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
-			int node = cvmx_get_node_num();
-			cvmx_pki_endis_l2_errs(node, offset, 1, 0, 0);
-			cvmx_pki_endis_fcs_check(node, offset, 0, 0);
+			cvmx_pki_endis_l2_errs(xi.node, offset, 1, 0, 0);
+                        cvmx_pki_endis_fcs_check(xi.node, offset, 0, 0);
 		} else {
 			port_cfg.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(offset));
 			port_cfg.s.maxerr_en = 0;
@@ -125,15 +124,15 @@ int __cvmx_helper_loop_enable(int interface)
 		cvmx_write_csr(CVMX_IPD_SUB_PORT_FCS, ipd_sub_port_fcs.u64);
 	}
 	/*
-	 * Set PKND and BPID for loopback ports.
-	 */
+ 	 * Set PKND and BPID for loopback ports.
+ 	 */
 	if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
 		cvmx_pko_reg_loopback_pkind_t lp_pknd;
 		cvmx_pko_reg_loopback_bpid_t lp_bpid;
 
 		for (index = 0; index < num_ports; index++) {
-			int pknd = cvmx_helper_get_pknd(interface, index);
-			int bpid = cvmx_helper_get_bpid(interface, index);
+			int pknd = cvmx_helper_get_pknd(xiface, index);
+			int bpid = cvmx_helper_get_bpid(xiface, index);
 
 			lp_pknd.u64 = cvmx_read_csr(CVMX_PKO_REG_LOOPBACK_PKIND);
 			lp_bpid.u64 = cvmx_read_csr(CVMX_PKO_REG_LOOPBACK_BPID);
@@ -178,13 +177,13 @@ int __cvmx_helper_loop_enable(int interface)
 			cvmx_write_csr(CVMX_PKO_REG_LOOPBACK_PKIND, lp_pknd.u64);
 			cvmx_write_csr(CVMX_PKO_REG_LOOPBACK_BPID, lp_bpid.u64);
 		}
-	} else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+	} else if (octeon_has_feature(OCTEON_FEATURE_BGX)) {
 		cvmx_lbk_chx_pkind_t lbk_pkind;
 
 		for (index = 0; index < num_ports; index++) {
 			lbk_pkind.u64 = 0;
-			lbk_pkind.s.pkind = cvmx_helper_get_pknd(interface, index);
-			cvmx_write_csr(CVMX_LBK_CHX_PKIND(index), lbk_pkind.u64);
+			lbk_pkind.s.pkind = cvmx_helper_get_pknd(xiface, index);
+			cvmx_write_csr_node(xi.node, CVMX_LBK_CHX_PKIND(index), lbk_pkind.u64);
 		}
 	}
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c b/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c
index 77baf91..4559307 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-npi.c
@@ -43,7 +43,7 @@
  * Functions for NPI initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 107050 $<hr>
+ * <hr>$Revision: 120569 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -84,10 +84,12 @@ void cvmx_npi_config_set_num_pipes(int num_pipes)
 int __cvmx_helper_npi_probe(int interface)
 {
 	if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
-			return 32;
-	} else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
-			return 64;
-	} else if (!(OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_X) ||
+		return 32;
+        } else if (OCTEON_IS_MODEL(OCTEON_CN73XX)) {
+		return 128;
+        } else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+                return 64;
+        } else if (!(OCTEON_IS_MODEL(OCTEON_CN52XX_PASS1_X) ||
 		   OCTEON_IS_MODEL(OCTEON_CN56XX_PASS1_X) ||
 		   OCTEON_IS_MODEL(OCTEON_CN31XX) ||
 		   OCTEON_IS_MODEL(OCTEON_CN50XX) ||
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
index 5d42f44..90d4b07 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pki.c
@@ -51,6 +51,7 @@
 #include <asm/octeon/cvmx-pki-resources.h>
 #include <asm/octeon/cvmx-helper-util.h>
 #include <asm/octeon/cvmx-ipd.h>
+#include <asm/octeon/cvmx-helper-pki.h>
 #include <asm/octeon/cvmx-global-resources.h>
 #else
 #include "cvmx.h"
@@ -551,6 +552,7 @@ EXPORT_SYMBOL(cvmx_helper_pki_port_shutdown);
  */
 void cvmx_helper_pki_shutdown(int node)
 {
+	int i, k;
 	/* remove pcam entries */
 	/* Disable PKI */
 	cvmx_pki_disable(node);
@@ -561,6 +563,25 @@ void cvmx_helper_pki_shutdown(int node)
 	/* Free all the allocated PKI resources
 	except fpa pools & aura which will be done in fpa block */
 	__cvmx_pki_global_rsrc_free(node);
+	/* Setup some configuration registers to the reset state.*/
+	for (i = 0; i < CVMX_PKI_NUM_PKIND; i++) {
+		for (k = 0; k < (int)CVMX_PKI_NUM_CLUSTER; k++) {
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(i, k), 0);			
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(i, k), 0);			
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_SKIP(i, k), 0);			
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_L2_CUSTOM(i, k), 0);			
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_PKINDX_LG_CUSTOM(i, k), 0);			
+		}
+		cvmx_write_csr_node(node, CVMX_PKI_PKINDX_ICGSEL(k), 0);			
+	}
+	for (i = 0; i < CVMX_PKI_NUM_FINAL_STYLE; i++) {
+		for (k = 0; k < (int)CVMX_PKI_NUM_CLUSTER; k++) {
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(i, k), 0);			
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(i, k), 0);			
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(i, k), 0);			
+		}
+		cvmx_write_csr_node(node, CVMX_PKI_STYLEX_BUF(k), (0x5 << 22) | 0x20);			
+	}
 }
 EXPORT_SYMBOL(cvmx_helper_pki_shutdown);
 
@@ -775,15 +796,15 @@ int __cvmx_helper_pki_port_rsrcs(int node, struct cvmx_pki_prt_schd *prtsch)
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 	int rs;
 
-       /* Erratum 22557: Disable per-port allocation for CN78XX pass 1.X */
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
+	/* Erratum 22557: Disable per-port allocation for CN78XX pass 1.X */
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) { 
 		static bool warned = false;
 		prtsch->pool_per_prt = 0;
 		if (!warned)
 			cvmx_printf("WARNING: %s: "
 			"Ports configured in single-pool mode "
 			"per erratum 22557.\n", __func__);
-			warned = true;
+		warned = true;
 	}
 	/* Reserve pool resources */
 	if (prtsch->pool_per_prt && prtsch->pool_num < 0) {
@@ -946,7 +967,7 @@ int cvmx_helper_pki_set_gbl_schd(int node, struct cvmx_pki_global_schd *gblsch)
 {
 	int rs;
 
-	if (gblsch->setup_pool) {
+	if (gblsch->setup_pool && gblsch->pool_num < 0) {
 		if (pki_helper_debug)
 			cvmx_dprintf("%s: gbl setup global pool %d buff_size %d blocks %d\n",
 				__func__, gblsch->pool_num,
@@ -969,7 +990,7 @@ int cvmx_helper_pki_set_gbl_schd(int node, struct cvmx_pki_global_schd *gblsch)
 		if (pki_helper_debug)
 			cvmx_dprintf("pool alloced is %d\n", gblsch->pool_num);
 	}
-	if (gblsch->setup_aura) {
+	if (gblsch->setup_aura && gblsch->aura_num < 0) {
 		if (pki_helper_debug)
 			cvmx_dprintf("%s: gbl setup global aura %d pool %d blocks %d\n",
 				__func__, gblsch->aura_num, gblsch->pool_num,
@@ -993,7 +1014,7 @@ int cvmx_helper_pki_set_gbl_schd(int node, struct cvmx_pki_global_schd *gblsch)
 			cvmx_dprintf("aura alloced is %d\n", gblsch->aura_num);
 
 	}
-	if (gblsch->setup_sso_grp) {
+	if (gblsch->setup_sso_grp && gblsch->sso_grp < 0) {
 		//unsigned grp_node;
 		//grp_node = (abs)(gblsch->setup_sso_grp + CVMX_PKI_FIND_AVAILABLE_RSRC);/*vinita_to_do to extract node*/
 		rs = cvmx_sso_reserve_group(node);
@@ -1701,7 +1722,7 @@ void cvmx_helper_pki_set_fcs_op(int node, int interface, int nports, int has_fcs
 {
 	int index;
 	int pknd;
-	int cluster = 0;
+	unsigned cluster = 0;
 	cvmx_pki_clx_pkindx_cfg_t pkind_cfg;
 
 	for (index = 0; index < nports; index++) {
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c
index 32700eb..e398cf7 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pko.c
@@ -42,7 +42,7 @@
  *
  * Helper Functions for the PKO
  *
- * $Id: cvmx-helper-pko.c 108660 2014-11-25 06:45:45Z cchavva $
+ * $Id: cvmx-helper-pko.c 115744 2015-04-04 04:36:36Z awilliams $
  */
 
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
@@ -62,7 +62,7 @@
 #include "cvmx-global-resources.h"
 #endif
 
-//XXX- these config data structures will go away soon!
+/* TODO: XXX- these config data structures will go away soon! */
 static CVMX_SHARED int64_t pko_fpa_config_pool = -1;
 static CVMX_SHARED uint64_t pko_fpa_config_size = 1024;
 static CVMX_SHARED uint64_t pko_fpa_config_count = 0;
@@ -85,7 +85,7 @@ void cvmx_pko_set_cmd_que_pool_config(int64_t pool, uint64_t buffer_size,
 	pko_fpa_config_pool = pool;
 	pko_fpa_config_size = buffer_size;
 	pko_fpa_config_count = buffer_count;
-	
+
 }
 EXPORT_SYMBOL(cvmx_pko_set_cmd_que_pool_config);
 
@@ -178,7 +178,7 @@ int cvmx_helper_pko_init(void)
 	if (rc < 0)
 		return rc;
 #else
-	//#	error "Pool number in kernel not implemented"
+	/* #	error "Pool number in kernel not implemented" */
 #endif
 
 	__cvmx_helper_init_port_config_data(0);
@@ -247,11 +247,12 @@ int __cvmx_helper_interface_setup_pko(int interface)
 		ipd_port++;
 	}
 	return 0;
-//NOTE:
-// Now this function is called for all chips including 68xx,
-// but on the 68xx it does not enable multiple pko_iports per
-// eport, while before it was doing 3 pko_iport per eport
-// buf the reason for that is not clear.
+	/* NOTE:
+	 * Now this function is called for all chips including 68xx,
+	 * but on the 68xx it does not enable multiple pko_iports per
+	 * eport, while before it was doing 3 pko_iport per eport
+	 * buf the reason for that is not clear.
+	 */
 }
 
 /**
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c b/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
index c98fc67..54a4d62 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-pko3.c
@@ -42,7 +42,7 @@
  *
  * PKOv3 helper file
  */
-//#define	__SUPPORT_PFC_ON_XAUI
+/* #define	__SUPPORT_PFC_ON_XAUI */
 
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/module.h>
@@ -70,46 +70,14 @@
 #include "cvmx-helper-cfg.h"
 #endif
 
-/* bootloader has limited memory, not much traffic */
-#if defined(__U_BOOT__)
-#define CVMX_PKO3_POOL_BUFFERS (1024)
-#endif
-
-/*
- * PKO3 requires 4 buffers for each active Descriptor Queue,
- * and because it is not known how many DQs will in fact be used
- * when the PKO pool is populated, it is allocated the maximum
- * number it may required.
- * The additional 1K buffers are provisioned to acomodate
- * jump buffers for long descriptor commands, that should
- * be rarely used.
- */
-#ifndef CVMX_PKO3_POOL_BUFFERS
-/* Calculate buffer count based on maximum queue depth if defined */
-#ifdef CVMX_PKO3_DQ_MAX_DEPTH
-/* Number of command words per buffer */
-#define _NUMW	(4*1024/8)
-/* Assume 8 concurrently active, fully loaded queues */
-#define _NUMQ	1024
-/* Assume worst case of 16 words per command per DQ */
-#define _DQ_SZ	16
-/* Combine the above guesstimates into the total buffer count */
-#define CVMX_PKO3_POOL_BUFFERS (CVMX_PKO3_DQ_MAX_DEPTH*_DQ_SZ/_NUMW*_NUMQ+1024)
-#else
-#define CVMX_PKO3_POOL_BUFFERS (1024*4+1024)
-#endif
-#endif
-
-/* Simulator has limited memory, use fewer buffers */
-#ifndef	CVMX_PKO3_POOL_BUFS_SIM
-#define CVMX_PKO3_POOL_BUFS_SIM (1024*4+1024)
-#endif
-
 /* channels are present at L2 queue level by default */
-static const int cvmx_pko_default_channel_level = 0;
+static const enum cvmx_pko3_level_e
+cvmx_pko_default_channel_level = CVMX_PKO_L2_QUEUES;
 
 static const int debug = 0;
 
+static CVMX_SHARED int __pko_pkt_budget, __pko_pkt_quota;
+
 /* These global variables are relevant for boot CPU only */
 static CVMX_SHARED cvmx_fpa3_gaura_t __cvmx_pko3_aura[CVMX_MAX_NODES];
 
@@ -160,15 +128,56 @@ static int __cvmx_pko3_config_memory(unsigned node)
 	cvmx_fpa3_gaura_t aura;
 	int aura_num;
 	unsigned buf_count;
+	bool small_mem;
+	int i, num_intf = 0;
+	const unsigned pkt_per_buf =
+		(CVMX_PKO3_POOL_BUFFER_SIZE / sizeof(uint64_t) / 16);
+	const unsigned base_buf_count = 1024*4;
+
+	/* Simulator has limited memory, but uses one interface at a time */
+	small_mem = cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM;
+
+	/* Count the number of live interfaces */
+	for (i = 0; i < cvmx_helper_get_number_of_interfaces(); i++) {
+		int xiface = cvmx_helper_node_interface_to_xiface(node, i);
+		if ( CVMX_HELPER_INTERFACE_MODE_DISABLED !=
+		    cvmx_helper_interface_get_mode(xiface))
+			num_intf ++;
+	}
 
-	/* Simulator has limited memory */
-	if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
-		buf_count = CVMX_PKO3_POOL_BUFS_SIM;
-	else
-		buf_count = CVMX_PKO3_POOL_BUFFERS;
+#if defined(__U_BOOT__)
+	buf_count = 1024;
+	__pko_pkt_quota = buf_count * pkt_per_buf;
+	__pko_pkt_budget = __pko_pkt_quota *  num_intf;
+	(void) small_mem; (void) base_buf_count;
+#else /* !U_BOOT */
+	if (small_mem) {
+		buf_count = base_buf_count + 1024;
+		__pko_pkt_quota = buf_count * pkt_per_buf;
+		__pko_pkt_budget = __pko_pkt_quota *  num_intf;
+	} else {
+		buf_count = 1024 / pkt_per_buf;
+
+		/* Make room for 2 milliseconds at maximum packet rate */
+		buf_count += 100 * 2000 / pkt_per_buf;
+
+		/* Save per-interface queue depth quota */
+		__pko_pkt_quota = buf_count * pkt_per_buf;
 
-	cvmx_dprintf("%s: creating AURA with %u buffers\n",
-		__func__, buf_count);
+		/* Multiply by interface count */
+		buf_count *= num_intf;
+
+		/* Save total packet budget */
+		__pko_pkt_budget = buf_count * pkt_per_buf;
+
+		/* Add 4 more buffers per DQ */
+		buf_count += base_buf_count;
+	}
+#endif /* !U_BOOT */
+
+	cvmx_dprintf("%s: creating AURA with %u buffers for up to %d packets, "
+		"%d per interface\n",
+		__func__, buf_count, __pko_pkt_budget, __pko_pkt_quota);
 
 	aura = cvmx_fpa3_setup_aura_and_pool(node, -1,
 		"PKO3 AURA", NULL,
@@ -186,153 +195,36 @@ static int __cvmx_pko3_config_memory(unsigned node)
 
 	return aura_num;
 }
-
-
-#endif
-
-/** Initialize a single ILK link
- *
- * Each ILK link is one interface, the port portion of IPD
- * represents a logical channel.
- * The number of channels for each interface is derived from the ILK
- * module configuration.
- */
-static int __cvmx_pko3_config_ilk_interface(int xiface,
-	unsigned num_dq, bool prioritized)
-{
-	int l1_q_num;
-	int l2_q_num;
-	int res;
-	int res_owner;
-	int pko_mac_num;
-	unsigned num_chans, i;
-	uint16_t ipd_port;
-	int prio;
-	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-
-	if (prioritized && num_dq > 1)
-		prio = num_dq;
-	else
-		prio = -1;
-
-	num_chans = __cvmx_helper_ilk_enumerate(xiface);
-
-	if(debug)
-		cvmx_dprintf("%s: configuring ILK xiface %u:%u with "
-				"%u chans %u queues each\n",
-				__FUNCTION__, xi.node, xi.interface,
-				num_chans, num_dq);
-
-	/* ILK channels all go to the same mac */
-	pko_mac_num = __cvmx_pko3_get_mac_num(xiface, 0);
-	if (pko_mac_num < 0) {
-                cvmx_dprintf ("%s: ERROR Invalid interface\n", __FUNCTION__);
-		return -1;
-	}
-
-	/* Resources of all channels on this link have common owner */
-	ipd_port = cvmx_helper_get_ipd_port(xiface, 0);
-
-	/* Build an identifiable owner */
-	res_owner = __cvmx_helper_pko3_res_owner(ipd_port);
-
-	/* Reserve port queue to make sure the MAC is not already configured */
-	l1_q_num = pko_mac_num;
-        l1_q_num = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_PORT_QUEUES,
-				res_owner, l1_q_num, 1);
-
-	if (l1_q_num != pko_mac_num) {
-                cvmx_dprintf ("%s: ERROR Reserving L1 PQ\n", __FUNCTION__);
-		return -1;
-	}
-
-
-        /* allocate level 2 queues, one per channel */
-        l2_q_num = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L2_QUEUES, res_owner,
-					 -1, num_chans);
-        if (l2_q_num < 0) {
-                cvmx_dprintf ("%s: ERROR allocation L2 SQ\n", __FUNCTION__);
-                return -1;
-        }
-
-
-	/* Configre <num_chans> children for MAC, with Fair-RR scheduling */
-	res = cvmx_pko3_pq_config_children( xi.node,
-			pko_mac_num, l2_q_num, num_chans, -1);
-
-	if (res < 0) {
-		cvmx_dprintf("%s: ERROR: Could not setup ILK Channel queues\n",
-			__FUNCTION__);
-		return -1;
-	}
-
-	/* Configure children with one DQ per channel */
-	for (i = 0; i < num_chans; i++) {
-		int l3_q, l4_q, l5_q, dq, res;
-
-		l3_q = l4_q = l5_q = dq = -1;
-		ipd_port = cvmx_helper_get_ipd_port(xiface, i);
-
-		/* map channels to l2 queues */
-		cvmx_pko3_map_channel(xi.node, l1_q_num, l2_q_num+i, ipd_port);
-
-		l3_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L3_QUEUES,
-			res_owner, -1, 1);
-		if(l3_q < 0) goto _fail;
-
-		res = cvmx_pko3_sq_config_children(xi.node, 2, l2_q_num+i, l3_q, 1, 1);
-		if(res < 0) goto _fail;
-
-		l4_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L4_QUEUES,
-			res_owner, -1, 1);
-		if(l4_q < 0) goto _fail;
-		res = cvmx_pko3_sq_config_children(xi.node, 3, l3_q, l4_q, 1, 1);
-		if(res < 0) goto _fail;
-
-		l5_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L5_QUEUES,
-			res_owner, -1, 1);
-		if(l5_q < 0) goto _fail;
-		res = cvmx_pko3_sq_config_children(xi.node, 4, l4_q, l5_q, 1, 1);
-		if(res < 0) goto _fail;
-
-		dq = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_DESCR_QUEUES,
-			res_owner, -1, num_dq);
-		if(dq < 0) goto _fail;
-
-		res = cvmx_pko3_sq_config_children(xi.node, 5, l5_q,
-			dq, num_dq, prio);
-		if(res < 0) goto _fail;
-
-		/* register DQ range with the translation table */
-		res = __cvmx_pko3_ipd_dq_register(xiface, i, dq, num_dq);
-		if(res < 0) goto _fail;
-	}
-
-	return 0;
-  _fail:
-	cvmx_dprintf("ERROR: %s: configuring queues for xiface %u:%u chan %u\n",
-				__FUNCTION__, xi.node, xi.interface, i);
-	return -1;
-}
+#endif /* !CVMX_BUILD_FOR_LINUX_KERNEL */
 
 
 /** Initialize a channelized port
- * This is intended for LOOP and NPI interfaces which have one MAC
+ * This is intended for LOOP, ILK and NPI interfaces which have one MAC
  * per interface and need a channel per subinterface (e.g. ring).
- * This function is somewhat similar to __cvmx_pko3_config_ilk_interface()
- * but are kept separate for easier maintenance.
+ * Each channel then may have 'num_queues' descriptor queues
+ * attached to it, which can also be prioritized or fair.
  */
 static int __cvmx_pko3_config_chan_interface( int xiface, unsigned num_chans,
 	uint8_t num_queues, bool prioritized)
 {
 	int l1_q_num;
-	int l2_q_num;
+	int l2_q_base;
+	enum cvmx_pko3_level_e level;
 	int res;
+	int parent_q, child_q;
+	unsigned chan, dq;
 	int pko_mac_num;
 	uint16_t ipd_port;
 	int res_owner, prio;
 	unsigned i;
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	unsigned node = xi.node;
+	char b1[12];
+
+	if (num_queues == 0)
+		num_queues = 1;
+	if ((cvmx_pko3_num_level_queues(CVMX_PKO_DESCR_QUEUES) / num_chans) < 3)
+		num_queues = 1;
 
 	if (prioritized && num_queues > 1)
 		prio = num_queues;
@@ -348,7 +240,7 @@ static int __cvmx_pko3_config_chan_interface( int xiface, unsigned num_chans,
 	/* all channels all go to the same mac */
 	pko_mac_num = __cvmx_pko3_get_mac_num(xiface, 0);
 	if (pko_mac_num < 0) {
-                cvmx_dprintf ("%s: ERROR Invalid interface\n", __FUNCTION__);
+                cvmx_printf ("ERROR: %s: Invalid interface\n", __func__);
 		return -1;
 	}
 
@@ -358,86 +250,110 @@ static int __cvmx_pko3_config_chan_interface( int xiface, unsigned num_chans,
 	/* Build an identifiable owner */
 	res_owner = __cvmx_helper_pko3_res_owner(ipd_port);
 
+	/* Start configuration at L1/PQ */
+	level = CVMX_PKO_PORT_QUEUES;
+
 	/* Reserve port queue to make sure the MAC is not already configured */
-	l1_q_num = pko_mac_num;
-        l1_q_num = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_PORT_QUEUES,
-				res_owner, l1_q_num, 1);
+        l1_q_num = cvmx_pko_alloc_queues(node, level, res_owner, -1, 1);
+
+	if (l1_q_num < 0) {
+                cvmx_printf ("ERROR: %s: Reserving L1 PQ\n", __func__);
+		return -1;
+	}
 
-	if (l1_q_num != pko_mac_num) {
-                cvmx_dprintf ("%s: ERROR Reserving L1 PQ\n", __FUNCTION__);
+	res = cvmx_pko3_pq_config(node, pko_mac_num, l1_q_num);
+	if (res < 0) {
+                cvmx_printf ("ERROR: %s: Configuring L1 PQ\n", __func__);
 		return -1;
 	}
 
+	/* next queue level = L2/SQ */
+	level = __cvmx_pko3_sq_lvl_next(level);
 
         /* allocate level 2 queues, one per channel */
-        l2_q_num = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L2_QUEUES, res_owner,
+        l2_q_base = cvmx_pko_alloc_queues(node, level, res_owner,
 					 -1, num_chans);
-        if (l2_q_num < 0) {
-                cvmx_dprintf ("%s: ERROR allocation L2 SQ\n", __FUNCTION__);
+        if (l2_q_base < 0) {
+                cvmx_printf ("ERROR: %s: allocation L2 SQ\n", __func__);
                 return -1;
         }
 
-
-	/* Configre <num_chans> children for MAC, non-prioritized */
-	res = cvmx_pko3_pq_config_children( xi.node,
-			l1_q_num, l2_q_num, num_chans, -1);
+	/* Configre <num_chans> L2 children for PQ, non-prioritized */
+	res = cvmx_pko3_sq_config_children(node, level,
+			l1_q_num, l2_q_base, num_chans, -1);
 
 	if (res < 0) {
-		cvmx_dprintf("%s: ERROR: Failed channel queues\n",
-			__FUNCTION__);
+		cvmx_printf("ERROR: %s: Failed channel queues\n", __func__);
 		return -1;
 	}
 
-	/* Configure children with one DQ per channel */
-	for (i = 0; i < num_chans; i++) {
-		int l3_q, l4_q, l5_q, dq, res;
-		unsigned chan = i;
+	/* map channels to l2 queues */
+	for (chan = 0; chan < num_chans; chan++) {
+		ipd_port = cvmx_helper_get_ipd_port(xiface, chan);
+		cvmx_pko3_map_channel(node,
+			l1_q_num, l2_q_base + chan, ipd_port);
+	}
 
-		l3_q = l4_q = l5_q = dq = -1;
+	/* next queue level = L3/SQ */
+	level = __cvmx_pko3_sq_lvl_next(level);
+	parent_q = l2_q_base;
 
-		ipd_port = cvmx_helper_get_ipd_port(xiface, chan);
+	do {
+		child_q = cvmx_pko_alloc_queues(node, level,
+			res_owner, -1, num_chans);
 
-		/* map channels to l2 queues */
-		cvmx_pko3_map_channel(xi.node, l1_q_num, l2_q_num+chan,
-			ipd_port);
+		if (child_q < 0) {
+			cvmx_printf ("ERROR: %s: allocating %s\n",
+				__func__,
+				__cvmx_pko3_sq_str(b1, level, child_q));
+			return -1;
+		}
 
-		l3_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L3_QUEUES,
-			res_owner, -1, 1);
-		if(l3_q < 0) goto _fail;
+		for (i = 0; i < num_chans; i++) {
+			res = cvmx_pko3_sq_config_children(node, level,
+				parent_q + i, child_q + i, 1, 1);
 
-		res = cvmx_pko3_sq_config_children(xi.node, 2, l2_q_num+chan,
-			l3_q, 1, 1);
-		if(res < 0) goto _fail;
+			if (res < 0) {
+				cvmx_printf ("ERROR: %s: configuring %s\n",
+					__func__,
+					__cvmx_pko3_sq_str(b1, level, child_q));
+				return -1;
+			}
 
-		l4_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L4_QUEUES,
-			res_owner, -1, 1);
-		if(l4_q < 0) goto _fail;
+		} /* for i */
 
-		res = cvmx_pko3_sq_config_children(xi.node, 3,
-			l3_q, l4_q, 1, 1);
-		if(res < 0) goto _fail;
+		parent_q = child_q;
+		level = __cvmx_pko3_sq_lvl_next(level);
 
-		l5_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L5_QUEUES,
-			res_owner, -1, 1);
-		if(l5_q < 0) goto _fail;
-		res = cvmx_pko3_sq_config_children(xi.node, 4,
-			l4_q, l5_q, 1, 1);
-		if(res < 0) goto _fail;
+	/* Terminate loop on DQ level, it has special handling */
+	} while (level != CVMX_PKO_DESCR_QUEUES &&
+		level != CVMX_PKO_LEVEL_INVAL);
+
+	if (level != CVMX_PKO_DESCR_QUEUES) {
+		cvmx_printf("ERROR: %s: level sequence error\n", __func__);
+		return -1;
+	}
 
-		dq = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_DESCR_QUEUES,
+	/* Configure DQs, num_dqs per chan */
+	for (chan = 0; chan < num_chans; chan++) {
+
+		res = cvmx_pko_alloc_queues(node, level,
 			res_owner, -1, num_queues);
-		if(dq < 0) goto _fail;
+
+		if(res < 0) goto _fail;
+		dq = res;
 
 		if(OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_0) && (dq & 7))
 			cvmx_dprintf("WARNING: %s: DQ# %u not integral of 8\n",
 				__func__, dq);
 
-		res = cvmx_pko3_sq_config_children(xi.node, 5, l5_q,
+		res = cvmx_pko3_sq_config_children(node, level, parent_q + chan,
 			dq, num_queues, prio);
 		if(res < 0) goto _fail;
 
 		/* register DQ range with the translation table */
-		res = __cvmx_pko3_ipd_dq_register(xiface, chan, dq, num_queues);
+		res = __cvmx_pko3_ipd_dq_register(xiface,
+				chan, dq, num_queues);
 		if(res < 0) goto _fail;
 	}
 
@@ -462,26 +378,29 @@ static int __cvmx_pko3_config_chan_interface( int xiface, unsigned num_chans,
  */
 static int __cvmx_pko3_config_pfc_interface(int xiface, unsigned port)
 {
-	int l1_q_num;
-	int l2_q_num;
-	int res;
+	enum cvmx_pko3_level_e level;
 	int pko_mac_num;
-	int l3_q, l4_q, l5_q, dq;
+	int l1_q_num, l2_q_base;
+	int child_q, parent_q;
+	int dq_base;
+	int res;
 	const unsigned num_chans = 8;
 	cvmx_xiface_t xi = cvmx_helper_xiface_to_node_interface(xiface);
 	unsigned node = xi.node;
 	uint16_t ipd_port;
 	int res_owner;
+	char b1[12];
 	unsigned i;
 
-	if(debug)
-		cvmx_dprintf("%s: configuring xiface %u:%u port %u with %u PFC channels\n",
-			__FUNCTION__, node, xi.interface, port, num_chans);
+	if (debug)
+		cvmx_dprintf("%s: configuring xiface %u:%u port %u "
+			"with %u PFC channels\n",
+			__func__, node, xi.interface, port, num_chans);
 
 	/* Get MAC number for the iface/port */
 	pko_mac_num = __cvmx_pko3_get_mac_num(xiface, port);
 	if (pko_mac_num < 0) {
-		cvmx_dprintf ("%s: ERROR Invalid interface\n", __FUNCTION__);
+		cvmx_printf ("ERROR: %s: Invalid interface\n", __func__);
 		return -1;
 	}
 
@@ -490,79 +409,126 @@ static int __cvmx_pko3_config_pfc_interface(int xiface, unsigned port)
 	/* Build an identifiable owner identifier */
 	res_owner = __cvmx_helper_pko3_res_owner(ipd_port);
 
+	level = CVMX_PKO_PORT_QUEUES;
+
 	/* Allocate port queue to make sure the MAC is not already configured */
-	l1_q_num = cvmx_pko_alloc_queues(node, CVMX_PKO_PORT_QUEUES,
-				res_owner, pko_mac_num, 1);
+	l1_q_num = cvmx_pko_alloc_queues(node, level, res_owner, -1, 1);
 
-	if (l1_q_num != pko_mac_num) {
-		cvmx_dprintf ("%s: ERROR allocation L1 SQ\n", __FUNCTION__);
+	if (l1_q_num < 0) {
+		cvmx_printf ("ERROR: %s: allocation L1 PQ\n", __func__);
 		return -1;
 	}
 
-
-	/* allocate or reserve level 2 queues */
-	l2_q_num = cvmx_pko_alloc_queues(node, CVMX_PKO_L2_QUEUES, res_owner,
-					 -1, num_chans);
-	if (l2_q_num < 0) {
-		cvmx_dprintf ("%s: ERROR allocation L2 SQ\n", __FUNCTION__);
+	res = cvmx_pko3_pq_config(xi.node, pko_mac_num, l1_q_num);
+	if (res < 0) {
+                cvmx_printf ("ERROR: %s: Configuring %s\n", __func__,
+				__cvmx_pko3_sq_str(b1, level, l1_q_num));
 		return -1;
 	}
 
+	/* Determine the next queue level */
+	level = __cvmx_pko3_sq_lvl_next(level);
 
-	/* Configre <num_chans> children for MAC, with static priority */
-	res = cvmx_pko3_pq_config_children( node,
-			pko_mac_num, l2_q_num, num_chans, num_chans);
-
-	if (res < 0) {
-		cvmx_dprintf("Error: Could not setup PFC Channel queues\n");
+	/* Allocate 'num_chans' L2 queues, one per channel */
+	l2_q_base = cvmx_pko_alloc_queues(node, level, res_owner,
+		-1, num_chans);
+	if (l2_q_base < 0) {
+		cvmx_printf ("ERROR: %s: allocation L2 SQ\n", __func__);
 		return -1;
 	}
 
-	/* Allocate all SQ levels at once to assure contigous range */
-	l3_q = cvmx_pko_alloc_queues(node, CVMX_PKO_L3_QUEUES,
-			res_owner, -1, num_chans);
-	l4_q = cvmx_pko_alloc_queues(node, CVMX_PKO_L4_QUEUES,
-			res_owner, -1, num_chans);
-	l5_q = cvmx_pko_alloc_queues(node, CVMX_PKO_L5_QUEUES,
-			res_owner, -1, num_chans);
-	dq = cvmx_pko_alloc_queues(node, CVMX_PKO_DESCR_QUEUES,
-			res_owner, -1, num_chans);
-	if (l3_q < 0 || l4_q < 0 || l5_q < 0 ||dq < 0) {
-		cvmx_dprintf("%s: ERROR:could not allocate queues, "
-			"xiface %u:%u port %u\n",
-			__FUNCTION__, xi.node, xi.interface, port);
+	/* Configre <num_chans> L2 children for PQ, with static priority */
+	res = cvmx_pko3_sq_config_children(node, level,
+			l1_q_num, l2_q_base, num_chans, num_chans);
+
+	if (res < 0) {
+                cvmx_printf ("ERROR: %s: Configuring %s for PFC\n", __func__,
+				__cvmx_pko3_sq_str(b1, level, l1_q_num));
 		return -1;
 	}
 
-	/* Configure children with one DQ per channel */
+	/* Map each of the allocated channels */
 	for (i = 0; i < num_chans; i++) {
-		uint16_t chan, dq_num;
-		/* <i> moves in priority order, 0=highest, 7=lowest */
+		uint16_t chan;
 
 		/* Get CHAN_E value for this PFC channel, PCP in low 3 bits */
 		chan = ipd_port | cvmx_helper_prio2qos(i);
 
-		/* map channels to L2 queues */
-		cvmx_pko3_map_channel(node, l1_q_num, l2_q_num+i, chan);
+		cvmx_pko3_map_channel(node, l1_q_num, l2_q_base + i, chan);
+
+	}
+
+	/* Iterate through the levels until DQ and allocate 'num_chans'
+	 * consecutive queues at each level and hook them up
+	 * one-to-one with the parent level queues
+	 */
+
+	parent_q = l2_q_base;
+	level = __cvmx_pko3_sq_lvl_next(level);
 
-		cvmx_pko3_sq_config_children(node, 2, l2_q_num+i, l3_q+i, 1, 1);
+	do {
 
-		cvmx_pko3_sq_config_children(node, 3, l3_q+i, l4_q+i, 1, 1);
+		child_q = cvmx_pko_alloc_queues(node, level,
+			res_owner, -1, num_chans);
+
+		if (child_q < 0) {
+			cvmx_printf ("ERROR: %s: allocating %s\n",
+				__func__,
+				__cvmx_pko3_sq_str(b1, level, child_q));
+			return -1;
+		}
+
+		for (i = 0; i < num_chans; i++) {
+			res = cvmx_pko3_sq_config_children(node, level,
+				parent_q + i, child_q + i, 1, 1);
+
+			if (res < 0) {
+				cvmx_printf ("ERROR: %s: configuring %s\n",
+					__func__,
+					__cvmx_pko3_sq_str(b1, level, child_q));
+				return -1;
+			}
+
+		} /* for i */
+
+		parent_q = child_q;
+		level = __cvmx_pko3_sq_lvl_next(level);
+
+	/* Terminate loop on DQ level, it has special handling */
+	} while (level != CVMX_PKO_DESCR_QUEUES &&
+		level != CVMX_PKO_LEVEL_INVAL);
+
+	if (level != CVMX_PKO_DESCR_QUEUES) {
+		cvmx_printf("ERROR: %s: level sequence error\n", __func__);
+		return -1;
+	}
 
-		cvmx_pko3_sq_config_children(node, 4, l4_q+i, l5_q+i, 1, 1);
+	dq_base = cvmx_pko_alloc_queues(node, level, res_owner, -1, num_chans);
+	if (dq_base < 0) {
+		cvmx_printf ("ERROR: %s: allocating %s\n", __func__,
+				__cvmx_pko3_sq_str(b1, level, dq_base));
+		return -1;
+	}
 
-		/* Configure DQs in QoS order, so that QoS/PCP can be index */
-		dq_num = dq + cvmx_helper_prio2qos(i);
-		cvmx_pko3_sq_config_children(node, 5, l5_q+i, dq_num, 1, 1);
+	/* Configure DQs in QoS order, so that QoS/PCP can be index */
+	for (i = 0; i < num_chans; i++) {
+		int dq_num = dq_base + cvmx_helper_prio2qos(i);
+		res = cvmx_pko3_sq_config_children(node, level,
+			parent_q + i, dq_num, 1, 1);
+		if (res < 0) {
+			cvmx_printf ("ERROR: %s: configuring %s\n", __func__,
+				__cvmx_pko3_sq_str(b1, level, dq_num));
+			return -1;
+		}
 	}
 
 	/* register entire DQ range with the IPD translation table */
-	__cvmx_pko3_ipd_dq_register(xiface, port, dq, num_chans);
+	__cvmx_pko3_ipd_dq_register(xiface, port, dq_base, num_chans);
 
 	return 0;
 }
 
-/** 
+/**
  * Initialize a simple interface with a a given number of
  * fair or prioritized queues.
  * This function will assign one channel per sub-interface.
@@ -570,14 +536,17 @@ static int __cvmx_pko3_config_pfc_interface(int xiface, unsigned port)
 int __cvmx_pko3_config_gen_interface(int xiface, uint8_t subif,
 	uint8_t num_queues, bool prioritized)
 {
+	cvmx_xiface_t xi = cvmx_helper_xiface_to_node_interface(xiface);
+	uint8_t node = xi.node;
 	int l1_q_num;
-	int l2_q_num;
+	int parent_q, child_q;
+	int dq;
 	int res, res_owner;
 	int pko_mac_num;
-	int l3_q, l4_q, l5_q, dq;
+	enum cvmx_pko3_level_e level;
 	uint16_t ipd_port;
-	cvmx_xiface_t xi = cvmx_helper_xiface_to_node_interface(xiface);
 	int static_pri;
+	char b1[12];
 
 #if defined(__U_BOOT__)
 	num_queues = 1;
@@ -585,10 +554,19 @@ int __cvmx_pko3_config_gen_interface(int xiface, uint8_t subif,
 
 	if (num_queues == 0) {
 		num_queues = 1;
-		cvmx_dprintf("%s: WARNING xiface %#x misconfigured\n",
+		cvmx_printf("WARNING: %s: xiface %#x misconfigured\n",
 			__func__, xiface);
 	}
 
+	/* Configure DQs relative priority (a.k.a. scheduling) */
+	if (prioritized) {
+		/* With 8 queues or fewer, use static priority, else WRR */
+		static_pri = (num_queues < 9)? num_queues: 0;
+	} else {
+		/* Set equal-RR scheduling among queues */
+		static_pri = -1;
+	}
+
 	if (debug)
 		cvmx_dprintf("%s: configuring xiface %u:%u/%u nq=%u %s\n",
 			     __FUNCTION__, xi.node, xi.interface, subif,
@@ -597,8 +575,8 @@ int __cvmx_pko3_config_gen_interface(int xiface, uint8_t subif,
 	/* Get MAC number for the iface/port */
 	pko_mac_num = __cvmx_pko3_get_mac_num(xiface, subif);
 	if (pko_mac_num < 0) {
-		cvmx_dprintf ("%s: ERROR Invalid interface %u:%u\n",
-			__FUNCTION__, xi.node, xi.interface);
+		cvmx_printf ("ERROR: %s: Invalid interface %u:%u\n",
+			__func__, xi.node, xi.interface);
 		return -1;
 	}
 
@@ -612,74 +590,86 @@ int __cvmx_pko3_config_gen_interface(int xiface, uint8_t subif,
 	/* Build an identifiable owner identifier */
 	res_owner = __cvmx_helper_pko3_res_owner(ipd_port);
 
+	level = CVMX_PKO_PORT_QUEUES;
+
 	/* Reserve port queue to make sure the MAC is not already configured */
-	l1_q_num = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_PORT_QUEUES,
-					 res_owner, pko_mac_num, 1);
+	l1_q_num = cvmx_pko_alloc_queues(node, level, res_owner, -1, 1);
 
-	if (l1_q_num != pko_mac_num) {
-		cvmx_dprintf("%s: ERROR xiface %u:%u/%u"
-			" failed allocation L1 SQ\n",
-			__FUNCTION__, xi.node, xi.interface, subif);
+	if (l1_q_num < 0) {
+		cvmx_printf("ERROR %s: xiface %u:%u/%u"
+			" failed allocation L1 PQ\n",
+			__func__, xi.node, xi.interface, subif);
 		return -1;
 	}
 
-	/* allocate or reserve level 2 queues */
-	l2_q_num = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L2_QUEUES, res_owner,
-				-1, 1);
-	if (l2_q_num < 0) {
-		cvmx_dprintf("%s: ERROR xiface %u:%u/%u"
-			"  failed allocation L2 SQ\n",
-			__FUNCTION__, xi.node, xi.interface, subif);
+	res = cvmx_pko3_pq_config(node, pko_mac_num, l1_q_num);
+	if (res < 0) {
+                cvmx_printf ("ERROR %s: Configuring L1 PQ\n", __func__);
 		return -1;
 	}
 
+	parent_q = l1_q_num;
 
-	/* Configre L2 SQ */
-	res = cvmx_pko3_pq_config_children(xi.node, pko_mac_num,
-				l2_q_num, 1, 1);
+	/* Determine the next queue level */
+	level = __cvmx_pko3_sq_lvl_next(level);
 
-	if (res < 0) {
-		cvmx_dprintf("%s: ERROR xiface %u:%u/%u"
-			" failed configuring PQ\n",
-			__FUNCTION__, xi.node, xi.interface, subif);
+	/* Simply chain queues 1-to-1 from L2 to one before DQ level */
+	do {
+
+		/* allocate next level queue */
+		child_q = cvmx_pko_alloc_queues(node, level, res_owner, -1, 1);
+
+		if (child_q < 0) {
+			cvmx_printf ("ERROR: %s: allocating %s\n",
+				__func__,
+				__cvmx_pko3_sq_str(b1, level, child_q));
+			return -1;
+		}
+
+		/* Configre newly allocated queue */
+		res = cvmx_pko3_sq_config_children(node, level,
+			parent_q, child_q, 1, 1);
+
+		if (res < 0) {
+			cvmx_printf ("ERROR: %s: configuring %s\n",
+				__func__,
+				__cvmx_pko3_sq_str(b1, level, child_q));
+			return -1;
+		}
+
+		/* map IPD/channel to L2/L3 queues */
+		if (level == cvmx_pko_default_channel_level)
+			cvmx_pko3_map_channel(node,
+				l1_q_num, child_q, ipd_port);
+
+		/* Prepare for next level */
+		level = __cvmx_pko3_sq_lvl_next(level);
+		parent_q = child_q;
+
+	/* Terminate loop on DQ level, it has special handling */
+	} while (level != CVMX_PKO_DESCR_QUEUES &&
+		level != CVMX_PKO_LEVEL_INVAL);
+
+	if (level != CVMX_PKO_DESCR_QUEUES) {
+		cvmx_printf("ERROR: %s: level sequence error\n", __func__);
 		return -1;
 	}
 
-	l3_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L3_QUEUES,
-				res_owner, -1, 1);
-	l4_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L4_QUEUES,
-				res_owner, -1, 1);
-	l5_q = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_L5_QUEUES,
-				res_owner, -1, 1);
-	dq = cvmx_pko_alloc_queues(xi.node, CVMX_PKO_DESCR_QUEUES,
-				res_owner, -1, num_queues);
+	/* Allocate descriptor queues for the port */
+	dq = cvmx_pko_alloc_queues(node, level, res_owner, -1, num_queues);
 	if (dq < 0) {
-		cvmx_dprintf("%s: ERROR xiface %u:%u/%u"
-			" failed configuring DQs\n",
-			__FUNCTION__, xi.node, xi.interface, subif);
+		cvmx_printf("ERROR: %s: could not reserve DQs\n", __func__);
 		return -1;
 	}
 
-	/* Configure hierarchy */
-	cvmx_pko3_sq_config_children(xi.node, 2, l2_q_num, l3_q, 1, 1);
-	cvmx_pko3_sq_config_children(xi.node, 3, l3_q, l4_q, 1, 1);
-	cvmx_pko3_sq_config_children(xi.node, 4, l4_q, l5_q, 1, 1);
-
-	/* Configure DQs relative priority (a.k.a. scheduling) */
-	if (prioritized) {
-		/* With 8 queues or fewer, use static priority, else WRR */
-		static_pri = (num_queues < 9)? num_queues: 0;
-	} else {
-		/* Set equal-RR scheduling among queues */
-		static_pri = -1;
+	res = cvmx_pko3_sq_config_children(node, level, parent_q,
+		dq, num_queues, static_pri);
+	if (res < 0) {
+		cvmx_printf ("ERROR: %s: configuring %s\n", __func__,
+				__cvmx_pko3_sq_str(b1, level, dq));
+		return -1;
 	}
 
-	cvmx_pko3_sq_config_children(xi.node, 5, l5_q, dq,
-		num_queues, static_pri);
-
-	/* map IPD/channel to L2 queues */
-	cvmx_pko3_map_channel(xi.node, l1_q_num, l2_q_num, ipd_port);
-
 	/* register DQ/IPD translation */
 	__cvmx_pko3_ipd_dq_register(xiface, subif, dq, num_queues);
 
@@ -705,13 +695,22 @@ EXPORT_SYMBOL(__cvmx_pko3_config_gen_interface);
 static int __cvmx_pko3_config_null_interface(unsigned int node)
 {
 	int l1_q_num;
-	int l2_q_num;
-	int l3_q, l4_q, l5_q;
+	int parent_q, child_q;
+	enum cvmx_pko3_level_e level;
 	int i, res, res_owner;
 	int xiface, ipd_port;
 	int num_dq = 1;	/* # of DQs for NULL */
 	const int dq = 0;	/* Reserve DQ#0 for NULL */
-	const int pko_mac_num = 0x1C; /* MAC# 28 virtual MAC for NULL */
+	char pko_mac_num;
+	char b1[12];
+
+	if(OCTEON_IS_MODEL(OCTEON_CN78XX))
+		pko_mac_num = 0x1C; /* MAC# 28 virtual MAC for NULL */
+	else if (OCTEON_IS_MODEL(OCTEON_CN73XX) ||
+		OCTEON_IS_MODEL(OCTEON_CNF75XX))
+		pko_mac_num = 0x0F; /* MAC# 16 !? virtual MAC for NULL */
+	else
+		return -1;
 
 	if(OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_0))
 		num_dq = 8;
@@ -729,50 +728,79 @@ static int __cvmx_pko3_config_null_interface(unsigned int node)
 		return -1;
 	}
 
-	/* Reserve port queue to make sure the MAC is not already configured */
-	l1_q_num = cvmx_pko_alloc_queues(node, CVMX_PKO_PORT_QUEUES,
-				res_owner, pko_mac_num, 1);
+	level = CVMX_PKO_PORT_QUEUES;
 
-	if (l1_q_num != pko_mac_num) {
-		cvmx_dprintf ("%s: ERROR reserving L1 SQ\n", __FUNCTION__);
+	/* Allocate a port queue */
+	l1_q_num = cvmx_pko_alloc_queues(node, level, res_owner, -1, 1);
+
+	if (l1_q_num < 0) {
+		cvmx_dprintf ("%s: ERROR reserving L1 SQ\n", __func__);
 		return -1;
 	}
 
-	/* allocate or reserve level 2 queues */
-	l2_q_num = cvmx_pko_alloc_queues(node, CVMX_PKO_L2_QUEUES, res_owner,
-				-1, 1);
-	if (l2_q_num < 0) {
-		cvmx_dprintf ("%s: ERROR allocating L2 SQ\n", __FUNCTION__);
+	res = cvmx_pko3_pq_config(node, pko_mac_num, l1_q_num);
+	if (res < 0) {
+		cvmx_printf("ERROR: %s: PQ/L1 queue configuration\n", __func__);
 		return -1;
 	}
 
+	parent_q = l1_q_num;
 
-	/* Configre L2 SQ */
-	res = cvmx_pko3_pq_config_children(node, pko_mac_num, l2_q_num, 1, 1);
+	/* Determine the next queue level */
+	level = __cvmx_pko3_sq_lvl_next(level);
 
-	if (res < 0) {
-		cvmx_dprintf("%s: ERROR: L2 queue\n", __FUNCTION__);
+	/* Simply chain queues 1-to-1 from L2 to one before DQ level */
+	do {
+
+		/* allocate next level queue */
+		child_q = cvmx_pko_alloc_queues(node, level, res_owner, -1, 1);
+
+		if (child_q < 0) {
+			cvmx_printf ("ERROR: %s: allocating %s\n",
+				__func__,
+				__cvmx_pko3_sq_str(b1, level, child_q));
+			return -1;
+		}
+
+		/* Configre newly allocated queue */
+		res = cvmx_pko3_sq_config_children(node, level,
+			parent_q, child_q, 1, 1);
+
+		if (res < 0) {
+			cvmx_printf ("ERROR: %s: configuring %s\n",
+				__func__,
+				__cvmx_pko3_sq_str(b1, level, child_q));
+			return -1;
+		}
+
+		/* Prepare for next level */
+		level = __cvmx_pko3_sq_lvl_next(level);
+		parent_q = child_q;
+
+	/* Terminate loop on DQ level, it has special handling */
+	} while (level != CVMX_PKO_DESCR_QUEUES &&
+		level != CVMX_PKO_LEVEL_INVAL);
+
+	if (level != CVMX_PKO_DESCR_QUEUES) {
+		cvmx_printf("ERROR: %s: level sequence error\n", __func__);
 		return -1;
 	}
 
-	l3_q = cvmx_pko_alloc_queues(node, CVMX_PKO_L3_QUEUES, res_owner,-1, 1);
-	l4_q = cvmx_pko_alloc_queues(node, CVMX_PKO_L4_QUEUES, res_owner,-1, 1);
-	l5_q = cvmx_pko_alloc_queues(node, CVMX_PKO_L5_QUEUES, res_owner,-1, 1);
-
-	/* Reserve DQ at 0 by convention */
-	res = cvmx_pko_alloc_queues(node, CVMX_PKO_DESCR_QUEUES, res_owner,
-		dq, num_dq);
+	/* Reserve 'num_dq' DQ's at 0 by convention */
+	res = cvmx_pko_alloc_queues(node, level, res_owner, dq, num_dq);
 	if (dq != res) {
 		cvmx_dprintf("%s: ERROR: could not reserve DQs\n",
 			__FUNCTION__);
 		return -1;
 	}
 
-	/* Configure hierarchy */
-	cvmx_pko3_sq_config_children(node, 2, l2_q_num, l3_q, 1, 1);
-	cvmx_pko3_sq_config_children(node, 3, l3_q, l4_q, 1, 1);
-	cvmx_pko3_sq_config_children(node, 4, l4_q, l5_q, 1, 1);
-	cvmx_pko3_sq_config_children(node, 5, l5_q, dq, num_dq, num_dq);
+	res = cvmx_pko3_sq_config_children(node, level, parent_q,
+		dq, num_dq, num_dq);
+	if (res < 0) {
+		cvmx_printf ("ERROR: %s: configuring %s\n", __func__,
+				__cvmx_pko3_sq_str(b1, level, dq));
+		return -1;
+	}
 
 	/* NULL interface does not need to map to a CHAN_E */
 
@@ -781,8 +809,11 @@ static int __cvmx_pko3_config_null_interface(unsigned int node)
 	__cvmx_pko3_ipd_dq_register(xiface, 0, dq, num_dq);
 
 	/* open the null DQs here */
-	for(i = 0; i < num_dq; i++)
+	for(i = 0; i < num_dq; i++) {
+		unsigned limit = 128; /* NULL never really uses much */
 		cvmx_pko_dq_open(node, dq + i);
+		cvmx_pko3_dq_set_limit(node, dq + i, limit);
+	}
 
 	return 0;
 }
@@ -794,11 +825,12 @@ int __cvmx_pko3_helper_dqs_activate(int xiface, int index, bool min_pad)
 {
 	int  ipd_port,dq_base, dq_count, i;
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	unsigned limit;
 
 	/* Get local IPD port for the interface */
 	ipd_port = cvmx_helper_get_ipd_port(xiface, index);
 	if(ipd_port < 0) {
-		cvmx_dprintf("%s: ERROR: No IPD port for interface %d port %d\n",
+		cvmx_printf("ERROR: %s: No IPD port for interface %d port %d\n",
 			     __FUNCTION__, xiface, index);
 		return -1;
 	}
@@ -807,7 +839,7 @@ int __cvmx_pko3_helper_dqs_activate(int xiface, int index, bool min_pad)
 	dq_base = cvmx_pko3_get_queue_base(ipd_port);
 	dq_count = cvmx_pko3_get_queue_num(ipd_port);
 	if( dq_base < 0 || dq_count <= 0) {
-		cvmx_dprintf("%s: ERROR: No descriptor queues for interface %d port %d\n",
+		cvmx_printf("ERROR: %s: No descriptor queues for interface %d port %d\n",
 			     __FUNCTION__, xiface, index);
 		return -1;
 	}
@@ -815,11 +847,28 @@ int __cvmx_pko3_helper_dqs_activate(int xiface, int index, bool min_pad)
 	/* Mask out node from global DQ# */
 	dq_base &= (1<<10)-1;
 
+	limit = __pko_pkt_quota / dq_count /
+		cvmx_helper_interface_enumerate(xiface);
+
 	for(i = 0; i < dq_count; i++) {
+		/* FIXME: 2ms at 1Gbps max packet rate, make speed dependent */
 		cvmx_pko_dq_open(xi.node, dq_base + i);
 		cvmx_pko3_dq_options(xi.node, dq_base + i, min_pad);
+
+		if (debug)
+			cvmx_dprintf("%s: DQ%u limit %d\n",
+				__func__, dq_base + i, limit);
+
+		cvmx_pko3_dq_set_limit(xi.node, dq_base + i, limit);
+		__pko_pkt_budget -= limit;
 	}
 
+	if (__pko_pkt_budget < 0)
+		cvmx_printf("WARNING: %s: PKO buffer deficit %d\n",
+			__func__, __pko_pkt_budget);
+	else if (debug)
+		cvmx_dprintf("%s: PKO remaining packet budget: %d\n",
+			__func__, __pko_pkt_budget);
 
 	return i;
 }
@@ -862,17 +911,15 @@ int cvmx_helper_pko3_init_interface(int xiface)
 	}
 
 	/* Force 8 DQs per port for pass 1.0 to circumvent limitations */
-	if(OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_0)) {
+	if(OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_0))
 		num_queues = 8;
-		qos = true;
-	}
 
 	/* For ILK there is one IPD port per channel */
 	if ((mode == CVMX_HELPER_INTERFACE_MODE_ILK))
 		num_ports =  __cvmx_helper_ilk_enumerate(xiface);
 
 	/* Skip non-existent interfaces */
-	if(num_ports < 1) {
+	if (num_ports < 1) {
 		cvmx_dprintf("ERROR: %s: invalid iface %u:%u\n",
 			     __FUNCTION__, xi.node, xi.interface);
 		return -1;
@@ -895,7 +942,6 @@ int cvmx_helper_pko3_init_interface(int xiface)
 			goto __cfg_error;
 		}
 	}
-
 	else if (mode == CVMX_HELPER_INTERFACE_MODE_NPI) {
 		num_queues =
 			__cvmx_pko_queue_static_config.
@@ -913,13 +959,13 @@ int cvmx_helper_pko3_init_interface(int xiface)
 			goto __cfg_error;
 		}
 	}
-
 	/* ILK-specific queue configuration */
 	else if (mode == CVMX_HELPER_INTERFACE_MODE_ILK) {
+		unsigned num_chans = __cvmx_helper_ilk_enumerate(xiface);
 		num_queues = 8; qos = true; pfc = false;
-		res = __cvmx_pko3_config_ilk_interface(xiface, num_queues, qos);
+		res = __cvmx_pko3_config_chan_interface(xiface, num_chans,
+				num_queues, qos);
 	}
-
 	/* Setup all ethernet configured for PFC */
 	else if (pfc) {
 		/* PFC interfaces have 8 prioritized queues */
@@ -934,7 +980,6 @@ int cvmx_helper_pko3_init_interface(int xiface)
 				xi.interface, subif, true);
 		}
 	}
-
 	/* All other interfaces follow static configuration */
 	else {
 
@@ -950,7 +995,10 @@ int cvmx_helper_pko3_init_interface(int xiface)
 	fcs_enable = __cvmx_helper_get_has_fcs(xiface);
 	pad_enable = __cvmx_helper_get_pko_padding(xiface);
 
-	if(OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
+	// XXX for some reason, turning off BGX FCS does not work on o75/o73
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X) ||
+	    OCTEON_IS_MODEL(OCTEON_CNF75XX) ||
+	    OCTEON_IS_MODEL(OCTEON_CN73XX))
 		pad_enable_pko = false;
 	else
 		pad_enable_pko = pad_enable;
@@ -982,7 +1030,11 @@ int cvmx_helper_pko3_init_interface(int xiface)
 		if (mode == CVMX_HELPER_INTERFACE_MODE_NPI && subif > 0)
 			continue;
 
-		if (xi.interface > 5) {
+		/* for sRIO there is 16 byte sRIO header, outside of FCS */
+		if (mode == CVMX_HELPER_INTERFACE_MODE_SRIO)
+			fcs_sof_off = 16;
+
+		if (xi.interface >= CVMX_HELPER_MAX_GMX) {
 			/* Non-BGX interface, use PKO for FCS/PAD */
 			res = cvmx_pko3_interface_options(xiface, subif,
 				fcs_enable, pad_enable_pko, fcs_sof_off);
@@ -990,14 +1042,14 @@ int cvmx_helper_pko3_init_interface(int xiface)
 			/* BGX interface: FCS/PAD done by PKO */
 			res = cvmx_pko3_interface_options(xiface, subif,
 				  fcs_enable, pad_enable, fcs_sof_off);
-			cvmx_helper_bgx_tx_options(xi.node, xi.interface, subif,
-				false, false);
+			cvmx_helper_bgx_tx_options(xi.node, xi.interface,
+				subif, false, false);
 		} else {
 			/* BGX interface: FCS/PAD done by BGX */
 			res = cvmx_pko3_interface_options(xiface, subif,
 				  false, false, fcs_sof_off);
-			cvmx_helper_bgx_tx_options(xi.node, xi.interface, subif,
-				fcs_enable, pad_enable);
+			cvmx_helper_bgx_tx_options(xi.node, xi.interface,
+				subif, fcs_enable, pad_enable);
 		}
 
 		if(res < 0)
@@ -1038,8 +1090,7 @@ int __cvmx_helper_pko3_init_global(unsigned int node, uint16_t gaura)
 	}
 
 	/* configure channel level */
-	cvmx_pko_setup_channel_credit_level(node,
-		cvmx_pko_default_channel_level);
+	cvmx_pko3_channel_credit_level(node, cvmx_pko_default_channel_level);
 
 	/* add NULL MAC/DQ setup */
 	res = __cvmx_pko3_config_null_interface(node);
@@ -1051,46 +1102,6 @@ int __cvmx_helper_pko3_init_global(unsigned int node, uint16_t gaura)
 }
 EXPORT_SYMBOL(__cvmx_helper_pko3_init_global);
 
-/*#define	__PKO_HW_DEBUG*/
-#ifdef	__PKO_HW_DEBUG
-#define	CVMX_DUMP_REGX(reg) cvmx_dprintf("%s=%#llx\n",#reg,(long long)cvmx_read_csr(reg))
-#define	CVMX_DUMP_REGD(reg) cvmx_dprintf("%s=%lld.\n",#reg,(long long)cvmx_read_csr(reg))
-/*
- * function for debugging PKO reconfiguration
- */
-void cvmx_fpa3_aura_dump_regs(unsigned node, uint16_t aura)
-	{
-	int pool_num =
-		cvmx_read_csr_node(node,CVMX_FPA_AURAX_POOL(aura));
-
-	CVMX_DUMP_REGX(CVMX_FPA_AURAX_POOL(aura));
-	CVMX_DUMP_REGX(CVMX_FPA_POOLX_CFG(pool_num));
-	CVMX_DUMP_REGX(CVMX_FPA_POOLX_OP_PC(pool_num));
-	CVMX_DUMP_REGX(CVMX_FPA_POOLX_INT(pool_num));
-	CVMX_DUMP_REGD(CVMX_FPA_POOLX_AVAILABLE(pool_num));
-	CVMX_DUMP_REGD(CVMX_FPA_POOLX_THRESHOLD(pool_num));
-	CVMX_DUMP_REGX(CVMX_FPA_AURAX_CFG(aura));
-	CVMX_DUMP_REGX(CVMX_FPA_AURAX_INT(aura));
-	CVMX_DUMP_REGD(CVMX_FPA_AURAX_CNT(aura));
-	CVMX_DUMP_REGD(CVMX_FPA_AURAX_CNT_LIMIT(aura));
-	CVMX_DUMP_REGX(CVMX_FPA_AURAX_CNT_THRESHOLD(aura));
-	CVMX_DUMP_REGX(CVMX_FPA_AURAX_CNT_LEVELS(aura));
-	CVMX_DUMP_REGX(CVMX_FPA_AURAX_POOL_LEVELS(aura));
-
-	}
-
-void cvmx_pko3_dump_regs(unsigned node)
-{
-	(void) node;
-	CVMX_DUMP_REGX( CVMX_PKO_NCB_INT );
-	CVMX_DUMP_REGX( CVMX_PKO_PEB_ERR_INT );
-	CVMX_DUMP_REGX( CVMX_PKO_PDM_ECC_DBE_STS_CMB0 );
-	CVMX_DUMP_REGX( CVMX_PKO_PDM_ECC_SBE_STS_CMB0 );
-	CVMX_DUMP_REGX( CVMX_PKO_PEB_ECC_DBE_STS0 );
-	CVMX_DUMP_REGX( CVMX_PKO_PEB_ECC_DBE_STS_CMB0 );
-}
-#endif	/* __PKO_HW_DEBUG */
-
 /**
  * Global initialization for PKO3
  *
@@ -1158,6 +1169,8 @@ int cvmx_helper_pko3_shut_interface(int xiface)
 	int dq_base, dq_count;
 	uint16_t ipd_port;
 	int i, res_owner, res;
+	enum cvmx_pko3_level_e level;
+	cvmx_pko3_dq_params_t *pParam;
 	uint64_t cycles;
 	const unsigned timeout = 10;	/* milliseconds */
 	cvmx_xiface_t xi = cvmx_helper_xiface_to_node_interface(xiface);
@@ -1245,6 +1258,10 @@ int cvmx_helper_pko3_shut_interface(int xiface)
 				cvmx_dprintf("ERROR: %s: closing queue %u\n",
 					__FUNCTION__, dq_base + i);
 
+			/* Return DQ packet budget */
+			pParam = cvmx_pko3_dq_parameters(xi.node, dq_base  + i);
+			__pko_pkt_budget += pParam->limit;
+			pParam->limit = 0;
 		}
 
 		/* Release all global resources owned by this interface/port */
@@ -1256,12 +1273,12 @@ int cvmx_helper_pko3_shut_interface(int xiface)
 			continue;
 		}
 
-		cvmx_pko_free_queues(xi.node, CVMX_PKO_DESCR_QUEUES, res_owner);
-		cvmx_pko_free_queues(xi.node, CVMX_PKO_L5_QUEUES, res_owner);
-		cvmx_pko_free_queues(xi.node, CVMX_PKO_L4_QUEUES, res_owner);
-		cvmx_pko_free_queues(xi.node, CVMX_PKO_L3_QUEUES, res_owner);
-		cvmx_pko_free_queues(xi.node, CVMX_PKO_L2_QUEUES, res_owner);
-		cvmx_pko_free_queues(xi.node, CVMX_PKO_PORT_QUEUES, res_owner);
+		/* Actuall PQ/SQ/DQ associations left intact */
+		for(level = CVMX_PKO_PORT_QUEUES;
+		    level != CVMX_PKO_LEVEL_INVAL;
+		    level = __cvmx_pko3_sq_lvl_next(level)) {
+			cvmx_pko_free_queues(xi.node, level, res_owner);
+		}
 
 	} /* for port */
 
@@ -1309,3 +1326,699 @@ int cvmx_helper_pko3_shutdown(unsigned int node)
 	return res;
 }
 EXPORT_SYMBOL(cvmx_helper_pko3_shutdown);
+
+/*#define	__PKO_HW_DEBUG*/
+#ifdef	__PKO_HW_DEBUG
+#define	CVMX_DUMP_REGX(reg) cvmx_dprintf("%s=%#llx\n",#reg,(long long)cvmx_read_csr(reg))
+#define	CVMX_DUMP_REGD(reg) cvmx_dprintf("%s=%lld.\n",#reg,(long long)cvmx_read_csr(reg))
+/*
+ * function for debugging PKO reconfiguration
+ */
+void cvmx_fpa3_aura_dump_regs(unsigned node, uint16_t aura)
+	{
+	int pool_num =
+		cvmx_read_csr_node(node,CVMX_FPA_AURAX_POOL(aura));
+
+	CVMX_DUMP_REGX(CVMX_FPA_AURAX_POOL(aura));
+	CVMX_DUMP_REGX(CVMX_FPA_POOLX_CFG(pool_num));
+	CVMX_DUMP_REGX(CVMX_FPA_POOLX_OP_PC(pool_num));
+	CVMX_DUMP_REGX(CVMX_FPA_POOLX_INT(pool_num));
+	CVMX_DUMP_REGD(CVMX_FPA_POOLX_AVAILABLE(pool_num));
+	CVMX_DUMP_REGD(CVMX_FPA_POOLX_THRESHOLD(pool_num));
+	CVMX_DUMP_REGX(CVMX_FPA_AURAX_CFG(aura));
+	CVMX_DUMP_REGX(CVMX_FPA_AURAX_INT(aura));
+	CVMX_DUMP_REGD(CVMX_FPA_AURAX_CNT(aura));
+	CVMX_DUMP_REGD(CVMX_FPA_AURAX_CNT_LIMIT(aura));
+	CVMX_DUMP_REGX(CVMX_FPA_AURAX_CNT_THRESHOLD(aura));
+	CVMX_DUMP_REGX(CVMX_FPA_AURAX_CNT_LEVELS(aura));
+	CVMX_DUMP_REGX(CVMX_FPA_AURAX_POOL_LEVELS(aura));
+
+	}
+
+void cvmx_pko3_dump_regs(unsigned node)
+{
+	(void) node;
+	CVMX_DUMP_REGX( CVMX_PKO_NCB_INT );
+	CVMX_DUMP_REGX( CVMX_PKO_PEB_ERR_INT );
+	CVMX_DUMP_REGX( CVMX_PKO_PDM_ECC_DBE_STS_CMB0 );
+	CVMX_DUMP_REGX( CVMX_PKO_PDM_ECC_SBE_STS_CMB0 );
+	CVMX_DUMP_REGX( CVMX_PKO_PEB_ECC_DBE_STS0 );
+	CVMX_DUMP_REGX( CVMX_PKO_PEB_ECC_DBE_STS_CMB0 );
+}
+
+#endif	/* __PKO_HW_DEBUG */
+
+#ifdef CVMX_DUMP_PKO
+/*
+ * Show PKO integrated configuration.
+ * See function prototype in cvmx-helper-pko3.h
+ */
+#define PKO_PRN_HEADLEN  16 
+#define PKO_PRN_DATALEN  64
+#define PKO_PRN_LINELEN  (PKO_PRN_HEADLEN + PKO_PRN_DATALEN)
+#define PKO_PRN_DPLEN(__n)  (PKO_PRN_DATALEN / __n)
+
+#define DLMPRINT(__format, ...) \
+do { \
+	int __n; \
+	sprintf(lines[1], __format, ## __VA_ARGS__); \
+	__n = PKO_PRN_LINELEN - strlen(lines[1]) - 1; \
+	memset(lines[0], '-', __n);  lines[0][__n] = '\0'; \
+	cvmx_printf("%s %s\n", lines[0], lines[1]); \
+} while (0)
+
+#define PARPRINT(__offs, __head, __format, ...) \
+do {\
+	cvmx_printf("%*s%-*s", __offs, "", PKO_PRN_HEADLEN - __offs, __head); \
+	cvmx_printf(__format, ## __VA_ARGS__); \
+} while (0)
+
+#define PKO_MAC_NUM	32
+char *pko_macmap[PKO_MAC_NUM][3] = {
+			/*CN78XX		CN73XX			CNF75XX*/
+	[0]  = {"LBK      ",	"LBK      ",	"LBK      "},
+	[1]  = {"DPI      ",	"DPI      ",	"DPI      "},
+	[2]  = {"ILK0     ",	"BGX0:MAC0",	"BGX0:MAC0"},
+	[3]  = {"ILK1     ",	"BGX0:MAC1",	"BGX0:MAC1"},
+	[4]  = {"BGX0:MAC0",	"BGX0:MAC2",	"BGX0:MAC2"},
+	[5]  = {"BGX0:MAC1",	"BGX0:MAC3",	"BGX0:MAC3"},
+	[6]  = {"BGX0:MAC2",	"BGX1:MAC0",	"SRIO0-0  "},
+	[7]  = {"BGX0:MAC3",	"BGX1:MAC1",	"SRIO0-1  "},
+	[8]  = {"BGX1:MAC0",	"BGX1:MAC2",	"SRIO1-0  "},
+	[9]  = {"BGX1:MAC1",	"BGX1:MAC3",	"SRIO1-1  "},
+	[10] = {"BGX1:MAC2",	"BGX2:MAC0",	"NULL     "},
+	[11] = {"BGX1:MAC3",	"BGX2:MAC1",	NULL},
+	[12] = {"BGX2:MAC0",	"BGX2:MAC2",	NULL},
+	[13] = {"BGX2:MAC1",	"BGX2:MAC3",	NULL},
+	[14] = {"BGX2:MAC2",	"NULL     ",	NULL},
+	[15] = {"BGX2:MAC3",	NULL,			NULL},
+	[16] = {"BGX3:MAC0",	NULL,			NULL},
+	[17] = {"BGX3:MAC1",	NULL,			NULL},
+	[18] = {"BGX3:MAC2",	NULL,			NULL},
+	[19] = {"BGX3:MAC3",	NULL,			NULL},
+	[20] = {"BGX4:MAC0",	NULL,			NULL},
+	[21] = {"BGX4:MAC1",	NULL,			NULL},
+	[22] = {"BGX4:MAC2",	NULL,			NULL},
+	[23] = {"BGX4:MAC3",	NULL,			NULL},
+	[24] = {"BGX5:MAC0",	NULL,			NULL},
+	[25] = {"BGX5:MAC1",	NULL,			NULL},
+	[26] = {"BGX5:MAC2",	NULL,			NULL},
+	[27] = {"BGX5:MAC3",	NULL,			NULL},
+	[28] = {"NULL     ",	NULL,			NULL},
+	[29] = {NULL,			NULL,			NULL},
+	[30] = {NULL,			NULL,			NULL},
+	[31] = {NULL,			NULL,			NULL}
+};
+
+int cvmx_helper_pko3_config_dump(unsigned int node)
+{
+	int queue, nqueues, group, base, nmacs, ngroups;
+	cvmx_pko_dqx_sw_xoff_t dqxoff;
+	cvmx_pko_dqx_topology_t dqtop;
+	cvmx_pko_l5_sqx_topology_t l5top;
+	cvmx_pko_l4_sqx_topology_t l4top;
+	cvmx_pko_l3_sqx_topology_t l3top;
+	cvmx_pko_l2_sqx_topology_t l2top;
+	cvmx_pko_l1_sqx_topology_t l1top;
+	cvmx_pko_dqx_schedule_t dqsch;
+	cvmx_pko_l5_sqx_schedule_t l5sch;
+	cvmx_pko_l4_sqx_schedule_t l4sch;
+	cvmx_pko_l3_sqx_schedule_t l3sch;
+	cvmx_pko_l2_sqx_schedule_t l2sch;
+	cvmx_pko_l1_sqx_schedule_t l1sch;
+	cvmx_pko_dqx_shape_t dqshape;
+	cvmx_pko_l5_sqx_shape_t l5shape;
+	cvmx_pko_l4_sqx_shape_t l4shape;
+	cvmx_pko_l3_sqx_shape_t l3shape;
+	cvmx_pko_l2_sqx_shape_t l2shape;
+	cvmx_pko_l1_sqx_shape_t l1shape;
+	cvmx_pko_dqx_cir_t dqcir;
+	cvmx_pko_l5_sqx_cir_t l5cir;
+	cvmx_pko_l4_sqx_cir_t l4cir;
+	cvmx_pko_l3_sqx_cir_t l3cir;
+	cvmx_pko_l2_sqx_cir_t l2cir;
+	cvmx_pko_l1_sqx_cir_t l1cir;
+	cvmx_pko_dqx_pir_t dqpir;
+	cvmx_pko_l5_sqx_pir_t l5pir;
+	cvmx_pko_l4_sqx_pir_t l4pir;
+	cvmx_pko_l3_sqx_pir_t l3pir;
+	cvmx_pko_l2_sqx_pir_t l2pir;
+	cvmx_pko_macx_cfg_t maccfg;
+	cvmx_pko_l3_l2_sqx_channel_t chcfg;
+	cvmx_pko_channel_level_t chlvl;
+	cvmx_pko_shaper_cfg_t shapercfg;
+	cvmx_pko_l1_sqx_link_t l1link;
+	uint32_t crc32, pcrc32;
+	char lines[4][128];
+	int ciren, piren;
+	uint64_t dqsh_clk, pqsh_clk;
+	int shaper_rate(int shclk, int man, int exp, int div) {
+		return (CVMX_SHOFT_TO_U64(man, exp) * shclk >> div) * 8/*bits*/ / 1000000/*Mbps*/;
+	}
+	int shaper_burst(int man, int exp) {
+		return CVMX_SHOFT_TO_U64(man, (exp + 1));
+	}
+
+	if (!(OCTEON_IS_MODEL(OCTEON_CN78XX) ||
+		OCTEON_IS_MODEL(OCTEON_CN73XX) ||
+		OCTEON_IS_MODEL(OCTEON_CNF75XX))) {
+		cvmx_printf("PKO3 Config Dump is not supported on this OCTEON model\n");
+		return 0;
+	}
+	dqsh_clk = cvmx_pko3_dq_tw_clock_rate_node(node);
+	pqsh_clk = cvmx_pko3_pq_tw_clock_rate_node(node);
+
+	memset(lines[3], '*', PKO_PRN_LINELEN);  lines[3][PKO_PRN_LINELEN] = '\0';
+	cvmx_printf("\n%s\n", lines[3]);
+	cvmx_printf("   PKO Configuration (Node %d)\n", node);
+	cvmx_printf("%s\n", lines[3]);
+
+	/* Global parameters: */
+	chlvl.u64 = cvmx_read_csr_node(node, CVMX_PKO_CHANNEL_LEVEL);
+	shapercfg.u64 = cvmx_read_csr_node(node, CVMX_PKO_SHAPER_CFG);
+	PARPRINT(0, "CC Level", "%*d\n", PKO_PRN_DPLEN(1), chlvl.s.cc_level + 2);
+	PARPRINT(0, "Color Aware", "%*s\n", PKO_PRN_DPLEN(1), (shapercfg.s.color_aware == 1) ? "Yes":"No");
+
+	/* Queues: */
+	cvmx_printf("%-*s%*s%*s%*s%*s%*s%*s%*s%*s\n", PKO_PRN_HEADLEN, "",
+		PKO_PRN_DPLEN(8), "DQ", PKO_PRN_DPLEN(8), "L5", PKO_PRN_DPLEN(8), "L4",
+		PKO_PRN_DPLEN(8), "L3", PKO_PRN_DPLEN(8),"L2", PKO_PRN_DPLEN(8), "L1",
+		PKO_PRN_DPLEN(8), "MAC", PKO_PRN_DPLEN(8), "FIFO");
+
+	nqueues = cvmx_pko3_num_level_queues(CVMX_PKO_DESCR_QUEUES);
+	nmacs = __cvmx_pko3_num_macs();
+	for (queue = 0, pcrc32 = 0, base = 0; queue < nqueues; queue++) {
+		CVMX_MT_CRC_POLYNOMIAL(0x1edc6f41);
+		CVMX_MT_CRC_IV(0xffffffff);
+		/* Descriptor Queue Level: */
+		dqxoff.u64 = cvmx_read_csr_node(node, CVMX_PKO_DQX_SW_XOFF(queue));
+		CVMX_MT_CRC_DWORD(dqxoff.u64);
+		dqtop.u64 = cvmx_read_csr_node(node, CVMX_PKO_DQX_TOPOLOGY(queue));
+		CVMX_MT_CRC_DWORD(dqtop.u64);
+		dqsch.u64 = cvmx_read_csr_node(node, CVMX_PKO_DQX_SCHEDULE(queue));
+		CVMX_MT_CRC_DWORD(dqsch.u64);
+		dqshape.u64 = cvmx_read_csr_node(node, CVMX_PKO_DQX_SHAPE(queue));
+		CVMX_MT_CRC_DWORD(dqshape.u64);
+		dqcir.u64 = cvmx_read_csr_node(node, CVMX_PKO_DQX_CIR(queue));
+		CVMX_MT_CRC_DWORD(dqcir.u64);
+		dqpir.u64 = cvmx_read_csr_node(node, CVMX_PKO_DQX_PIR(queue));
+		CVMX_MT_CRC_DWORD(dqpir.u64);
+
+		/* L5-L3 Queue Levels: */
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX)) { 
+			l5top.u64 = cvmx_read_csr_node(node, CVMX_PKO_L5_SQX_TOPOLOGY(dqtop.s.parent));
+			CVMX_MT_CRC_DWORD(l5top.u64);
+			l4top.u64 = cvmx_read_csr_node(node, CVMX_PKO_L4_SQX_TOPOLOGY(l5top.s.parent));
+			CVMX_MT_CRC_DWORD(l4top.u64);
+			l3top.u64 = cvmx_read_csr_node(node, CVMX_PKO_L3_SQX_TOPOLOGY(l4top.s.parent));
+			CVMX_MT_CRC_DWORD(l3top.u64);
+
+			l5sch.u64 = cvmx_read_csr_node(node, CVMX_PKO_L5_SQX_SCHEDULE(dqtop.s.parent));
+			CVMX_MT_CRC_DWORD(l5sch.u64);
+			l4sch.u64 = cvmx_read_csr_node(node, CVMX_PKO_L4_SQX_SCHEDULE(l5top.s.parent));
+			CVMX_MT_CRC_DWORD(l4sch.u64);
+			l3sch.u64 = cvmx_read_csr_node(node, CVMX_PKO_L3_SQX_SCHEDULE(l4top.s.parent));
+			CVMX_MT_CRC_DWORD(l3sch.u64);
+
+			l5shape.u64 = cvmx_read_csr_node(node, CVMX_PKO_L5_SQX_SHAPE(dqtop.s.parent));
+			CVMX_MT_CRC_DWORD(l5shape.u64);
+			l4shape.u64 = cvmx_read_csr_node(node, CVMX_PKO_L4_SQX_SHAPE(l5top.s.parent));
+			CVMX_MT_CRC_DWORD(l4shape.u64);
+			l3shape.u64 = cvmx_read_csr_node(node, CVMX_PKO_L3_SQX_SHAPE(l4top.s.parent));
+			CVMX_MT_CRC_DWORD(l3shape.u64);
+
+			l5cir.u64 = cvmx_read_csr_node(node, CVMX_PKO_L5_SQX_CIR(dqtop.s.parent));
+			CVMX_MT_CRC_DWORD(l5cir.u64);
+			l4cir.u64 = cvmx_read_csr_node(node, CVMX_PKO_L4_SQX_CIR(l5top.s.parent));
+			CVMX_MT_CRC_DWORD(l4cir.u64);
+			l3cir.u64 = cvmx_read_csr_node(node, CVMX_PKO_L3_SQX_CIR(l4top.s.parent));
+			CVMX_MT_CRC_DWORD(l3cir.u64);
+
+			l5pir.u64 = cvmx_read_csr_node(node, CVMX_PKO_L5_SQX_PIR(dqtop.s.parent));
+			CVMX_MT_CRC_DWORD(l5pir.u64);
+			l4pir.u64 = cvmx_read_csr_node(node, CVMX_PKO_L4_SQX_PIR(l5top.s.parent));
+			CVMX_MT_CRC_DWORD(l4pir.u64);
+			l3pir.u64 = cvmx_read_csr_node(node, CVMX_PKO_L3_SQX_PIR(l4top.s.parent));
+			CVMX_MT_CRC_DWORD(l3pir.u64);
+		}
+		else {
+			l5top.u64 = l4top.u64 = 0;
+			l3top.u64 = cvmx_read_csr_node(node, CVMX_PKO_L3_SQX_TOPOLOGY(dqtop.s.parent));
+			CVMX_MT_CRC_DWORD(l3top.u64);
+
+			l5sch.u64 = l4sch.u64 = 0;
+			l3sch.u64 = cvmx_read_csr_node(node, CVMX_PKO_L3_SQX_SCHEDULE(dqtop.s.parent));
+			CVMX_MT_CRC_DWORD(l3sch.u64);
+
+			l5shape.u64 = l4shape.u64 = 0;
+			l3shape.u64 = cvmx_read_csr_node(node, CVMX_PKO_L3_SQX_SHAPE(dqtop.s.parent));
+			CVMX_MT_CRC_DWORD(l3shape.u64);
+
+			l5cir.u64 = l4cir.u64 = l5pir.u64 = l4pir.u64 = 0;
+			l3cir.u64 = cvmx_read_csr_node(node, CVMX_PKO_L3_SQX_CIR(dqtop.s.parent));
+			CVMX_MT_CRC_DWORD(l3cir.u64);
+			l3pir.u64 = cvmx_read_csr_node(node, CVMX_PKO_L3_SQX_PIR(dqtop.s.parent));
+			CVMX_MT_CRC_DWORD(l3pir.u64);
+		}
+		/* L2-L1 Queue Levels: */
+		l2top.u64 = cvmx_read_csr_node(node, CVMX_PKO_L2_SQX_TOPOLOGY(l3top.s.parent));
+		CVMX_MT_CRC_DWORD(l2top.u64);
+		l1top.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_TOPOLOGY(l2top.s.parent));
+		CVMX_MT_CRC_DWORD(l1top.u64);
+
+		l2sch.u64 = cvmx_read_csr_node(node, CVMX_PKO_L2_SQX_SCHEDULE(l3top.s.parent));
+		CVMX_MT_CRC_DWORD(l2sch.u64);
+		l1sch.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_SCHEDULE(l2top.s.parent));
+		CVMX_MT_CRC_DWORD(l1sch.u64);
+
+		l2shape.u64 = cvmx_read_csr_node(node, CVMX_PKO_L2_SQX_SHAPE(l3top.s.parent));
+		CVMX_MT_CRC_DWORD(l2shape.u64);
+		l1shape.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_SHAPE(l2top.s.parent));
+		CVMX_MT_CRC_DWORD(l1shape.u64);
+
+		l2cir.u64 = cvmx_read_csr_node(node, CVMX_PKO_L2_SQX_CIR(l3top.s.parent));
+		CVMX_MT_CRC_DWORD(l2cir.u64);
+		l1cir.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_CIR(l2top.s.parent));
+		CVMX_MT_CRC_DWORD(l1cir.u64);
+
+		l2pir.u64 = cvmx_read_csr_node(node, CVMX_PKO_L2_SQX_PIR(l3top.s.parent));
+		CVMX_MT_CRC_DWORD(l2pir.u64);
+
+		l1link.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_LINK(l2top.s.parent));
+		CVMX_MT_CRC_DWORD(l1link.u64 & (0x1Full << 44 | 0x2ull));
+
+		/* MAC/FIFO Level: */
+		if (l1top.s.link > nmacs) {
+			maccfg.u64 = 0;
+			sprintf(lines[0], "Undef");
+		}
+		else if (l1top.s.link == nmacs) {
+			maccfg.u64 = 0;
+			sprintf(lines[0], "NULL");
+		}
+		else {
+			maccfg.u64 = cvmx_read_csr_node(node, CVMX_PKO_MACX_CFG(l1top.s.link));
+			if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+				sprintf(lines[0], "%s", pko_macmap[l1top.s.link][0]);
+			else if (OCTEON_IS_MODEL(OCTEON_CN73XX))
+				sprintf(lines[0], "%s", pko_macmap[l1top.s.link][1]);
+			else if (OCTEON_IS_MODEL(OCTEON_CNF75XX))
+				sprintf(lines[0], "%s", pko_macmap[l1top.s.link][2]);
+		}
+		sprintf(lines[1], "%d", maccfg.s.fifo_num);
+		CVMX_MT_CRC_DWORD(maccfg.u64);
+
+		if (chlvl.s.cc_level == 0) { /* Level 2 as the Channel Level?*/
+			chcfg.u64 = cvmx_read_csr_node(node, CVMX_PKO_L3_L2_SQX_CHANNEL(l3top.s.parent));
+			sprintf(lines[2], "%s", "--");
+			sprintf(lines[3], "%d", chcfg.s.cc_channel);
+		}
+		else {
+			if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+				chcfg.u64 = cvmx_read_csr_node(node, CVMX_PKO_L3_L2_SQX_CHANNEL(l4top.s.parent));
+			else
+				chcfg.u64 = cvmx_read_csr_node(node, CVMX_PKO_L3_L2_SQX_CHANNEL(dqtop.s.parent));
+			sprintf(lines[2], "%d", chcfg.s.cc_channel);
+			sprintf(lines[3], "%s", "--");
+		}
+		CVMX_MT_CRC_DWORD(chcfg.u64);
+		CVMX_MF_CRC_IV(crc32);
+		if (crc32 == pcrc32)
+			continue;
+
+		/* Display DQ...FIFO Queue-Chain configuration: */
+		if (queue > 0 && (queue - 1) != base)
+			cvmx_printf("\nDQ(s) %02d-%02d -- same as DQ %02d\n",
+				queue - 1, base + 1, base);
+		pcrc32 = crc32;
+		base = queue;
+		cvmx_printf("DQ%d:\n", queue);
+		PARPRINT(2, "Channel", "%*s%-*s%*s%*s%*s%*s%-*s%*s\n",
+			PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8) + 1, lines[0],
+			PKO_PRN_DPLEN(8) - 1, "", PKO_PRN_DPLEN(8), lines[2],
+			PKO_PRN_DPLEN(8), lines[3], PKO_PRN_DPLEN(8), "",
+			PKO_PRN_DPLEN(8), chcfg.s.cc_enable ? "CC-En" : "CC-Dis",
+			PKO_PRN_DPLEN(8), l1link.s.cc_enable ? "LC-En" : "LC-Dis");
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+			ciren = dqcir.s.enable + l5cir.s.enable + l4cir.s.enable +
+				l3cir.s.enable + l2cir.s.enable + l1cir.s.enable;
+			piren = dqpir.s.enable + l5pir.s.enable + l4pir.s.enable +
+				l3pir.s.enable + l2pir.s.enable;
+
+			PARPRINT(2, "Path", "%*s%*d%*d%*d%*d%*d%*d%*s\n",
+				PKO_PRN_DPLEN(8), "--", PKO_PRN_DPLEN(8), dqtop.s.parent,
+				PKO_PRN_DPLEN(8), l5top.s.parent, PKO_PRN_DPLEN(8), l4top.s.parent,
+				PKO_PRN_DPLEN(8), l3top.s.parent, PKO_PRN_DPLEN(8), l2top.s.parent,
+				PKO_PRN_DPLEN(8), l1top.s.link, PKO_PRN_DPLEN(8), lines[1]);
+			PARPRINT(2, "Prio-Anchor", "%*s%*d%*d%*d%*d%*d%*s%*s\n",
+				PKO_PRN_DPLEN(8), "--", PKO_PRN_DPLEN(8), l5top.s.prio_anchor,
+				PKO_PRN_DPLEN(8), l4top.s.prio_anchor, PKO_PRN_DPLEN(8), l3top.s.prio_anchor,
+				PKO_PRN_DPLEN(8), l2top.s.prio_anchor, PKO_PRN_DPLEN(8), l1top.s.prio_anchor,
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+			PARPRINT(2, "Prio", "%*d%*d%*d%*d%*d%*s%*s%*s\n",
+				PKO_PRN_DPLEN(8), dqsch.s.prio, PKO_PRN_DPLEN(8), l5sch.s.prio,
+				PKO_PRN_DPLEN(8), l4sch.s.prio, PKO_PRN_DPLEN(8), l3sch.s.prio,
+				PKO_PRN_DPLEN(8), l2sch.s.prio, PKO_PRN_DPLEN(8), "--",
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+			PARPRINT(2, "RR-Prio", "%*s%*d%*d%*d%*d%*d%*s%*s\n",
+				PKO_PRN_DPLEN(8), "--", PKO_PRN_DPLEN(8), l5top.s.rr_prio,
+				PKO_PRN_DPLEN(8), l4top.s.rr_prio, PKO_PRN_DPLEN(8), l3top.s.rr_prio,
+				PKO_PRN_DPLEN(8), l2top.s.rr_prio, PKO_PRN_DPLEN(8), l1top.s.rr_prio,
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+			PARPRINT(2, "RR-Quantum", "%*x%*x%*x%*x%*x%*x%*s%*s\n",
+				PKO_PRN_DPLEN(8), dqsch.s.rr_quantum, PKO_PRN_DPLEN(8), l5sch.s.rr_quantum,
+				PKO_PRN_DPLEN(8), l4sch.s.rr_quantum, PKO_PRN_DPLEN(8), l3sch.s.rr_quantum,
+				PKO_PRN_DPLEN(8), l2sch.s.rr_quantum, PKO_PRN_DPLEN(8), l1sch.s.rr_quantum,
+				PKO_PRN_DPLEN(8), "(hex)", PKO_PRN_DPLEN(8), "");
+			PARPRINT(2, "Len.Disable", "%*d%*d%*d%*d%*d%*d%*s%*s\n",
+				PKO_PRN_DPLEN(8), dqshape.s.length_disable, PKO_PRN_DPLEN(8), l5shape.s.length_disable,
+				PKO_PRN_DPLEN(8), l4shape.s.length_disable, PKO_PRN_DPLEN(8), l3shape.s.length_disable,
+				PKO_PRN_DPLEN(8), l2shape.s.length_disable, PKO_PRN_DPLEN(8), l1shape.s.length_disable,
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+			PARPRINT(2, "Len.Adjust", "%*d%*d%*d%*d%*d%*d%*s%*s\n",
+				PKO_PRN_DPLEN(8), dqshape.s.adjust, PKO_PRN_DPLEN(8), l5shape.s.adjust,
+				PKO_PRN_DPLEN(8), l4shape.s.adjust, PKO_PRN_DPLEN(8), l3shape.s.adjust,
+				PKO_PRN_DPLEN(8), l2shape.s.adjust, PKO_PRN_DPLEN(8), l1shape.s.adjust,
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+			if (ciren || piren) {
+				PARPRINT(2, "YELLOW Dis", "%*d%*d%*d%*d%*d%*s%*s%*s\n",
+					PKO_PRN_DPLEN(8), dqshape.s.yellow_disable, PKO_PRN_DPLEN(8), l5shape.s.yellow_disable,
+					PKO_PRN_DPLEN(8), l4shape.s.yellow_disable, PKO_PRN_DPLEN(8), l3shape.s.yellow_disable,
+					PKO_PRN_DPLEN(8), l2shape.s.yellow_disable, PKO_PRN_DPLEN(8), "--",
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+				PARPRINT(2, "RED Dis", "%*d%*d%*d%*d%*d%*s%*s%*s\n",
+					PKO_PRN_DPLEN(8), dqshape.s.red_disable, PKO_PRN_DPLEN(8), l5shape.s.red_disable,
+					PKO_PRN_DPLEN(8), l4shape.s.red_disable, PKO_PRN_DPLEN(8), l3shape.s.red_disable,
+					PKO_PRN_DPLEN(8), l2shape.s.red_disable, PKO_PRN_DPLEN(8), "--",
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+				PARPRINT(2, "RED Algo", "%*d%*d%*d%*d%*d%*s%*s%*s\n",
+					PKO_PRN_DPLEN(8), dqshape.s.red_algo, PKO_PRN_DPLEN(8), l5shape.s.red_algo,
+					PKO_PRN_DPLEN(8), l4shape.s.red_algo, PKO_PRN_DPLEN(8), l3shape.s.red_algo,
+					PKO_PRN_DPLEN(8), l2shape.s.red_algo, PKO_PRN_DPLEN(8), "--",
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+			}
+			if (ciren) {
+				PARPRINT(2, "CIR Enable", "%*d%*d%*d%*d%*d%*d%*s%*s\n",
+					PKO_PRN_DPLEN(8), dqcir.s.enable, PKO_PRN_DPLEN(8), l5cir.s.enable,
+					PKO_PRN_DPLEN(8), l4cir.s.enable, PKO_PRN_DPLEN(8), l3cir.s.enable,
+					PKO_PRN_DPLEN(8), l2cir.s.enable, PKO_PRN_DPLEN(8), l1cir.s.enable,
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+				PARPRINT(2, "CIR Burst", "%*d%*d%*d%*d%*d%*d%*s%*s\n",
+					PKO_PRN_DPLEN(8), shaper_burst(dqcir.s.burst_mantissa, dqcir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), shaper_burst(l5cir.s.burst_mantissa, l5cir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), shaper_burst(l4cir.s.burst_mantissa, l4cir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), shaper_burst(l3cir.s.burst_mantissa, l3cir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), shaper_burst(l2cir.s.burst_mantissa, l2cir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), shaper_burst(l1cir.s.burst_mantissa, l1cir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), "(bytes)", PKO_PRN_DPLEN(8), "");
+				PARPRINT(2, "CIR Rate", "%*d%*d%*d%*d%*d%*d%*s%*s\n",
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, dqcir.s.rate_mantissa,
+						dqcir.s.rate_exponent, dqcir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, l5cir.s.rate_mantissa,
+						l5cir.s.rate_exponent, l5cir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, l4cir.s.rate_mantissa,
+						l4cir.s.rate_exponent, l4cir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, l3cir.s.rate_mantissa,
+						l3cir.s.rate_exponent, l3cir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, l2cir.s.rate_mantissa,
+						l2cir.s.rate_exponent, l2cir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), shaper_rate(pqsh_clk, l1cir.s.rate_mantissa,
+						l1cir.s.rate_exponent, l1cir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), "(Mbps)", PKO_PRN_DPLEN(8), "");
+			}
+			if (piren) {
+				PARPRINT(2, "PIR Enable", "%*d%*d%*d%*d%*d%*s%*s%*s\n",
+					PKO_PRN_DPLEN(8), dqpir.s.enable, PKO_PRN_DPLEN(8), l5pir.s.enable,
+					PKO_PRN_DPLEN(8), l4pir.s.enable, PKO_PRN_DPLEN(8), l3pir.s.enable,
+					PKO_PRN_DPLEN(8), l2pir.s.enable, PKO_PRN_DPLEN(8), "--",
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+				PARPRINT(2, "PIR Burst", "%*d%*d%*d%*d%*d%*s%*s%*s\n",
+					PKO_PRN_DPLEN(8), shaper_burst(dqpir.s.burst_mantissa, dqpir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), shaper_burst(l5pir.s.burst_mantissa, l5pir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), shaper_burst(l4pir.s.burst_mantissa, l4pir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), shaper_burst(l3pir.s.burst_mantissa, l3pir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), shaper_burst(l2pir.s.burst_mantissa, l2pir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), "--", PKO_PRN_DPLEN(8), "(bytes)", PKO_PRN_DPLEN(8), "");
+				PARPRINT(2, "PIR Rate", "%*d%*d%*d%*d%*d%*s%*s%*s\n",
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, dqpir.s.rate_mantissa,
+						dqpir.s.rate_exponent, dqpir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, l5pir.s.rate_mantissa,
+						l5pir.s.rate_exponent, l5pir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, l4pir.s.rate_mantissa,
+						l4pir.s.rate_exponent, l4pir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, l3pir.s.rate_mantissa,
+						l3pir.s.rate_exponent, l3pir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, l2pir.s.rate_mantissa,
+						l2pir.s.rate_exponent, l2pir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), "--", PKO_PRN_DPLEN(8), "(Mbps)", PKO_PRN_DPLEN(8), "");
+			}
+		}
+		else {
+			ciren = dqcir.s.enable + l3cir.s.enable + l2cir.s.enable + l1cir.s.enable;
+			piren = dqpir.s.enable + l3pir.s.enable + l2pir.s.enable;
+
+			PARPRINT(2, "Path", "%*s%*s%*s%*d%*d%*d%*d%*s\n",
+				PKO_PRN_DPLEN(8), "--", PKO_PRN_DPLEN(8), "",
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), dqtop.s.parent,
+				PKO_PRN_DPLEN(8), l3top.s.parent, PKO_PRN_DPLEN(8), l2top.s.parent,
+				PKO_PRN_DPLEN(8), l1top.s.link, PKO_PRN_DPLEN(8), lines[1]);
+			PARPRINT(2, "Prio-Anchor", "%*s%*s%*s%*d%*d%*d%*s%*s\n",
+				PKO_PRN_DPLEN(8), "--", PKO_PRN_DPLEN(8), "",
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), l3top.s.prio_anchor,
+				PKO_PRN_DPLEN(8), l2top.s.prio_anchor, PKO_PRN_DPLEN(8), l1top.s.prio_anchor,
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+			PARPRINT(2, "Prio", "%*d%*s%*s%*d%*d%*s%*s%*s\n",
+				PKO_PRN_DPLEN(8), dqsch.s.prio, PKO_PRN_DPLEN(8), "",
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), l3sch.s.prio,
+				PKO_PRN_DPLEN(8), l2sch.s.prio, PKO_PRN_DPLEN(8), "--",
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+			PARPRINT(2, "RR-Prio", "%*s%*s%*s%*d%*d%*d%*s%*s\n",
+				PKO_PRN_DPLEN(8), "--", PKO_PRN_DPLEN(8), "",
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), l3top.s.rr_prio,
+				PKO_PRN_DPLEN(8), l2top.s.rr_prio, PKO_PRN_DPLEN(8), l1top.s.rr_prio,
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+			PARPRINT(2, "RR-Quantum", "%*x%*s%*s%*x%*x%*x%*s%*s\n",
+				PKO_PRN_DPLEN(8), dqsch.s.rr_quantum, PKO_PRN_DPLEN(8), "",
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), l3sch.s.rr_quantum,
+				PKO_PRN_DPLEN(8), l2sch.s.rr_quantum, PKO_PRN_DPLEN(8), l1sch.s.rr_quantum,
+				PKO_PRN_DPLEN(8), "(hex)", PKO_PRN_DPLEN(8), "");
+			PARPRINT(2, "Len.Disable", "%*d%*s%*s%*d%*d%*d%*s%*s\n",
+				PKO_PRN_DPLEN(8), dqshape.s.length_disable, PKO_PRN_DPLEN(8), "",
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), l3shape.s.length_disable,
+				PKO_PRN_DPLEN(8), l2shape.s.length_disable, PKO_PRN_DPLEN(8), l1shape.s.length_disable,
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+			PARPRINT(2, "Len.Adjust", "%*d%*s%*s%*d%*d%*d%*s%*s\n",
+				PKO_PRN_DPLEN(8), dqshape.s.adjust, PKO_PRN_DPLEN(8), "",
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), l3shape.s.adjust,
+				PKO_PRN_DPLEN(8), l2shape.s.adjust, PKO_PRN_DPLEN(8), l1shape.s.adjust,
+				PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+			if (ciren || piren) {
+				PARPRINT(2, "YELLOW Dis", "%*d%*s%*s%*d%*d%*s%*s%*s\n",
+					PKO_PRN_DPLEN(8), dqshape.s.yellow_disable, PKO_PRN_DPLEN(8), "",
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), l3shape.s.yellow_disable,
+					PKO_PRN_DPLEN(8), l2shape.s.yellow_disable, PKO_PRN_DPLEN(8), "--",
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+				PARPRINT(2, "RED Dis", "%*d%*s%*s%*d%*d%*s%*s%*s\n",
+					PKO_PRN_DPLEN(8), dqshape.s.red_disable, PKO_PRN_DPLEN(8), "",
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), l3shape.s.red_disable,
+					PKO_PRN_DPLEN(8), l2shape.s.red_disable, PKO_PRN_DPLEN(8), "--",
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+				PARPRINT(2, "RED Algo", "%*d%*s%*s%*d%*d%*s%*s%*s\n",
+					PKO_PRN_DPLEN(8), dqshape.s.red_algo, PKO_PRN_DPLEN(8), "",
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), l3shape.s.red_algo,
+					PKO_PRN_DPLEN(8), l2shape.s.red_algo, PKO_PRN_DPLEN(8), "--",
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+			}
+			if (ciren) {
+				PARPRINT(2, "CIR Enable", "%*d%*s%*s%*d%*d%*d%*s%*s\n",
+					PKO_PRN_DPLEN(8), dqcir.s.enable, PKO_PRN_DPLEN(8), "",
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), l3cir.s.enable,
+					PKO_PRN_DPLEN(8), l2cir.s.enable, PKO_PRN_DPLEN(8), l1cir.s.enable,
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+				PARPRINT(2, "CIR Burst", "%*d%*s%*s%*d%*d%*d%*s%*s\n",
+					PKO_PRN_DPLEN(8), shaper_burst(dqcir.s.burst_mantissa, dqcir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "",
+					PKO_PRN_DPLEN(8), shaper_burst(l3cir.s.burst_mantissa, l3cir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), shaper_burst(l2cir.s.burst_mantissa, l2cir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), shaper_burst(l1cir.s.burst_mantissa, l1cir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), "(bytes)", PKO_PRN_DPLEN(8), "");
+				PARPRINT(2, "CIR Rate", "%*d%*s%*s%*d%*d%*d%*s%*s\n",
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, dqcir.s.rate_mantissa,
+						dqcir.s.rate_exponent, dqcir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "",
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, l3cir.s.rate_mantissa,
+						l3cir.s.rate_exponent, l3cir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, l2cir.s.rate_mantissa,
+						l2cir.s.rate_exponent, l2cir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), shaper_rate(pqsh_clk, l1cir.s.rate_mantissa,
+						l1cir.s.rate_exponent, l1cir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), "(Mbps)", PKO_PRN_DPLEN(8), "");
+			}
+			if (piren) {
+				PARPRINT(2, "PIR Enable", "%*d%*s%*s%*d%*d%*s%*s%*s\n",
+					PKO_PRN_DPLEN(8), dqpir.s.enable, PKO_PRN_DPLEN(8), "",
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), l3pir.s.enable,
+					PKO_PRN_DPLEN(8), l2pir.s.enable, PKO_PRN_DPLEN(8), "--",
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "");
+				PARPRINT(2, "PIR Burst", "%*d%*s%*s%*d%*d%*s%*s%*s\n",
+					PKO_PRN_DPLEN(8), shaper_burst(dqpir.s.burst_mantissa, dqpir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "",
+					PKO_PRN_DPLEN(8), shaper_burst(l3pir.s.burst_mantissa, l3pir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), shaper_burst(l2pir.s.burst_mantissa, l2pir.s.burst_exponent),
+					PKO_PRN_DPLEN(8), "--", PKO_PRN_DPLEN(8), "(bytes)", PKO_PRN_DPLEN(8), "");
+				PARPRINT(2, "PIR Rate", "%*d%*s%*s%*d%*d%*s%*s%*s\n",
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, dqpir.s.rate_mantissa,
+						dqpir.s.rate_exponent, dqpir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), "", PKO_PRN_DPLEN(8), "",
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, l3pir.s.rate_mantissa,
+						l3pir.s.rate_exponent, l3pir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), shaper_rate(dqsh_clk, l2pir.s.rate_mantissa,
+						l2pir.s.rate_exponent, l2pir.s.rate_divider_exponent),
+					PKO_PRN_DPLEN(8), "--", PKO_PRN_DPLEN(8), "(Mbps)", PKO_PRN_DPLEN(8), "");
+			}
+		}
+	}
+	if ((queue - 1) != base)
+		cvmx_printf("\nDQ(s) %02d-%02d -- same as DQ %02d\n",
+			queue - 1, base + 1, base);
+
+	/* Display FIFO Groups: */
+	DLMPRINT("FIFO Groups:");
+	cvmx_printf("Group: (FIFOs)\n");
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		ngroups = 7;
+	else
+		ngroups = 4;
+	for (group = 0, pcrc32 = 0, base = 0; group < ngroups; group++) {
+		cvmx_pko_ptgfx_cfg_t fgcfg;
+		CVMX_MT_CRC_POLYNOMIAL(0x1edc6f41);
+		CVMX_MT_CRC_IV(0xffffffff);
+		fgcfg.u64 = cvmx_read_csr_node(node, CVMX_PKO_PTGFX_CFG(group));
+		CVMX_MT_CRC_DWORD(fgcfg.u64);
+		CVMX_MF_CRC_IV(crc32);
+		if (crc32 == pcrc32)
+			continue;
+		if (group > 0 && (group - 1) != base)
+			cvmx_printf("\nGROUP(s) %02d-%02d -- same as GROUP %02d\n",
+				group - 1, base + 1, base);
+		pcrc32 = crc32;
+		base = group;
+		cvmx_printf("Group %d: (%d, %d, %d, %d)\n",
+			group, group * 4, group * 4 + 1, group * 4 + 2, group * 4 + 3);
+		PARPRINT(2, "Size", "%*d\n", PKO_PRN_DPLEN(1), fgcfg.s.size);
+		PARPRINT(2, "Rate", "%*d\n", PKO_PRN_DPLEN(1), fgcfg.s.rate);
+	}
+	if ((group - 1) != base)
+		cvmx_printf("\nGROUP(s) %02d-%02d -- same as GROUP %02d\n",
+			group - 1, base + 1, base);
+	return 0;
+}
+
+#undef PKO_PRN_HEADLEN
+#define PKO_PRN_HEADLEN  36 
+#undef PKO_PRN_DATALEN
+#define PKO_PRN_DATALEN  44
+int cvmx_helper_pko3_stats_dump(unsigned int node)
+{
+	int queue, nqueues, n;
+	cvmx_pko_dqx_packets_t dq_pkts;
+	cvmx_pko_dqx_bytes_t dq_bytes;
+	cvmx_pko_dqx_dropped_packets_t dq_drppkts;
+	cvmx_pko_dqx_dropped_bytes_t dq_drpbytes;
+	cvmx_pko_dqx_shape_state_t dq_shape_stat;
+	cvmx_pko_l1_sqx_dropped_packets_t l1_drppkts;
+	cvmx_pko_l1_sqx_dropped_bytes_t l1_drpbytes;
+	cvmx_pko_l1_sqx_red_packets_t l1_redpkts;
+	cvmx_pko_l1_sqx_red_bytes_t l1_redbytes;
+	cvmx_pko_l1_sqx_yellow_packets_t l1_yelpkts;
+	cvmx_pko_l1_sqx_yellow_bytes_t l1_yelbytes;
+	cvmx_pko_l1_sqx_green_packets_t l1_grnpkts;
+	cvmx_pko_l1_sqx_green_bytes_t l1_grnbytes;
+	cvmx_pko_l1_sqx_topology_t l1top;
+	cvmx_pko_l1_sqx_shape_state_t l1_shape_stat;
+	cvmx_pko_macx_cfg_t maccfg;
+	char lines[4][256];
+
+	memset(lines[3], '*', PKO_PRN_LINELEN);  lines[3][PKO_PRN_LINELEN] = '\0';
+	cvmx_printf("\n%s\n", lines[3]);
+	cvmx_printf("   PKO Statistics (Node %d)\n", node);
+	cvmx_printf("%s\n", lines[3]);
+	DLMPRINT("Descriptor Queues:");
+	nqueues = cvmx_pko3_num_level_queues(CVMX_PKO_DESCR_QUEUES);
+	for (queue = 0; queue < nqueues; queue++) {
+		dq_pkts.u64 = cvmx_read_csr_node(node, CVMX_PKO_DQX_PACKETS(queue));
+		dq_bytes.u64 = cvmx_read_csr_node(node, CVMX_PKO_DQX_BYTES(queue));
+		dq_drppkts.u64 = cvmx_read_csr_node(node, CVMX_PKO_DQX_DROPPED_PACKETS(queue));
+		dq_drpbytes.u64 = cvmx_read_csr_node(node, CVMX_PKO_DQX_DROPPED_BYTES(queue));
+		n = dq_pkts.s.count + dq_bytes.s.count + dq_drppkts.s.count + dq_drpbytes.s.count;
+		if (n == 0)
+			continue;
+		cvmx_printf("DQ%d:\n", queue);
+		dq_shape_stat.u64 = cvmx_read_csr_node(node, CVMX_PKO_DQX_SHAPE_STATE(queue));
+		switch (dq_shape_stat.s.color) {
+		case 0: sprintf(lines[0], "Green"); break;
+		case 1: sprintf(lines[0], "Yellow"); break;
+		case 2: sprintf(lines[0], "Red"); break;
+		default: sprintf(lines[0], "Undef"); break;
+		}
+		PARPRINT(4, "Conn.Status", "%*s\n", PKO_PRN_DPLEN(1), lines[0]);
+		PARPRINT(4, "PIR Accum", "%*d\n", PKO_PRN_DPLEN(1), dq_shape_stat.s.pir_accum);
+		PARPRINT(4, "CIR Accum", "%*d\n", PKO_PRN_DPLEN(1), dq_shape_stat.s.cir_accum);
+		if (dq_pkts.s.count)
+			PARPRINT(4, "Packets", "%*lld\n", PKO_PRN_DPLEN(1), (long long)dq_pkts.s.count);
+		if (dq_bytes.s.count)
+			PARPRINT(4, "Bytes", "%*lld\n", PKO_PRN_DPLEN(1), (long long)dq_bytes.s.count);
+		if (dq_drppkts.s.count)
+			PARPRINT(4, "Dropped Packets", "%*lld\n", PKO_PRN_DPLEN(1), (long long)dq_drppkts.s.count);
+		if (dq_drpbytes.s.count)
+			PARPRINT(4, "Dropped Bytes", "%*lld\n", PKO_PRN_DPLEN(1), (long long)dq_drpbytes.s.count);
+	}
+	DLMPRINT("Port(L1) Queues:");
+	nqueues = cvmx_pko3_num_level_queues(CVMX_PKO_PORT_QUEUES);
+	for (queue = 0; queue < nqueues; queue++) {
+		l1_grnpkts.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_GREEN_PACKETS(queue));
+		l1_grnbytes.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_GREEN_BYTES(queue));
+		l1_yelpkts.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_YELLOW_PACKETS(queue));
+		l1_yelbytes.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_YELLOW_BYTES(queue));
+		l1_redpkts.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_RED_PACKETS(queue));
+		l1_redbytes.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_RED_BYTES(queue));
+		l1_drppkts.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_DROPPED_PACKETS(queue));
+		l1_drpbytes.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_DROPPED_BYTES(queue));
+		n = l1_grnpkts.s.count + l1_grnbytes.s.count + l1_yelpkts.s.count + l1_yelbytes.s.count +
+			l1_redpkts.s.count + l1_redbytes.s.count + l1_drppkts.s.count + l1_drpbytes.s.count;
+		if (n == 0)
+			continue;
+
+		l1_shape_stat.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_SHAPE_STATE(queue));
+		l1top.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_TOPOLOGY(queue));
+		if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+			sprintf(lines[0], "%s", pko_macmap[l1top.s.link][0]);
+		else if (OCTEON_IS_MODEL(OCTEON_CN73XX))
+			sprintf(lines[0], "%s", pko_macmap[l1top.s.link][1]);
+		else if (OCTEON_IS_MODEL(OCTEON_CNF75XX))
+			sprintf(lines[0], "%s", pko_macmap[l1top.s.link][2]);
+		maccfg.u64 = cvmx_read_csr_node(node, CVMX_PKO_MACX_CFG(l1top.s.link));
+		cvmx_printf("L1-SQ%d => MAC%d (%s) => FIFO%d:\n", queue, l1top.s.link, lines[0], maccfg.s.fifo_num);
+		PARPRINT(4, "Conn.Status", "%*s\n", PKO_PRN_DPLEN(1), (l1_shape_stat.s.color == 0) ? "Green" : "Red");
+		PARPRINT(4, "CIR Accum", "%*d\n", PKO_PRN_DPLEN(1), l1_shape_stat.s.cir_accum);
+		if (l1_grnpkts.s.count)
+			PARPRINT(4, "Green Packets", "%*lld\n", PKO_PRN_DPLEN(1), (long long)l1_grnpkts.s.count);
+		if (l1_grnbytes.s.count)
+			PARPRINT(4, "Green Bytes", "%*lld\n", PKO_PRN_DPLEN(1), (long long)l1_grnbytes.s.count);
+		if (l1_yelpkts.s.count)
+			PARPRINT(4, "Yellow Packets", "%*lld\n", PKO_PRN_DPLEN(1), (long long)l1_yelpkts.s.count);
+		if (l1_yelbytes.s.count)
+			PARPRINT(4, "Yellow Bytes", "%*lld\n", PKO_PRN_DPLEN(1), (long long)l1_yelbytes.s.count);
+		if (l1_redpkts.s.count)
+			PARPRINT(4, "Red Packets", "%*lld\n", PKO_PRN_DPLEN(1), (long long)l1_redpkts.s.count);
+		if (l1_redbytes.s.count)
+			PARPRINT(4, "Red Bytes", "%*lld\n", PKO_PRN_DPLEN(1), (long long)l1_redbytes.s.count);
+		if (l1_drppkts.s.count)
+			PARPRINT(4, "Dropped Packets", "%*lld\n", PKO_PRN_DPLEN(1), (long long)l1_drppkts.s.count);
+		if (l1_drpbytes.s.count)
+			PARPRINT(4, "Dropped Bytes", "%*lld\n", PKO_PRN_DPLEN(1), (long long)l1_drpbytes.s.count);
+	}
+	return 0;
+}
+#endif /* CVMX_DUMP_PKO */
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c b/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c
index 47f1db3..3992f6e 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-rgmii.c
@@ -43,7 +43,7 @@
  * Functions for RGMII/GMII/MII initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 107050 $<hr>
+ * <hr>$Revision: 107037 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
index ee9dc89..0880e32 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-sgmii.c
@@ -43,7 +43,7 @@
  * Functions for SGMII initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 108660 $<hr>
+ * <hr>$Revision: 122069 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -682,7 +682,8 @@ cvmx_helper_link_info_t __cvmx_helper_sgmii_link_get(int ipd_port)
 
 	pcsx_miscx_ctl_reg.u64 =
 		cvmx_read_csr(CVMX_PCSX_MISCX_CTL_REG(index, interface));
-	if (pcsx_miscx_ctl_reg.s.mac_phy) {
+	if (pcsx_miscx_ctl_reg.s.mac_phy ||
+	    cvmx_helper_get_port_force_link_up(interface, index)) {
 		/* PHY Mode */
 		/* Note that this also works for 1000base-X mode */
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-srio.c b/arch/mips/cavium-octeon/executive/cvmx-helper-srio.c
index add1b92..9983850 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-srio.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-srio.c
@@ -55,14 +55,54 @@
 #include <asm/octeon/cvmx-sriox-defs.h>
 #include <asm/octeon/cvmx-sriomaintx-defs.h>
 #include <asm/octeon/cvmx-dpi-defs.h>
+#include <asm/octeon/cvmx-pki.h>
 #else
 
 #include "cvmx.h"
 #include "cvmx-helper.h"
 #include "cvmx-srio.h"
 #include "cvmx-qlm.h"
+#include "cvmx-pki.h"
 #endif
 
+static const int debug = 0;
+
+/**
+ * @INTERNAL
+ * Convert interface number to sRIO link number
+ * per SoC model.
+ *
+ * @param xiface Interface to convert
+ *
+ * @return Srio link number
+ */
+int __cvmx_helper_srio_port(int xiface)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int  srio_port = -1;
+
+	if (!octeon_has_feature(OCTEON_FEATURE_SRIO))
+		return -1;
+
+	if (xi.node != 0)
+		return -1;
+
+	if (OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+		srio_port = xi.interface - 1;
+		if (srio_port > 1)
+			srio_port = -1;
+	} else if (OCTEON_IS_MODEL(OCTEON_CN66XX)) {
+		srio_port = xi.interface - 4;
+		if (srio_port > 3 || srio_port == 1)
+			srio_port = -1;
+	} else {
+		srio_port = xi.interface - 4;
+		if (srio_port > 1)
+			srio_port = -1;
+	}
+
+	return srio_port;
+}
 
 /**
  * @INTERNAL
@@ -76,17 +116,20 @@
  */
 int __cvmx_helper_srio_probe(int xiface)
 {
-	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	cvmx_sriox_status_reg_t srio0_status_reg;
-	cvmx_sriox_status_reg_t srio1_status_reg;
+	cvmx_sriox_status_reg_t srio_status_reg;
+	int srio_port;
 
 	if (!octeon_has_feature(OCTEON_FEATURE_SRIO))
 		return 0;
 
+	srio_port = __cvmx_helper_srio_port(xiface);
+	if (srio_port < 0)
+		return 0;
+
 	/* Read MIO_QLMX_CFG CSRs to find SRIO mode. */
 	if (OCTEON_IS_MODEL(OCTEON_CN66XX)) {
+		/* QLM0 exclusively used by sRIO */
 		enum cvmx_qlm_mode mode = cvmx_qlm_get_mode(0);
-		int srio_port = xi.interface - 4;
 		switch (srio_port) {
 		case 0:	/* 1x4 lane */
 			if (mode == CVMX_QLM_MODE_SRIO_1X4 ||
@@ -103,15 +146,22 @@ int __cvmx_helper_srio_probe(int xiface)
 			if (mode == CVMX_QLM_MODE_SRIO_4X1)
 				return 2;
 			break;
+		/* Note: srio_port 1 does not exist! */
 		default:
 			break;
 		}
 		return 0;
+	} else if (OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+		srio_status_reg.u64 =
+			cvmx_read_csr(CVMX_SRIOX_STATUS_REG(srio_port));
+		if (srio_status_reg.s.srio)
+			return 2;
+		else
+			return 0;
 	}
 
-	srio0_status_reg.u64 = cvmx_read_csr(CVMX_SRIOX_STATUS_REG(0));
-	srio1_status_reg.u64 = cvmx_read_csr(CVMX_SRIOX_STATUS_REG(1));
-	if (srio0_status_reg.s.srio || srio1_status_reg.s.srio)
+	srio_status_reg.u64 = cvmx_read_csr(CVMX_SRIOX_STATUS_REG(srio_port));
+	if (srio_status_reg.s.srio)
 		return 2;
 	else
 		return 0;
@@ -130,35 +180,155 @@ int __cvmx_helper_srio_probe(int xiface)
 int __cvmx_helper_srio_enable(int xiface)
 {
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	int interface = xi.interface;
-	int num_ports = cvmx_helper_ports_on_interface(interface);
+	int num_ports = cvmx_helper_ports_on_interface(xi.interface);
 	int index;
 	cvmx_sriomaintx_core_enables_t sriomaintx_core_enables;
 	cvmx_sriox_imsg_ctrl_t sriox_imsg_ctrl;
 	cvmx_sriox_status_reg_t srio_status_reg;
 	cvmx_dpi_ctl_t dpi_ctl;
-	int srio_port = interface - 4;
+	int srio_port;
+
+	srio_port = __cvmx_helper_srio_port(xiface);
+	if (srio_port < 0)
+		return -1;
+
+	if (debug)
+		cvmx_dprintf("%s: interface %d SRIO%d ports %d\n",
+			__func__, xi.interface, srio_port, num_ports);
+
+	/* Make sure register access is allowed */
+	srio_status_reg.u64 = cvmx_read_csr(CVMX_SRIOX_STATUS_REG(srio_port));
+
+	if (debug)
+		cvmx_dprintf("%s: SRIO%d srio_status_reg=%#llx\n",
+			__func__, srio_port, CAST_ULL(srio_status_reg.u64));
+
+	if (!srio_status_reg.s.access) {
+		cvmx_printf("WARNING: %s: SRIO%d is not enabled\n",
+			__func__, srio_port);
+		return -1;
+	}
 
 	/* All SRIO ports have a cvmx_srio_rx_message_header_t header
 	   on them that must be skipped by IPD */
 	for (index = 0; index < num_ports; index++) {
-		cvmx_pip_prt_cfgx_t port_config;
 		cvmx_sriox_omsg_portx_t sriox_omsg_portx;
 		cvmx_sriox_omsg_sp_mrx_t sriox_omsg_sp_mrx;
 		cvmx_sriox_omsg_fmp_mrx_t sriox_omsg_fmp_mrx;
 		cvmx_sriox_omsg_nmp_mrx_t sriox_omsg_nmp_mrx;
-		int ipd_port = cvmx_helper_get_ipd_port(interface, index);
-		port_config.u64 = cvmx_read_csr(CVMX_PIP_PRT_CFGX(ipd_port));
-		/* Only change the skip if the user hasn't already set it */
-		if (!port_config.s.skip) {
-			port_config.s.skip = sizeof(cvmx_srio_rx_message_header_t);
-			cvmx_write_csr(CVMX_PIP_PRT_CFGX(ipd_port), port_config.u64);
+
+		if (!octeon_has_feature(OCTEON_FEATURE_PKI)) {
+			int ipd_port = cvmx_helper_get_ipd_port(xiface, index);
+			cvmx_pip_prt_cfgx_t port_config;
+			port_config.u64 =
+				cvmx_read_csr(CVMX_PIP_PRT_CFGX(ipd_port));
+			/* Only change the skip if
+			 * the user hasn't already set it
+			 */
+			if (!port_config.s.skip) {
+				port_config.s.skip =
+					sizeof(cvmx_srio_rx_message_header_t);
+				cvmx_write_csr(CVMX_PIP_PRT_CFGX(ipd_port),
+					port_config.u64);
+			}
+		} else {
+			bool use_inst_hdr = false;
+			int pknd, style, i, fcs;
+			struct cvmx_pki_pkind_config pkind_cfg;
+			struct cvmx_pki_style_config style_cfg;
+			struct cvmx_pki_qpg_config qpg_cfg;
+
+			fcs = __cvmx_helper_get_has_fcs(xiface);
+
+			/* Initialize sRIO INST_HDR_S registers */
+			// FIXME Move this code to cvmx-srio.c !!
+			if (use_inst_hdr)
+			    for(i = 0; i < 256 && index == 0; i++) {
+				cvmx_sriox_imsg_inst_hdrx_t inst_hdr;
+				inst_hdr.u64 = (1ull << 63) |(1ull << 47) |
+					(0x1a91a9 << 8) | i ; // only tag
+				cvmx_write_csr(
+					CVMX_SRIOX_IMSG_INST_HDRX(i, srio_port),
+					 inst_hdr.u64);
+			}
+
+			/* Find out the intended PKIND for this port */
+			pknd = cvmx_helper_get_pknd(xiface, index);
+
+			/* Modify PKIND configuration */
+			cvmx_pki_read_pkind_config(0, pknd, &pkind_cfg);
+			pkind_cfg.initial_parse_mode = CVMX_PKI_PARSE_LA_TO_LG;
+
+			/* 16 bytes contain SRIO header and INST_HDR_S data */
+			pkind_cfg.fcs_skip = 16;
+			pkind_cfg.inst_skip = 16;
+
+			/* Use this to honor the INST_HDR_S from sRIO regs */
+			if (use_inst_hdr) {
+				pkind_cfg.inst_skip = 8;
+				pkind_cfg.parse_en.inst_hdr = 1;
+			}
+			pkind_cfg.fcs_pres = fcs;
+
+			cvmx_pki_write_pkind_config(0, pknd, &pkind_cfg);
+
+			style = cvmx_pki_get_pkind_style(0, pknd);
+
+			if (debug)
+				cvmx_dprintf("%s: configuring PKI style %d\n",
+					__func__, style);
+			/* Customize style */
+			cvmx_pki_read_style_config(0, style,
+				CVMX_PKI_CLUSTER_ALL, &style_cfg);
+
+			/* FIXME
+			 * the following lines are only meant
+			 * to mitigate bugs in the PKI config
+			 * code, and should be redundant once
+			 * these bugs get fixed.
+			 */
+			if (debug)
+				cvmx_dprintf("%s: Style %d FCS check %d strip %d\n",
+					__func__, style,
+					style_cfg.parm_cfg.fcs_chk,
+					style_cfg.parm_cfg.fcs_strip);
+
+			/* do not include port# in QPG index calculation */
+			style_cfg.parm_cfg.qpg_port_msb = 0;
+
+			/* Force FCS checking off for sRIO */
+			style_cfg.parm_cfg.fcs_chk = fcs;
+			style_cfg.parm_cfg.fcs_strip = fcs;
+
+			cvmx_pki_write_style_config(0, style,
+				CVMX_PKI_CLUSTER_ALL, &style_cfg);
+			if (debug)
+				cvmx_dprintf(" Style qpg_base = %d, "
+				"dis_padd %d, dis_aura %d\n",
+				 style_cfg.parm_cfg.qpg_base,
+				 style_cfg.parm_cfg.qpg_dis_padd,
+				 style_cfg.parm_cfg.qpg_dis_aura);
+
+			cvmx_pki_read_qpg_entry(0,
+				style_cfg.parm_cfg.qpg_base, &qpg_cfg);
+			if (debug)
+				cvmx_dprintf(" Aura=%d\n", qpg_cfg.aura_num);
+
+			/* Set PKIND in SRIO IMSG register */
+			if (debug)
+				cvmx_dprintf("%s: setting PKIND=%d\n",
+				__func__, pknd);
+			cvmx_srio_set_pkind(srio_port, index, pknd);
 		}
 
 		/* Enable TX with PKO */
 		sriox_omsg_portx.u64 = cvmx_read_csr(CVMX_SRIOX_OMSG_PORTX(index, srio_port));
-		sriox_omsg_portx.s.port = (srio_port) * 2 + index;
-		sriox_omsg_portx.s.enable = 1;
+		if (OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+			sriox_omsg_portx.cnf75xx.enable = 1;
+		} else {
+			sriox_omsg_portx.s.port = (srio_port) * 2 + index;
+			sriox_omsg_portx.s.enable = 1;
+		}
 		cvmx_write_csr(CVMX_SRIOX_OMSG_PORTX(index, srio_port), sriox_omsg_portx.u64);
 
 		/* Allow OMSG controller to send regardless of the state of any other
@@ -211,23 +381,27 @@ int __cvmx_helper_srio_enable(int xiface)
 	cvmx_write_csr(CVMX_SRIOX_IMSG_CTRL(srio_port), sriox_imsg_ctrl.u64);
 
 	/* DPI must be enabled for us to RX messages */
+	/* FIXME: This condition is questionable */
 	dpi_ctl.u64 = cvmx_read_csr(CVMX_DPI_CTL);
-	dpi_ctl.s.clk = 1;
+	if (!OCTEON_IS_MODEL(OCTEON_CNF75XX))
+		dpi_ctl.s.clk = 1;
 	dpi_ctl.s.en = 1;
 	cvmx_write_csr(CVMX_DPI_CTL, dpi_ctl.u64);
 
-	/* Make sure register access is allowed */
-	srio_status_reg.u64 = cvmx_read_csr(CVMX_SRIOX_STATUS_REG(srio_port));
-	if (!srio_status_reg.s.access)
-		return 0;
-
 	/* Enable RX */
+	if (debug)
+		cvmx_dprintf("%s: SRIO%u enabling RX via maint regs\n",
+			__func__, srio_port);
+
 	if (!cvmx_srio_config_read32(srio_port, 0, -1, 0, 0, CVMX_SRIOMAINTX_CORE_ENABLES(srio_port), &sriomaintx_core_enables.u32)) {
 		sriomaintx_core_enables.s.imsg0 = 1;
 		sriomaintx_core_enables.s.imsg1 = 1;
 		cvmx_srio_config_write32(srio_port, 0, -1, 0, 0, CVMX_SRIOMAINTX_CORE_ENABLES(srio_port), sriomaintx_core_enables.u32);
 	}
 
+	if (debug)
+		cvmx_dprintf("%s: SRIO%u packets enabled\n",
+			__func__, srio_port);
 	return 0;
 }
 
@@ -241,8 +415,8 @@ int __cvmx_helper_srio_enable(int xiface)
  */
 cvmx_helper_link_info_t __cvmx_helper_srio_link_get(int ipd_port)
 {
-	int interface = cvmx_helper_get_interface_num(ipd_port);
-	int srio_port = interface - 4;
+	int xiface = cvmx_helper_get_interface_num(ipd_port);
+	int srio_port;
 	cvmx_helper_link_info_t result;
 	cvmx_sriox_status_reg_t srio_status_reg;
 	cvmx_sriomaintx_port_0_err_stat_t sriomaintx_port_0_err_stat;
@@ -250,6 +424,9 @@ cvmx_helper_link_info_t __cvmx_helper_srio_link_get(int ipd_port)
 	cvmx_sriomaintx_port_0_ctl2_t sriomaintx_port_0_ctl2;
 
 	result.u64 = 0;
+	srio_port = __cvmx_helper_srio_port(xiface);
+	if (srio_port < 0)
+		return result;
 
 	/* Make sure register access is allowed */
 	srio_status_reg.u64 = cvmx_read_csr(CVMX_SRIOX_STATUS_REG(srio_port));
@@ -319,6 +496,7 @@ cvmx_helper_link_info_t __cvmx_helper_srio_link_get(int ipd_port)
  */
 int __cvmx_helper_srio_link_set(int ipd_port, cvmx_helper_link_info_t link_info)
 {
+	/* FIXME: Add support for changing baud rate, reduce lanes, etc. */
 	return 0;
 }
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
index 7c46b47..1beaae0 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-util.c
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2003-2015  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -86,6 +86,8 @@
  *	This type is used for sgmii, rgmii, xaui and rxaui interfaces.
  * @param ILK
  *	This type is used for ilk interfaces.
+ * @param SRIO
+ *	This type is used for serial-RapidIo interfaces.
  * @param NPI
  *	This type is used for npi interfaces.
  * @param LB
@@ -97,6 +99,7 @@ enum port_map_if_type {
 	INVALID_IF_TYPE = 0,
 	GMII,
 	ILK,
+	SRIO,
 	NPI,
 	LB
 };
@@ -144,7 +147,7 @@ static const struct ipd_port_map ipd_port_map_68xx[CVMX_HELPER_MAX_IFACE] = {
  * Interface number to ipd port map for the octeon 78xx.
  *
  * This mapping corresponds to WQE(CHAN) enumeration in
- * HRM Sections 11.15, MKI_CHAN_E, Section 11.6
+ * HRM Sections 11.15, PKI_CHAN_E, Section 11.6
  *
  */
 static const struct ipd_port_map ipd_port_map_78xx[CVMX_HELPER_MAX_IFACE] = {
@@ -160,6 +163,31 @@ static const struct ipd_port_map ipd_port_map_78xx[CVMX_HELPER_MAX_IFACE] = {
 	{LB,	0x000,	0x03f,	0x00},		/* Interface 9 - LOOPBACK */
 };
 
+/**
+ * @INTERNAL
+ * Interface number to ipd port map for the octeon 73xx.
+ */
+static const struct ipd_port_map ipd_port_map_73xx[CVMX_HELPER_MAX_IFACE] = {
+	{GMII,	0x800,	0x83f,	0x00},		/* Interface 0 - BGX(0,0-3) */
+	{GMII,	0x900,	0x93f,	0x00},		/* Interface 1  -BGX(1,0-3) */
+	{GMII,	0xa00,	0xa3f,	0x00},		/* Interface 2  -BGX(2,0-3) */
+	{NPI,	0x100,	0x17f,	0x00},		/* Interface 3 - DPI */
+	{LB,	0x000,	0x03f,	0x00},		/* Interface 4 - LOOPBACK */
+};
+
+/**
+ * @INTERNAL
+ * Interface number to ipd port map for the octeon 75xx.
+ */
+static const struct ipd_port_map ipd_port_map_75xx[CVMX_HELPER_MAX_IFACE] = {
+	{GMII,	0x800,	0x83f,	0x00},		/* Interface 0 - BGX0 */
+	{SRIO,	0x240,	0x241,	0x00},		/* Interface 1 - SRIO 0 */
+	{SRIO,	0x242,	0x243,	0x00},		/* Interface 2 - SRIO 1 */
+	{NPI,	0x100,	0x13f,	0x00},		/* Interface 3 - DPI */
+	{LB,	0x000,	0x03f,	0x00},		/* Interface 4 - LOOPBACK */
+};
+
+
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 /**
  * Get the version of the CVMX libraries.
@@ -221,6 +249,8 @@ const char *cvmx_helper_interface_mode_to_string(cvmx_helper_interface_mode_t mo
 		return "40G_KR4";
 	case CVMX_HELPER_INTERFACE_MODE_10G_KR:
 		return "10G_KR";
+	case CVMX_HELPER_INTERFACE_MODE_MIXED:
+		return "MIXED";
 	}
 	return "UNKNOWN";
 }
@@ -378,7 +408,7 @@ static void cvmx_packet_short_ptr_calculate(void)
 	union cvmx_pip_ip_offset pip_ip_offset;
 
 	/* Fill in the common values for all cases */
-	for (i = 0; i < 4; i++) {
+	for(i = 0; i < 4; i++) {
 		if (__cvmx_ipd_mode_no_wptr())
 			/* packet pool, set to 0 in hardware */
 			__cvmx_wqe_pool = 0;
@@ -446,14 +476,14 @@ static void cvmx_packet_short_ptr_calculate(void)
 cvmx_buf_ptr_t cvmx_wqe_get_packet_ptr(cvmx_wqe_t *work)
 {
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
-		cvmx_wqe_78xx_t *wqe = (void *) work;
+		cvmx_wqe_78xx_t * wqe = (void *) work;
 		cvmx_buf_ptr_t optr, lptr;
 		cvmx_buf_ptr_pki_t nptr;
 		unsigned pool, bufs;
 		int node = cvmx_get_node_num();
 
 		/* In case of repeated calls of this function */
-		if (wqe->pki_wqe_translated || wqe->word2.software) {
+		if (wqe->pki_wqe_translated || wqe->word2.software ) {
 			optr.u64 = wqe->packet_ptr.u64;
 			return optr;
 		}
@@ -462,7 +492,7 @@ cvmx_buf_ptr_t cvmx_wqe_get_packet_ptr(cvmx_wqe_t *work)
 		pool = wqe->word0.aura;
 		nptr.u64 = wqe->packet_ptr.u64;
 
-		optr.u64 = 0;
+		optr.u64=0;
 		optr.s.pool = pool;
 		optr.s.addr = nptr.addr;
 		if (bufs == 1)
@@ -481,7 +511,7 @@ cvmx_buf_ptr_t cvmx_wqe_get_packet_ptr(cvmx_wqe_t *work)
 
 		/* Follow pointer and convert all linked pointers */
 		while (bufs > 1) {
-			void *vptr;
+			void * vptr;
 
 			vptr = cvmx_phys_to_ptr(lptr.s.addr);
 
@@ -492,14 +522,14 @@ cvmx_buf_ptr_t cvmx_wqe_get_packet_ptr(cvmx_wqe_t *work)
 			here */
 			if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
 				nptr.u64 = __builtin_bswap64(nptr.u64);
-			lptr.u64 = 0;
+			lptr.u64=0;
 			lptr.s.pool = pool;
 			lptr.s.addr = nptr.addr;
 			lptr.s.size = nptr.size;
 			lptr.s.back = (pki_dflt_style[0].parm_cfg.later_skip + 8) >> 7;	/* TBD: not guaranteed !! */
 
 			memcpy(vptr-8, &lptr, 8);
-			bufs--;
+			bufs --;
 		}
 		/* Store translated bufptr in WQE, and set indicator */
 		wqe->pki_wqe_translated = 1;
@@ -564,7 +594,7 @@ void cvmx_wqe_free(cvmx_wqe_t *work)
 	uint64_t paddr, paddr1;
 
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
-		cvmx_wqe_78xx_t *wqe = (void *) work;
+		cvmx_wqe_78xx_t * wqe = (void *) work;
 		cvmx_fpa3_gaura_t aura;
 		cvmx_buf_ptr_pki_t bptr;
 
@@ -592,7 +622,7 @@ void cvmx_wqe_free(cvmx_wqe_t *work)
 		/* WQE is separate from packet buffer, free it */
 		aura = __cvmx_fpa3_gaura(
 				wqe->word0.aura >> 10,
-				wqe->word0.aura * 0x3ff);
+				wqe->word0.aura & 0x3ff);
 
 		cvmx_fpa3_free(work, aura, ncl);
 	} else {
@@ -677,7 +707,7 @@ void cvmx_helper_free_packet_data(cvmx_wqe_t *work)
 			cvmx_fpa3_gaura_t aura =
 				__cvmx_fpa3_gaura(
 					wqe->word0.aura >> 10,
-					wqe->word0.aura * 0x3ff);
+					wqe->word0.aura & 0x3ff);
 
 			bptr.u64 = buffer_ptr.u64;
 
@@ -752,7 +782,7 @@ void cvmx_helper_setup_legacy_red(int pass_thresh, int drop_thresh)
 	ena_red = 1;
 	/* This will enable RED on all interfaces since
 	they all have packet buffer coming from  same aura */
-	cvmx_helper_setup_aura_qos(node, aura, ena_red, ena_drop, pass_thresh,
+        cvmx_helper_setup_aura_qos(node, aura, ena_red, ena_drop, pass_thresh,
 				   drop_thresh, ena_bp, 0);
 }
 
@@ -799,7 +829,7 @@ int __cvmx_helper_setup_gmx(int xiface, int num_ports)
 
 	/* The common BGX settings are already done in the appropriate
 	   enable functions, nothing to do here. */
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+	if (octeon_has_feature(OCTEON_FEATURE_BGX))
 		return 0;
 
 	/* Tell GMX the number of TX ports on this interface */
@@ -942,7 +972,14 @@ int cvmx_helper_get_ipd_port(int xiface, int index)
 		} else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
 			port_map = ipd_port_map_78xx;
 			ipd_port = cvmx_helper_node_to_ipd_port(xi.node, 0);
-		} else
+		} else if (OCTEON_IS_MODEL(OCTEON_CN73XX)) {
+			port_map = ipd_port_map_73xx;
+			ipd_port = 0;
+		} else if (OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+			port_map = ipd_port_map_75xx;
+			ipd_port = 0;
+		}
+		else
 			return -1;
 
 		ipd_port += port_map[xi.interface].first_ipd_port;
@@ -960,6 +997,8 @@ int cvmx_helper_get_ipd_port(int xiface, int index)
 			return ipd_port + index;
 		else if (port_map[xi.interface].type == NPI)
 			return ipd_port + index;
+		else if (port_map[xi.interface].type == SRIO)
+			return ipd_port + index;
 		else if (port_map[xi.interface].type == LB)
 			return ipd_port + index;
 		else {
@@ -1029,7 +1068,7 @@ void cvmx_helper_show_stats(int port)
 	if (octeon_has_feature(OCTEON_FEATURE_ILK))
 		__cvmx_helper_ilk_show_stats();
 
-	/* PIP stats */
+        /* PIP stats */
 	cvmx_pip_get_port_stats(port, 0, &status);
 	cvmx_dprintf("port %d: the number of packets - ipd: %d\n",
 			     port, (int)status.packets);
@@ -1073,6 +1112,28 @@ int cvmx_helper_get_interface_num(int ipd_port)
 				return cvmx_helper_node_interface_to_xiface(xp.node, i);
 		}
 		return -1;
+	} else if (OCTEON_IS_MODEL(OCTEON_CN73XX)) {
+		const struct ipd_port_map	*port_map;
+		int				i;
+		struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+		port_map = ipd_port_map_73xx;
+		for (i = 0; i < CVMX_HELPER_MAX_IFACE; i++) {
+			if (xp.port >= port_map[i].first_ipd_port &&
+			    xp.port <= port_map[i].last_ipd_port)
+				return i;
+		}
+		return -1;
+	} else if (OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+		const struct ipd_port_map	*port_map;
+		int				i;
+		struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+		port_map = ipd_port_map_75xx;
+		for (i = 0; i < CVMX_HELPER_MAX_IFACE; i++) {
+			if (xp.port >= port_map[i].first_ipd_port &&
+			    xp.port <= port_map[i].last_ipd_port)
+				return cvmx_helper_node_interface_to_xiface(xp.node, i);
+		}
+		return -1;
 	} else if (OCTEON_IS_MODEL(OCTEON_CN70XX) && ipd_port == 24) {
 		return 4;
 	} else {
@@ -1122,7 +1183,18 @@ int cvmx_helper_get_interface_index_num(int ipd_port)
 			struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
 			port_map = ipd_port_map_78xx;
 			ipd_port = xp.port;
-		} else
+		}
+		else if (OCTEON_IS_MODEL(OCTEON_CN73XX)) {
+			struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+			port_map = ipd_port_map_73xx;
+			ipd_port = xp.port;
+		}
+		else if (OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+			struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
+			port_map = ipd_port_map_75xx;
+			ipd_port = xp.port;
+		}
+		else
 			return -1;
 
 		num_interfaces = cvmx_helper_get_number_of_interfaces();
@@ -1138,20 +1210,38 @@ int cvmx_helper_get_interface_index_num(int ipd_port)
 
 		/* Convert the ipd port to the interface port */
 		switch (type) {
+		/* Ethernet interfaces have a channel in lower 4 bits
+		 * that is does not discriminate traffic, and is ignored.
+		 */
 		case GMII:
-			port = ((ipd_port & 0xff) >> 6);
-			return port ? (port - 1) : ((ipd_port & 0xff) >> 4);
-			break;
+			port = ipd_port - port_map[i].first_ipd_port;
+
+			/* CN68XX adds 0x40 to IPD_PORT when in XAUI/RXAUI
+			 * mode of operation, adjust for that case
+			 */
+			if (port >= port_map[i].ipd_port_adj)
+				port -= port_map[i].ipd_port_adj;
+
+			port >>= 4;
+			/* cvmx_dprintf("%s: ipd_port=%#x port=%d,%d\n", __func__, ipd_port, port,i); */
+			return port;
 
+		/*
+		 * These interfaces do not have physical ports,
+		 * but have logical channels instead that separate
+		 * traffic into logical streams
+		 */
 		case ILK:
+		case SRIO:
 		case NPI:
 		case LB:
-			return ipd_port & 0xff;
-			break;
+			port = ipd_port - port_map[i].first_ipd_port;
+		        /* cvmx_dprintf("%s: ipd_port=%#x port=%d, i = %d\n", __func__, ipd_port, port,i); */
+			return port;
 
 		default:
-			cvmx_dprintf("cvmx_helper_get_interface_index_num: "
-				     "Illegal IPD port number %d\n", ipd_port);
+			cvmx_printf("ERROR: %s: Illegal IPD port number %#x\n",
+				__func__, ipd_port);
 			return -1;
 		}
 	}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c b/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c
index e5a3d86..0ad39c1 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper-xaui.c
@@ -43,7 +43,7 @@
  * Functions for XAUI initialization, configuration,
  * and monitoring.
  *
- * <hr>$Revision: 107050 $<hr>
+ * <hr>$Revision: 106932 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -192,7 +192,7 @@ int __cvmx_helper_xaui_probe(int xiface)
 
 /**
  * @INTERNAL
- * Bringup XAUI interface. After this call packet I/O should be
+ * Bringup XAUI interface. After this call packet I/O should be 
  * fully functional.
  *
  * @param interface Interface to bring up
diff --git a/arch/mips/cavium-octeon/executive/cvmx-helper.c b/arch/mips/cavium-octeon/executive/cvmx-helper.c
index 68d9414..a41264a 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-helper.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-helper.c
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2003-2015  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -78,6 +78,7 @@
 #include <asm/octeon/cvmx-helper-errata.h>
 #include <asm/octeon/cvmx-helper-cfg.h>
 #include <asm/octeon/cvmx-helper-pki.h>
+#include <asm/octeon/cvmx-helper-fpa.h>
 #include <asm/octeon/cvmx-pki.h>
 #include <asm/octeon/cvmx-helper-pko.h>
 #include <asm/octeon/cvmx-helper-pko3.h>
@@ -103,6 +104,7 @@
 #include "cvmx-helper-errata.h"
 #include "cvmx-helper-cfg.h"
 #include "cvmx-helper-pki.h"
+#include "cvmx-helper-fpa.h"
 #include "cvmx-pki.h"
 #include "cvmx-helper-pko.h"
 #include "cvmx-helper-pko3.h"
@@ -421,6 +423,21 @@ static const struct iface_ops iface_ops_spi = {
 /**
  * @INTERNAL
  * This structure specifies the interface methods used by interfaces
+ * configured as mixed mode, some ports are sgmii and some are xfi.
+ */
+static const struct iface_ops iface_ops_bgx_mixed = {
+	.mode		= CVMX_HELPER_INTERFACE_MODE_MIXED,
+	.enumerate	= __cvmx_helper_bgx_enumerate,
+	.probe		= __cvmx_helper_bgx_probe,
+	.enable		= __cvmx_helper_bgx_mixed_enable,
+	.link_get	= __cvmx_helper_bgx_mixed_link_get,
+	.link_set	= __cvmx_helper_bgx_mixed_link_set,
+	.loopback	= __cvmx_helper_bgx_mixed_configure_loopback,
+};
+
+/**
+ * @INTERNAL
+ * This structure specifies the interface methods used by interfaces
  * configured as loop.
  */
 static const struct iface_ops iface_ops_loop = {
@@ -683,6 +700,10 @@ int cvmx_helper_get_number_of_interfaces(void)
 		return 5;
 	else if (OCTEON_IS_MODEL(OCTEON_CN78XX))
 		return 10;
+	else if (OCTEON_IS_MODEL(OCTEON_CNF75XX))
+		return 5;
+	else if (OCTEON_IS_MODEL(OCTEON_CN73XX))
+		return 5;
 	else
 		return 3;
 }
@@ -705,16 +726,17 @@ EXPORT_SYMBOL(__cvmx_helper_early_ports_on_interface);
  * chip and configuration, this can be 1-16. A value of 0
  * specifies that the interface doesn't exist or isn't usable.
  *
- * @param interface Interface to get the port count for
+ * @param xiface xiface to get the port count for
  *
  * @return Number of ports on interface. Can be Zero.
  */
-int cvmx_helper_ports_on_interface(int interface)
+int cvmx_helper_ports_on_interface(int xiface)
 {
 	if (octeon_has_feature(OCTEON_FEATURE_PKND))
-		return cvmx_helper_interface_enumerate(interface);
+		return cvmx_helper_interface_enumerate(xiface);
 	else
-		return __cvmx_helper_get_num_ipd_ports(interface);
+		return __cvmx_helper_get_num_ipd_ports(xiface);
+
 }
 EXPORT_SYMBOL(cvmx_helper_ports_on_interface);
 
@@ -736,7 +758,8 @@ static cvmx_helper_interface_mode_t __cvmx_get_mode_cn70xx(int interface)
 			iface_ops[interface] = &iface_ops_rxaui;
 		else
 			iface_ops[interface] = &iface_ops_dis;
-	} else if (interface == 2) /* DPI */
+	}
+	else if (interface == 2) /* DPI */
 		iface_ops[interface] = &iface_ops_npi;
 	else if (interface == 3) /* LOOP */
 		iface_ops[interface] = &iface_ops_loop;
@@ -747,7 +770,8 @@ static cvmx_helper_interface_mode_t __cvmx_get_mode_cn70xx(int interface)
 			iface_ops[interface] = &iface_ops_agl;
 		else
 			iface_ops[interface] = &iface_ops_dis;
-	} else
+	}
+	else
 		iface_ops[interface] = &iface_ops_dis;
 
 	return iface_ops[interface]->mode;
@@ -762,7 +786,7 @@ static cvmx_helper_interface_mode_t __cvmx_get_mode_cn78xx(int xiface)
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 	/* SGMII/RXAUI/XAUI */
 	if (xi.interface < 6) {
-		int qlm = cvmx_qlm_interface(xiface);
+		int qlm = cvmx_qlm_lmac(xiface, 0);
 		enum cvmx_qlm_mode qlm_mode;
 
 		if (qlm == -1) {
@@ -779,28 +803,112 @@ static cvmx_helper_interface_mode_t __cvmx_get_mode_cn78xx(int xiface)
 			iface_node_ops[xi.node][xi.interface] = &iface_ops_bgx_xlaui;
 		else if (qlm_mode == CVMX_QLM_MODE_XFI)
 			iface_node_ops[xi.node][xi.interface] = &iface_ops_bgx_xfi;
+#ifndef CVMX_BUILD_FOR_UBOOT
+		/* Disable 10G_KR and 40G_KR4 in u-boot, 78xx pass1.x has
+		   errata related to training */
 		else if (qlm_mode == CVMX_QLM_MODE_10G_KR)
 			iface_node_ops[xi.node][xi.interface] = &iface_ops_bgx_10G_KR;
 		else if (qlm_mode == CVMX_QLM_MODE_40G_KR4)
 			iface_node_ops[xi.node][xi.interface] = &iface_ops_bgx_40G_KR4;
+#endif
 		else if (qlm_mode == CVMX_QLM_MODE_RXAUI)
 			iface_node_ops[xi.node][xi.interface] = &iface_ops_bgx_rxaui;
 		else
 			iface_node_ops[xi.node][xi.interface] = &iface_ops_dis;
 	} else if (xi.interface < 8) {
 		enum cvmx_qlm_mode qlm_mode;
+		int found = 0;
+		int i;
+		int intf, lane_mask;
+
 		if (xi.interface == 6) {
+			intf = 6;
+			lane_mask = cvmx_ilk_lane_mask[xi.node][0];
+		} else {
+			intf = 7;
+			lane_mask = cvmx_ilk_lane_mask[xi.node][1];
+		}
+		switch (lane_mask) {
+		default:
+		case 0x0:
+			iface_node_ops[xi.node][intf] = &iface_ops_dis;
+			break;
+		case 0xf:
 			qlm_mode = cvmx_qlm_get_mode_cn78xx(xi.node, 4);
 			if (qlm_mode == CVMX_QLM_MODE_ILK)
-				iface_node_ops[xi.node][xi.interface] = &iface_ops_ilk;
+				iface_node_ops[xi.node][intf] = &iface_ops_ilk;
+			else
+				iface_node_ops[xi.node][intf] = &iface_ops_dis;
+			break;
+		case 0xff:
+			found = 0;
+			for (i = 4; i < 6; i++) {
+				qlm_mode = cvmx_qlm_get_mode_cn78xx(xi.node, i);
+				if (qlm_mode == CVMX_QLM_MODE_ILK)
+					found++;
+			}
+			if (found == 2)
+				iface_node_ops[xi.node][intf] = &iface_ops_ilk;
 			else
-				iface_node_ops[xi.node][xi.interface] = &iface_ops_dis;
-		} else if (xi.interface == 7) {
+				iface_node_ops[xi.node][intf] = &iface_ops_dis;
+			break;
+		case 0xfff:
+			found = 0;
+			for (i = 4; i < 7; i++) {
+				qlm_mode = cvmx_qlm_get_mode_cn78xx(xi.node, i);
+				if (qlm_mode == CVMX_QLM_MODE_ILK)
+					found++;
+			}
+			if (found == 3)
+				iface_node_ops[xi.node][intf] = &iface_ops_ilk;
+			else
+				iface_node_ops[xi.node][intf] = &iface_ops_dis;
+			break;
+		case 0xff00:
+			found = 0;
+			for (i = 6; i < 8; i++) {
+				qlm_mode = cvmx_qlm_get_mode_cn78xx(xi.node, i);
+				if (qlm_mode == CVMX_QLM_MODE_ILK)
+					found++;
+			}
+			if (found == 2)
+				iface_node_ops[xi.node][intf] = &iface_ops_ilk;
+			else
+				iface_node_ops[xi.node][intf] = &iface_ops_dis;
+			break;
+		case 0xf0:
 			qlm_mode = cvmx_qlm_get_mode_cn78xx(xi.node, 5);
 			if (qlm_mode == CVMX_QLM_MODE_ILK)
-				iface_node_ops[xi.node][xi.interface] = &iface_ops_ilk;
+				iface_node_ops[xi.node][intf] = &iface_ops_ilk;
+			else
+				iface_node_ops[xi.node][intf] = &iface_ops_dis;
+			break;
+		case 0xf00:
+			qlm_mode = cvmx_qlm_get_mode_cn78xx(xi.node, 6);
+			if (qlm_mode == CVMX_QLM_MODE_ILK)
+				iface_node_ops[xi.node][intf] = &iface_ops_ilk;
+			else
+				iface_node_ops[xi.node][intf] = &iface_ops_dis;
+			break;
+		case 0xf000:
+			qlm_mode = cvmx_qlm_get_mode_cn78xx(xi.node, 7);
+			if (qlm_mode == CVMX_QLM_MODE_ILK)
+				iface_node_ops[xi.node][intf] = &iface_ops_ilk;
 			else
-				iface_node_ops[xi.node][xi.interface] = &iface_ops_dis;
+				iface_node_ops[xi.node][intf] = &iface_ops_dis;
+			break;
+		case 0xfff0:
+			found = 0;
+			for (i = 5; i < 8; i++) {
+				qlm_mode = cvmx_qlm_get_mode_cn78xx(xi.node, i);
+				if (qlm_mode == CVMX_QLM_MODE_ILK)
+					found++;
+			}
+			if (found == 3)
+				iface_node_ops[xi.node][intf] = &iface_ops_ilk;
+			else
+				iface_node_ops[xi.node][intf] = &iface_ops_dis;
+			break;
 		}
 	} else if (xi.interface == 8) { /* DPI */
 		int qlm = 0;
@@ -822,6 +930,141 @@ static cvmx_helper_interface_mode_t __cvmx_get_mode_cn78xx(int xiface)
 
 /**
  * @INTERNAL
+ * Return interface mode for CN73XX.
+ */
+static cvmx_helper_interface_mode_t __cvmx_get_mode_cn73xx(int xiface)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
+
+	/* SGMII/XAUI/XLAUI/XFI */
+	if (interface < 3) {
+		int qlm = cvmx_qlm_lmac(xiface, 0);
+		enum cvmx_qlm_mode qlm_mode;
+
+		if (qlm == -1) {
+			iface_ops[interface] = &iface_ops_dis;
+			return iface_ops[interface]->mode;
+		}
+		qlm_mode = cvmx_qlm_get_mode(qlm);
+
+		switch (qlm_mode) {
+		case CVMX_QLM_MODE_SGMII:
+		case CVMX_QLM_MODE_SGMII_2X1:
+		case CVMX_QLM_MODE_RGMII_SGMII:
+		case CVMX_QLM_MODE_RGMII_SGMII_1X1:
+			iface_ops[interface] = &iface_ops_bgx_sgmii;
+			break;
+		case CVMX_QLM_MODE_XAUI:
+		case CVMX_QLM_MODE_RGMII_XAUI:
+			iface_ops[interface] = &iface_ops_bgx_xaui;
+			break;
+		case CVMX_QLM_MODE_RXAUI:
+		case CVMX_QLM_MODE_RXAUI_1X2:
+		case CVMX_QLM_MODE_RGMII_RXAUI:
+			iface_ops[interface] = &iface_ops_bgx_rxaui;
+			break;
+		case CVMX_QLM_MODE_XLAUI:
+		case CVMX_QLM_MODE_RGMII_XLAUI:
+			iface_ops[interface] = &iface_ops_bgx_xlaui;
+			break;
+		case CVMX_QLM_MODE_XFI:
+		case CVMX_QLM_MODE_XFI_1X2:
+		case CVMX_QLM_MODE_RGMII_XFI:
+			iface_ops[interface] = &iface_ops_bgx_xfi;
+			break;
+		case CVMX_QLM_MODE_10G_KR:
+		case CVMX_QLM_MODE_10G_KR_1X2:
+		case CVMX_QLM_MODE_RGMII_10G_KR:
+			iface_ops[interface] = &iface_ops_bgx_10G_KR;
+			break;
+		case CVMX_QLM_MODE_40G_KR4:
+		case CVMX_QLM_MODE_RGMII_40G_KR4:
+			iface_ops[interface] = &iface_ops_bgx_40G_KR4;
+			break;
+		case CVMX_QLM_MODE_MIXED:
+			iface_ops[interface] = &iface_ops_bgx_mixed;
+			break;
+		default:
+			iface_ops[interface] = &iface_ops_dis;
+			break;
+		}
+	} else if (interface == 3) /* DPI */
+		iface_ops[interface] = &iface_ops_npi;
+	else if (interface == 4) /* LOOP */
+		iface_ops[interface] = &iface_ops_loop;
+	else
+		iface_ops[interface] = &iface_ops_dis;
+
+	return iface_ops[interface]->mode;
+}
+
+
+/**
+ * @INTERNAL
+ * Return interface mode for CNF75XX.
+ *
+ * CNF75XX has a single BGX block, which is attached to two DLMs,
+ * the first, GSER4 only supports SGMII mode, while the second,
+ * GSER5 supports 1G/10G single late modes, i.e. SGMII, XFI, 10G-KR.
+ * Each half-BGX is thus designated as a separate interface with two ports each.
+ */
+static cvmx_helper_interface_mode_t __cvmx_get_mode_cnf75xx(int xiface)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
+
+	/* BGX0: SGMII (DLM4/DLM5)/XFI(DLM5)  */
+	if (interface < 1) {
+		enum cvmx_qlm_mode qlm_mode;
+		int qlm = cvmx_qlm_lmac(xiface, 0);
+
+		if (qlm == -1) {
+			iface_ops[interface] = &iface_ops_dis;
+			return iface_ops[interface]->mode;
+		}
+		qlm_mode = cvmx_qlm_get_mode(qlm);
+
+		switch (qlm_mode) {
+		case CVMX_QLM_MODE_SGMII:
+		case CVMX_QLM_MODE_SGMII_2X1:
+			iface_ops[interface] = &iface_ops_bgx_sgmii;
+			break;
+		case CVMX_QLM_MODE_XFI_1X2:
+			iface_ops[interface] = &iface_ops_bgx_xfi;
+			break;
+		case CVMX_QLM_MODE_10G_KR_1X2:
+			iface_ops[interface] = &iface_ops_bgx_10G_KR;
+			break;
+		case CVMX_QLM_MODE_MIXED:
+			iface_ops[interface] = &iface_ops_bgx_mixed;
+			break;
+		default:
+			iface_ops[interface] = &iface_ops_dis;
+			break;
+		}
+	} else if (interface < 3) {
+		cvmx_sriox_status_reg_t sriox_status_reg;
+		int srio_port = interface - 1;
+		sriox_status_reg.u64 =
+			cvmx_read_csr(CVMX_SRIOX_STATUS_REG(srio_port));
+
+		if (sriox_status_reg.s.srio)
+			iface_ops[interface] = &iface_ops_srio;
+		else
+			iface_ops[interface] = &iface_ops_dis;
+	} else if (interface == 3) /* DPI */
+		iface_ops[interface] = &iface_ops_npi;
+	else if (interface == 4) /* LOOP */
+		iface_ops[interface] = &iface_ops_loop;
+	else
+		iface_ops[interface] = &iface_ops_dis;
+
+	return iface_ops[interface]->mode;
+}
+
+/**
+ * @INTERNAL
  * Return interface mode for CN68xx.
  */
 static cvmx_helper_interface_mode_t __cvmx_get_mode_cn68xx(int interface)
@@ -883,18 +1126,19 @@ static cvmx_helper_interface_mode_t __cvmx_get_mode_cn68xx(int interface)
 		break;
 
 	case 7:
-       {
-               union cvmx_mio_qlmx_cfg qlm_cfg1;
-               /* Check if PCIe0/PCIe1 is configured for PCIe */
-               qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(3));
-               qlm_cfg1.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(1));
-                /* QLM is disabled when QLM SPD is 15. */
-               if ((qlm_cfg.s.qlm_spd != 15 && qlm_cfg.s.qlm_cfg == 0)
+	{
+		union cvmx_mio_qlmx_cfg qlm_cfg1;
+		/* Check if PCIe0/PCIe1 is configured for PCIe */
+		qlm_cfg.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(3));
+		qlm_cfg1.u64 = cvmx_read_csr(CVMX_MIO_QLMX_CFG(1));
+		/* QLM is disabled when QLM SPD is 15. */
+		if ((qlm_cfg.s.qlm_spd != 15 && qlm_cfg.s.qlm_cfg == 0)
                     || (qlm_cfg1.s.qlm_spd != 15 && qlm_cfg1.s.qlm_cfg == 0))
-                        iface_ops[interface] = &iface_ops_npi;
-               else
-                       iface_ops[interface] = &iface_ops_dis;
-       }
+			iface_ops[interface] = &iface_ops_npi;
+		else
+			iface_ops[interface] = &iface_ops_dis;
+	}
+		break;
 
 	case 8:
 		iface_ops[interface] = &iface_ops_loop;
@@ -948,7 +1192,8 @@ static cvmx_helper_interface_mode_t __cvmx_get_mode_octeon2(int interface)
 			else
 				iface_ops[interface] = &iface_ops_dis;
 		}
-	} else if (OCTEON_IS_MODEL(OCTEON_CN66XX)) {
+	}
+	else if (OCTEON_IS_MODEL(OCTEON_CN66XX)) {
 		union cvmx_mio_qlmx_cfg mio_qlm_cfg;
 
 		/* QLM2 is SGMII0 and QLM1 is SGMII1 */
@@ -997,9 +1242,11 @@ static cvmx_helper_interface_mode_t __cvmx_get_mode_octeon2(int interface)
 				iface_ops[interface] = &iface_ops_sgmii;
 			else
 				iface_ops[interface] = &iface_ops_dis;
-		} else
+		}
+		else
 			iface_ops[interface] = &iface_ops_dis;
-	} else if (interface == 1 && OCTEON_IS_MODEL(OCTEON_CN63XX))
+	}
+	else if (interface == 1 && OCTEON_IS_MODEL(OCTEON_CN63XX))
 		iface_ops[interface] = &iface_ops_dis;
 	else {
 		mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(interface));
@@ -1043,7 +1290,6 @@ static cvmx_helper_interface_mode_t __cvmx_get_mode_octeon2(int interface)
  */
 cvmx_helper_interface_mode_t cvmx_helper_interface_get_mode(int xiface)
 {
-	union cvmx_gmxx_inf_mode mode;
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 
 	if (xi.interface < 0 ||
@@ -1055,8 +1301,11 @@ cvmx_helper_interface_mode_t cvmx_helper_interface_get_mode(int xiface)
 	 * simply return it. Otherwise, fall through the rest of the code to
 	 * determine the interface mode and cache it in iface_ops.
 	 */
-	if (iface_node_ops[xi.node][xi.interface] != NULL)
-		return iface_node_ops[xi.node][xi.interface]->mode;
+	if (iface_node_ops[xi.node][xi.interface] != NULL) {
+		cvmx_helper_interface_mode_t mode;
+		mode = iface_node_ops[xi.node][xi.interface]->mode;
+		return mode;
+	}
 
 	/*
 	 * OCTEON III models
@@ -1067,6 +1316,18 @@ cvmx_helper_interface_mode_t cvmx_helper_interface_get_mode(int xiface)
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
 		return __cvmx_get_mode_cn78xx(xiface);
 
+	if (OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+		cvmx_helper_interface_mode_t mode;
+		mode = __cvmx_get_mode_cnf75xx(xiface);
+		return mode;
+	}
+
+	if (OCTEON_IS_MODEL(OCTEON_CN73XX)) {
+		cvmx_helper_interface_mode_t mode;
+		mode = __cvmx_get_mode_cn73xx(xiface);
+		return mode;
+	}
+
 	/*
 	 * Octeon II models
 	 */
@@ -1084,7 +1345,8 @@ cvmx_helper_interface_mode_t cvmx_helper_interface_get_mode(int xiface)
 			iface_ops[xi.interface] = &iface_ops_loop;
 		else
 			iface_ops[xi.interface] = &iface_ops_dis;
-	} else if (xi.interface == 0 &&
+	}
+	else if (xi.interface == 0 &&
 	    cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_CN3005_EVB_HS5 &&
 	    cvmx_sysinfo_get()->board_rev_major == 1) {
 		/*
@@ -1098,7 +1360,8 @@ cvmx_helper_interface_mode_t cvmx_helper_interface_get_mode(int xiface)
 		 * setup between GMII and RGMII modes.
 		 */
 		iface_ops[xi.interface] = &iface_ops_gmii;
-	} else if ((xi.interface == 1)
+	}
+	else if ((xi.interface == 1)
 	    && (OCTEON_IS_MODEL(OCTEON_CN31XX)
 		|| OCTEON_IS_MODEL(OCTEON_CN30XX)
 		|| OCTEON_IS_MODEL(OCTEON_CN50XX)
@@ -1106,6 +1369,7 @@ cvmx_helper_interface_mode_t cvmx_helper_interface_get_mode(int xiface)
 		/* Interface 1 is always disabled on CN31XX and CN30XX */
 		iface_ops[xi.interface] = &iface_ops_dis;
 	else {
+		union cvmx_gmxx_inf_mode mode;
 		mode.u64 = cvmx_read_csr(CVMX_GMXX_INF_MODE(xi.interface));
 
 		if (OCTEON_IS_MODEL(OCTEON_CN56XX) ||
@@ -1214,6 +1478,7 @@ int cvmx_helper_interface_probe(int xiface)
 	case CVMX_HELPER_INTERFACE_MODE_XFI:
 	case CVMX_HELPER_INTERFACE_MODE_10G_KR:
 	case CVMX_HELPER_INTERFACE_MODE_40G_KR4:
+	case CVMX_HELPER_INTERFACE_MODE_MIXED:
 		has_fcs = 1;
 		padding = CVMX_PKO_PADDING_60;
 		break;
@@ -1284,7 +1549,7 @@ EXPORT_SYMBOL(cvmx_helper_interface_probe);
  *
  * @return Zero on success, negative on failure
  */
-static int __cvmx_helper_global_setup_backpressure(void)
+static int __cvmx_helper_global_setup_backpressure(int node)
 {
 	if (cvmx_rgmii_backpressure_dis) {
 		/* Disable backpressure if configured to do so */
@@ -1292,7 +1557,8 @@ static int __cvmx_helper_global_setup_backpressure(void)
 		int num_interfaces = cvmx_helper_get_number_of_interfaces();
 		int interface;
 		for (interface = 0; interface < num_interfaces; interface++) {
-			switch (cvmx_helper_interface_get_mode(interface)) {
+			int xiface = cvmx_helper_node_interface_to_xiface(node, interface);
+			switch (cvmx_helper_interface_get_mode(xiface)) {
 			case CVMX_HELPER_INTERFACE_MODE_DISABLED:
 			case CVMX_HELPER_INTERFACE_MODE_PCIE:
 			case CVMX_HELPER_INTERFACE_MODE_SRIO:
@@ -1312,8 +1578,9 @@ static int __cvmx_helper_global_setup_backpressure(void)
 			case CVMX_HELPER_INTERFACE_MODE_SGMII:
 			case CVMX_HELPER_INTERFACE_MODE_QSGMII:
 			case CVMX_HELPER_INTERFACE_MODE_PICMG:
-				if (OCTEON_IS_MODEL(OCTEON_CN78XX))
-					cvmx_bgx_set_backpressure_override(interface, 0xf);
+			case CVMX_HELPER_INTERFACE_MODE_MIXED:
+				if (octeon_has_feature(OCTEON_FEATURE_BGX))
+					cvmx_bgx_set_backpressure_override(xiface, 0xf);
 				else
 					cvmx_gmx_set_backpressure_override(interface, 0xf);
 				break;
@@ -1322,7 +1589,7 @@ static int __cvmx_helper_global_setup_backpressure(void)
 				break;
 			}
 		}
-		/* cvmx_dprintf("Disabling backpressure\n"); */
+		//cvmx_dprintf("Disabling backpressure\n");
 	}
 	return 0;
 }
@@ -1462,7 +1729,6 @@ int __cvmx_helper_packet_hardware_enable(int xiface)
 	int result = 0;
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 
-	cvmx_helper_interface_get_mode(xiface);
 	if (iface_node_ops[xi.node][xi.interface]->enable)
 		result = iface_node_ops[xi.node][xi.interface]->enable(xiface);
 	result |= __cvmx_helper_board_hardware_enable(xiface);
@@ -1489,7 +1755,6 @@ int cvmx_helper_ipd_and_packet_input_enable_node(int node)
 
 	if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
 		cvmx_helper_pki_enable(node);
-
 	} else
 		/* Enable IPD */
 		cvmx_ipd_enable();
@@ -1510,7 +1775,7 @@ int cvmx_helper_ipd_and_packet_input_enable_node(int node)
 	/* Finally enable PKO now that the entire path is up and running */
 	/* enable pko */
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
-		; /* cvmx_pko_enable_78xx(0); already enabled */
+		; // cvmx_pko_enable_78xx(0); already enabled
 	} else {
 #ifdef CVMX_BUILD_FOR_STANDALONE
 		__cvmx_install_gmx_error_handler_for_xaui();
@@ -1539,6 +1804,7 @@ int cvmx_helper_initialize_packet_io_node(unsigned int node)
 {
 	int result = 0;
 	int interface;
+	int xiface;
 	union cvmx_l2c_cfg l2c_cfg;
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 	union cvmx_smix_en smix_en;
@@ -1579,6 +1845,9 @@ int cvmx_helper_initialize_packet_io_node(unsigned int node)
 		if (OCTEON_IS_MODEL(OCTEON_CN68XX) ||
 		    OCTEON_IS_MODEL(OCTEON_CN78XX))
 			smi_inf = 4;
+		else if (OCTEON_IS_MODEL(OCTEON_CN73XX) ||
+			OCTEON_IS_MODEL(OCTEON_CNF75XX))
+			smi_inf = 2;
 		else if (OCTEON_IS_MODEL(OCTEON_CN3XXX) ||
 			 OCTEON_IS_MODEL(OCTEON_CN58XX) ||
 			 OCTEON_IS_MODEL(OCTEON_CN50XX))
@@ -1597,16 +1866,19 @@ int cvmx_helper_initialize_packet_io_node(unsigned int node)
 	}
 #endif
 
-	__cvmx_helper_init_port_valid();
+	__cvmx_helper_init_port_valid();//vinita_to_do ask it need to be modify for multinode
 
-	for (interface = 0; interface < num_interfaces; interface++)
-		result |= cvmx_helper_interface_probe(interface);
+	for (interface = 0; interface < num_interfaces; interface++) {
+		xiface = cvmx_helper_node_interface_to_xiface(node, interface);
+		result |= cvmx_helper_interface_probe(xiface);
+	}
 
 	/* PKO3 init precedes that of interfaces */
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
 		__cvmx_helper_init_port_config_data(node);
 		result = cvmx_helper_pko3_init_global(node);
-	} else {
+	}
+	else {
 		result = cvmx_helper_pko_init();
 	}
 
@@ -1614,17 +1886,18 @@ int cvmx_helper_initialize_packet_io_node(unsigned int node)
 		return result;
 
 	for (interface = 0; interface < num_interfaces; interface++) {
+		xiface = cvmx_helper_node_interface_to_xiface(node, interface);
 		/* Skip invalid/disabled interfaces */
-		if (cvmx_helper_ports_on_interface(interface) <= 0)
+		if (cvmx_helper_ports_on_interface(xiface) <= 0)
 			continue;
-		cvmx_printf("Interface %d has %d ports (%s)\n",
+		cvmx_printf("Node %d Interface %d has %d ports (%s)\n", node,
 			    interface,
-			    cvmx_helper_ports_on_interface(interface),
-			    cvmx_helper_interface_mode_to_string(cvmx_helper_interface_get_mode(interface)));
+			    cvmx_helper_ports_on_interface(xiface),
+			    cvmx_helper_interface_mode_to_string(cvmx_helper_interface_get_mode(xiface)));
 
-		result |= __cvmx_helper_ipd_setup_interface(interface);
+		result |= __cvmx_helper_ipd_setup_interface(xiface);
 		if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE))
-			result |= cvmx_helper_pko3_init_interface(cvmx_helper_node_interface_to_xiface(node, interface));
+			result |= cvmx_helper_pko3_init_interface(xiface);
 		else
 			result |= __cvmx_helper_interface_setup_pko(interface);
 	}
@@ -1635,7 +1908,7 @@ int cvmx_helper_initialize_packet_io_node(unsigned int node)
 		result |= __cvmx_helper_ipd_global_setup();
 
 	/* Enable any flow control and backpressure */
-	result |= __cvmx_helper_global_setup_backpressure();
+	result |= __cvmx_helper_global_setup_backpressure(node);
 
 	/* export app config if set */
 	if (cvmx_export_app_config) {
@@ -1739,124 +2012,52 @@ int cvmx_agl_set_backpressure_override(uint32_t interface, uint32_t port_mask)
 }
 
 /**
- * Disables the sending of flow control (pause) frames on the specified
- * BGX port(s).
- *
- * @param interface Which interface (0 or 1)
- * @param port_mask Mask (4bits) of which ports on the interface to disable
- *                  backpressure on.
- *                  1 => disable backpressure
- *                  0 => enable backpressure
- *
- * @return 0 on success
- *         -1 on error
- */
-int cvmx_bgx_set_backpressure_override(uint32_t interface, uint32_t port_mask)
-{
-	cvmx_bgxx_cmr_rx_ovr_bp_t rx_ovr_bp;
-
-	/* Check for valid arguments */
-	rx_ovr_bp.u64 = 0;
-	rx_ovr_bp.s.en = port_mask;	/* Per port Enable back pressure override */
-	rx_ovr_bp.s.ign_fifo_bp = port_mask;	/* Ignore the RX FIFO full when computing BP */
-	cvmx_write_csr(CVMX_BGXX_CMR_RX_OVR_BP(interface), rx_ovr_bp.u64);
-	return 0;
-}
-
-/**
  * Helper function for global packet IO shutdown
  */
-static int __cvmx_helper_shutdown_packet_io_global_cn78xx(int node)
+int cvmx_helper_shutdown_packet_io_global_cn78xx(int node)
 {
 	int num_interfaces = cvmx_helper_get_number_of_interfaces();
+	cvmx_wqe_t *work;
 	int interface;
 	int result = 0;
 
 	/* Shut down all interfaces and disable TX and RX on all ports */
 	for (interface = 0; interface < num_interfaces; interface++) {
-		switch (cvmx_helper_interface_get_mode(interface)) {
-		case CVMX_HELPER_INTERFACE_MODE_XAUI:
-		case CVMX_HELPER_INTERFACE_MODE_RXAUI:
-		case CVMX_HELPER_INTERFACE_MODE_XLAUI:
-		case CVMX_HELPER_INTERFACE_MODE_XFI:
-		case CVMX_HELPER_INTERFACE_MODE_10G_KR:
-		case CVMX_HELPER_INTERFACE_MODE_40G_KR4:
-		{
-			cvmx_bgxx_cmrx_config_t cmr_config;
-			int index;
-			int num_ports = cvmx_helper_ports_on_interface(interface);
-			if (num_ports > 4)
-				num_ports = 4;
-
-			cvmx_bgx_set_backpressure_override(interface, 0xf);
-			for (index = 0; index < num_ports; index++) {
-				if (!cvmx_helper_is_port_valid(interface, index))
-					continue;
-
-				/* Disable GMX before we make any changes. Remember the enable state */
-				cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
-				cmr_config.s.enable = 0;
-				cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
-
-				/* Clear all error interrupts before enabling the interface. */
-				if (cvmx_sysinfo_get()->board_type != CVMX_BOARD_TYPE_SIM) {
-				       cvmx_write_csr_node(node, CVMX_BGXX_SMUX_RX_INT(index, interface), ~0x0ull);
-                                       cvmx_write_csr_node(node, CVMX_BGXX_SMUX_TX_INT(index, interface), ~0x0ull);
-                                       cvmx_write_csr_node(node, CVMX_BGXX_SPUX_INT(index, interface), ~0x0ull);
-				}
+		int xiface = cvmx_helper_node_interface_to_xiface(node, interface);
+		int index;
+		int num_ports = cvmx_helper_ports_on_interface(xiface);
 
-				/* Wait for GMX RX to be idle */
- 				if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SMUX_CTRL(index, interface),
-							cvmx_bgxx_smux_ctrl_t, rx_idle, ==, 1, 10000))
-					return -1;
+		if (num_ports > 4)
+			num_ports = 4;
 
-				/* Wait for GMX TX to be idle */
-				if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_SMUX_CTRL(index, interface),
-							cvmx_bgxx_smux_ctrl_t, tx_idle, ==, 1, 10000))
-					return -1;
+		cvmx_bgx_set_backpressure_override(xiface, (1<<num_ports)-1);
+		for (index = 0; index < num_ports; index++) {
+			if (!cvmx_helper_is_port_valid(xiface, index))
+				continue;
 
-			}
-			break;
+			cvmx_helper_bgx_shutdown_port(xiface, index);
 		}
-		case CVMX_HELPER_INTERFACE_MODE_SGMII:
-		{
-			cvmx_bgxx_cmrx_config_t cmr_config;
-			int index;
-			int num_ports = cvmx_helper_ports_on_interface(interface);
-			if (num_ports > 4)
-				num_ports = 4;
+	}
 
-			cvmx_bgx_set_backpressure_override(interface, 0xf);
-			for (index = 0; index < num_ports; index++) {
-				if (!cvmx_helper_is_port_valid(interface, index))
-					continue;
-				/* Disable GMX before we make any changes. Remember the enable state */
-				cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface));
-				cmr_config.s.enable = 0;
-                        	cvmx_write_csr_node(node, CVMX_BGXX_CMRX_CONFIG(index, interface), cmr_config.u64);
-
-
-				/* Wait for GMX to be idle */
-				if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, interface),
-				  		cvmx_bgxx_gmp_gmi_prtx_cfg_t, rx_idle, ==, 1, 10000) ||
-				    CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, interface),
-						cvmx_bgxx_gmp_gmi_prtx_cfg_t, tx_idle, ==, 1, 10000)) {
-					cvmx_dprintf("SGMII%d: Timeout waiting for port %d to be idle\n",
-						     interface, index);
-					return -1;
-				}
+	/* Stop input first */
+	cvmx_helper_pki_shutdown(node);
 
-				/* Read GMX CFG again to make sure the disable completed */
-				cvmx_read_csr_node(node, CVMX_BGXX_GMP_GMI_PRTX_CFG(index, interface));
-			}
-			break;
-		}
-		default:
-			break;
-		}
+	/* Retrieve all packets from the SSO and free them */
+	result = 0;
+	while ((work = cvmx_pow_work_request_sync(CVMX_POW_WAIT))) {
+            cvmx_wqe_pki_free(work);
+            cvmx_helper_free_pki_pkt_data(work);
+	    result ++;
 	}
 
-	cvmx_helper_pki_shutdown(node);
+	if (result > 0)
+		cvmx_dprintf("%s: Purged %d packets from SSO\n",
+			__func__, result);
+
+	/*
+	 * No need to wait for PKO queues to drain,
+	 * dq_close() drains the queues to NULL.
+	 */
 
 	/* Shutdown PKO interfaces */
 	for (interface = 0; interface < num_interfaces; interface++) {
@@ -1875,23 +2076,17 @@ static int __cvmx_helper_shutdown_packet_io_global_cn78xx(int node)
 		case CVMX_HELPER_INTERFACE_MODE_10G_KR:
 		case CVMX_HELPER_INTERFACE_MODE_40G_KR4:
 		case CVMX_HELPER_INTERFACE_MODE_SGMII:
+		case CVMX_HELPER_INTERFACE_MODE_MIXED:
 		{
-			cvmx_bgxx_cmr_rx_adrx_cam_t cmr_rx_adr;
 			int index;
 			int num_ports = cvmx_helper_ports_on_interface(xiface);
-			if (num_ports > 4)
-				num_ports = 4;
 
 			for (index = 0; index < num_ports; index++) {
 				if (!cvmx_helper_is_port_valid(xiface, index))
 					continue;
-				cmr_rx_adr.u64 = 0;
-				cmr_rx_adr.s.id = index;
-				cmr_rx_adr.s.en = 0;
-				cvmx_write_csr_node(node, CVMX_BGXX_CMR_RX_ADRX_CAM(index * 8, interface),
-					       cmr_rx_adr.u64);
-				/* Disable multicast and broadcast packets */
-				cvmx_write_csr_node(node, CVMX_BGXX_CMRX_RX_ADR_CTL(index, interface), 0);
+
+				/* Reset MAC filtering */
+				cvmx_helper_bgx_rx_adr_ctl(node, interface, index, 0, 0, 0);
 			}
 			break;
 		}
@@ -1938,8 +2133,9 @@ int cvmx_helper_shutdown_packet_io_global(void)
 	int node = cvmx_get_node_num();
 	cvmx_pcsx_mrx_control_reg_t control_reg;
 
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
-		return __cvmx_helper_shutdown_packet_io_global_cn78xx(node);
+	if (octeon_has_feature(OCTEON_FEATURE_BGX)) {
+		return cvmx_helper_shutdown_packet_io_global_cn78xx(node);
+	}
 
 	/* Step 1: Disable all backpressure */
 	for (interface = 0; interface < num_interfaces; interface++) {
@@ -1960,6 +2156,7 @@ step2:
 
 	/* Step 3: Disable TX and RX on all ports */
 	for (interface = 0; interface < num_interfaces; interface++) {
+		int xiface = cvmx_helper_node_interface_to_xiface(node, interface);
 		switch (cvmx_helper_interface_get_mode(interface)) {
 		case CVMX_HELPER_INTERFACE_MODE_DISABLED:
 		case CVMX_HELPER_INTERFACE_MODE_PCIE:
@@ -1993,7 +2190,7 @@ step2:
 		case CVMX_HELPER_INTERFACE_MODE_RGMII:
 			/* Disable outermost RX at the ASX block */
 			cvmx_write_csr(CVMX_ASXX_RX_PRT_EN(interface), 0);
-			num_ports = cvmx_helper_ports_on_interface(interface);
+			num_ports = cvmx_helper_ports_on_interface(xiface);
 			if (num_ports > 4)
 				num_ports = 4;
 			for (index = 0; index < num_ports; index++) {
@@ -2027,7 +2224,7 @@ step2:
 		case CVMX_HELPER_INTERFACE_MODE_SGMII:
 		case CVMX_HELPER_INTERFACE_MODE_QSGMII:
 		case CVMX_HELPER_INTERFACE_MODE_PICMG:
-			num_ports = cvmx_helper_ports_on_interface(interface);
+			num_ports = cvmx_helper_ports_on_interface(xiface);
 			if (num_ports > 4)
 				num_ports = 4;
 			for (index = 0; index < num_ports; index++) {
@@ -2173,6 +2370,7 @@ step2:
 
 	/* Step 8: Disable MAC address filtering */
 	for (interface = 0; interface < num_interfaces; interface++) {
+		int xiface = cvmx_helper_node_interface_to_xiface(node, interface);
 		switch (cvmx_helper_interface_get_mode(interface)) {
 		case CVMX_HELPER_INTERFACE_MODE_DISABLED:
 		case CVMX_HELPER_INTERFACE_MODE_PCIE:
@@ -2189,7 +2387,7 @@ step2:
 		case CVMX_HELPER_INTERFACE_MODE_SGMII:
 		case CVMX_HELPER_INTERFACE_MODE_QSGMII:
 		case CVMX_HELPER_INTERFACE_MODE_PICMG:
-			num_ports = cvmx_helper_ports_on_interface(interface);
+			num_ports = cvmx_helper_ports_on_interface(xiface);
 			if (num_ports > 4)
 				num_ports = 4;
 			for (index = 0; index < num_ports; index++) {
@@ -2334,37 +2532,39 @@ int cvmx_helper_shutdown_packet_io_local(void)
  * function basically does the equivalent of:
  * cvmx_helper_link_set(ipd_port, cvmx_helper_link_get(ipd_port));
  *
- * @param ipd_port IPD/PKO port to auto configure
+ * @param xipd_port IPD/PKO port to auto configure
  *
  * @return Link state after configure
  */
-cvmx_helper_link_info_t cvmx_helper_link_autoconf(int ipd_port)
+cvmx_helper_link_info_t cvmx_helper_link_autoconf(int xipd_port)
 {
 	cvmx_helper_link_info_t link_info;
-	int interface = cvmx_helper_get_interface_num(ipd_port);
-	int index = cvmx_helper_get_interface_index_num(ipd_port);
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
+	int index = cvmx_helper_get_interface_index_num(xipd_port);
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	int interface = xi.interface;
 
 	if (interface == -1 || index == -1 ||
-	    index >= cvmx_helper_ports_on_interface(interface)) {
+	    index >= cvmx_helper_ports_on_interface(xiface)) {
 		link_info.u64 = 0;
 		return link_info;
 	}
 
-	link_info = cvmx_helper_link_get(ipd_port);
-	if (link_info.u64 == (__cvmx_helper_get_link_info(interface, index)).u64)
+	link_info = cvmx_helper_link_get(xipd_port);
+	if (link_info.u64 == (__cvmx_helper_get_link_info(xiface, index)).u64)
 		return link_info;
 
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 	if (!link_info.s.link_up)
-		cvmx_error_disable_group(CVMX_ERROR_GROUP_ETHERNET, ipd_port);
+		cvmx_error_disable_group(CVMX_ERROR_GROUP_ETHERNET, xipd_port);
 #endif
 
 	/* If we fail to set the link speed, port_link_info will not change */
-	cvmx_helper_link_set(ipd_port, link_info);
+	cvmx_helper_link_set(xipd_port, link_info);
 
 #ifndef CVMX_BUILD_FOR_LINUX_KERNEL
 	if (link_info.s.link_up)
-		cvmx_error_enable_group(CVMX_ERROR_GROUP_ETHERNET, ipd_port);
+		cvmx_error_enable_group(CVMX_ERROR_GROUP_ETHERNET, xipd_port);
 #endif
 
 	return link_info;
@@ -2377,15 +2577,15 @@ EXPORT_SYMBOL(cvmx_helper_link_autoconf);
  * Octeon's link config if auto negotiation has changed since
  * the last call to cvmx_helper_link_set().
  *
- * @param ipd_port IPD/PKO port to query
+ * @param xipd_port IPD/PKO port to query
  *
  * @return Link state
  */
-cvmx_helper_link_info_t cvmx_helper_link_get(int ipd_port)
+cvmx_helper_link_info_t cvmx_helper_link_get(int xipd_port)
 {
 	cvmx_helper_link_info_t result;
-	int xiface = cvmx_helper_get_interface_num(ipd_port);
-	int index = cvmx_helper_get_interface_index_num(ipd_port);
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
+	int index = cvmx_helper_get_interface_index_num(xipd_port);
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
 
 	/*
@@ -2398,9 +2598,8 @@ cvmx_helper_link_info_t cvmx_helper_link_get(int ipd_port)
 	    index >= cvmx_helper_ports_on_interface(xiface))
 		return result;
 
-	cvmx_helper_interface_get_mode(xiface);
 	if (iface_node_ops[xi.node][xi.interface]->link_get)
-		result = iface_node_ops[xi.node][xi.interface]->link_get(ipd_port);
+		result = iface_node_ops[xi.node][xi.interface]->link_get(xipd_port);
 
 	return result;
 }
@@ -2413,25 +2612,24 @@ EXPORT_SYMBOL(cvmx_helper_link_get);
  * by cvmx_helper_link_get(). It is normally best to use
  * cvmx_helper_link_autoconf() instead.
  *
- * @param ipd_port  IPD/PKO port to configure
+ * @param xipd_port  IPD/PKO port to configure
  * @param link_info The new link state
  *
  * @return Zero on success, negative on failure
  */
-int cvmx_helper_link_set(int ipd_port, cvmx_helper_link_info_t link_info)
+int cvmx_helper_link_set(int xipd_port, cvmx_helper_link_info_t link_info)
 {
 	int result = -1;
-	int xiface = cvmx_helper_get_interface_num(ipd_port);
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	int index = cvmx_helper_get_interface_index_num(ipd_port);
+	int index = cvmx_helper_get_interface_index_num(xipd_port);
 
 	if (__cvmx_helper_xiface_is_null(xiface) || index == -1 ||
 	    index >= cvmx_helper_ports_on_interface(xiface))
 		return -1;
 
-	cvmx_helper_interface_get_mode(xiface);
 	if (iface_node_ops[xi.node][xi.interface]->link_set)
-		result = iface_node_ops[xi.node][xi.interface]->link_set(ipd_port, link_info);
+		result = iface_node_ops[xi.node][xi.interface]->link_set(xipd_port, link_info);
 
 	/*
 	 * Set the port_link_info here so that the link status is
@@ -2449,7 +2647,7 @@ EXPORT_SYMBOL(cvmx_helper_link_set);
  * causes packets sent by the port to be received by Octeon. External loopback
  * causes packets received from the wire to sent out again.
  *
- * @param ipd_port IPD/PKO port to loopback.
+ * @param xipd_port IPD/PKO port to loopback.
  * @param enable_internal
  *                 Non zero if you want internal loopback
  * @param enable_external
@@ -2457,20 +2655,20 @@ EXPORT_SYMBOL(cvmx_helper_link_set);
  *
  * @return Zero on success, negative on failure.
  */
-int cvmx_helper_configure_loopback(int ipd_port, int enable_internal,
+int cvmx_helper_configure_loopback(int xipd_port, int enable_internal,
 				   int enable_external)
 {
 	int result = -1;
-	int xiface = cvmx_helper_get_interface_num(ipd_port);
+	int xiface = cvmx_helper_get_interface_num(xipd_port);
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	int index = cvmx_helper_get_interface_index_num(ipd_port);
+	int index = cvmx_helper_get_interface_index_num(xipd_port);
 
 	if (index >= cvmx_helper_ports_on_interface(xiface))
 		return -1;
 
 	cvmx_helper_interface_get_mode(xiface);
 	if (iface_node_ops[xi.node][xi.interface]->loopback)
-		result = iface_node_ops[xi.node][xi.interface]->loopback(ipd_port,
+		result = iface_node_ops[xi.node][xi.interface]->loopback(xipd_port,
 									 enable_internal,
 									 enable_external);
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-ila.c b/arch/mips/cavium-octeon/executive/cvmx-ila.c
new file mode 100644
index 0000000..cab3ede
--- /dev/null
+++ b/arch/mips/cavium-octeon/executive/cvmx-ila.c
@@ -0,0 +1,295 @@
+/***********************license start***************
+ * Copyright (c) 2014  Cavium Inc. (support@cavium.com). All rights
+ * reserved.
+ *
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are
+ * met:
+ *
+ *   * Redistributions of source code must retain the above copyright
+ *     notice, this list of conditions and the following disclaimer.
+ *
+ *   * Redistributions in binary form must reproduce the above
+ *     copyright notice, this list of conditions and the following
+ *     disclaimer in the documentation and/or other materials provided
+ *     with the distribution.
+
+ *   * Neither the name of Cavium Inc. nor the names of
+ *     its contributors may be used to endorse or promote products
+ *     derived from this software without specific prior written
+ *     permission.
+
+ * This Software, including technical data, may be subject to U.S. export  control
+ * laws, including the U.S. Export Administration Act and its  associated
+ * regulations, and may be subject to export or import  regulations in other
+ * countries.
+
+ * TO THE MAXIMUM EXTENT PERMITTED BY LAW, THE SOFTWARE IS PROVIDED "AS IS"
+ * AND WITH ALL FAULTS AND CAVIUM INC. MAKES NO PROMISES, REPRESENTATIONS OR
+ * WARRANTIES, EITHER EXPRESS, IMPLIED, STATUTORY, OR OTHERWISE, WITH RESPECT TO
+ * THE SOFTWARE, INCLUDING ITS CONDITION, ITS CONFORMITY TO ANY REPRESENTATION OR
+ * DESCRIPTION, OR THE EXISTENCE OF ANY LATENT OR PATENT DEFECTS, AND CAVIUM
+ * SPECIFICALLY DISCLAIMS ALL IMPLIED (IF ANY) WARRANTIES OF TITLE,
+ * MERCHANTABILITY, NONINFRINGEMENT, FITNESS FOR A PARTICULAR PURPOSE, LACK OF
+ * VIRUSES, ACCURACY OR COMPLETENESS, QUIET ENJOYMENT, QUIET POSSESSION OR
+ * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
+ * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
+ ***********************license end**************************************/
+
+/**
+ * @file
+ *
+ * Functions to configure the ILK-LA interface.
+ *
+ * <hr>$Id$<hr>
+ */
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+#include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-helper.h>
+#include <asm/octeon/cvmx-ila.h>
+#include <asm/octeon/cvmx-ila-defs.h>
+#include <asm/octeon/cvmx-qlm.h>
+#else
+#include "cvmx.h"
+#include "cvmx-helper.h"
+#include "cvmx-ila.h"
+#include "cvmx-qlm.h"
+#endif
+
+CVMX_SHARED int cvmx_ila_chans = 2;
+
+/**
+ * @INTERNAL
+ * Return the link state of an IPD/PKO port as returned by ILK-LA link status.
+ *
+ * @param lane_mask lane_mask
+ *
+ * @return Link state
+ */
+static cvmx_helper_link_info_t __cvmx_ila_link_get(int lane_mask)
+{
+	cvmx_helper_link_info_t result;
+	int node = cvmx_get_node_num();
+	int retry_count = 0;
+	cvmx_ila_rxx_cfg1_t rx_cfg1;
+	cvmx_ila_rxx_int_t rx_int;
+	int i;
+
+	result.u64 = 0;
+
+retry:
+	retry_count++;
+	if (retry_count > 10)
+		goto fail;
+
+	/* Read RX config and status bits */
+	rx_cfg1.u64 = cvmx_read_csr_node(node, CVMX_ILA_RXX_CFG1(0));
+	rx_int.u64 = cvmx_read_csr_node(node, CVMX_ILA_RXX_INT(0));
+
+	if (rx_cfg1.s.rx_bdry_lock_ena == 0) {
+		/* Clear the boundary lock status bit */
+		rx_int.u64 = 0;
+		rx_int.s.word_sync_done = 1;
+		cvmx_write_csr_node(node, CVMX_ILA_RXX_INT(0), rx_int.u64);
+
+		/* We need to start looking for work boundary lock */
+		rx_cfg1.s.rx_bdry_lock_ena = lane_mask;
+		rx_cfg1.s.rx_align_ena = 0;
+		cvmx_write_csr_node(node, CVMX_ILA_RXX_CFG1(0), rx_cfg1.u64);
+		//cvmx_dprintf("ILK-LA: Looking for word boundary lock\n");
+
+		goto retry;
+	}
+
+	if (rx_cfg1.s.rx_align_ena == 0) {	
+		if (rx_int.s.word_sync_done) {
+			/* Clear the lane align status bits */
+			rx_int.u64 = 0;
+			rx_int.s.lane_align_fail = 1;
+			rx_int.s.lane_align_done = 1;
+			cvmx_write_csr_node(node, CVMX_ILA_RXX_INT(0), rx_int.u64);
+
+			rx_cfg1.s.rx_align_ena = 1;
+			cvmx_write_csr_node(node, CVMX_ILA_RXX_CFG1(0), rx_cfg1.u64);
+			//cvmx_printf("ILK-LA: Looking for lane alignment\n");
+			goto retry;
+		}
+		goto fail;
+	}
+
+	if (rx_int.s.lane_align_fail) {	
+		rx_cfg1.s.rx_bdry_lock_ena = 0;
+		rx_cfg1.s.rx_align_ena = 0;
+		cvmx_write_csr_node(node, CVMX_ILA_RXX_CFG1(0), rx_cfg1.u64);
+		//cvmx_dprintf("ILK-LA: Lane alignment failed\n");
+		goto fail;
+	}
+
+	if (rx_cfg1.s.pkt_ena == 0 && rx_int.s.lane_align_done) {
+		cvmx_ila_txx_cfg1_t tx_cfg1;
+
+		rx_cfg1.u64 = cvmx_read_csr_node(node, CVMX_ILA_RXX_CFG1(0));
+		tx_cfg1.u64 = cvmx_read_csr_node(node, CVMX_ILA_TXX_CFG1(0));
+
+		rx_cfg1.s.pkt_ena = tx_cfg1.s.pkt_ena = 1;
+		cvmx_write_csr_node(node, CVMX_ILA_RXX_CFG1(0), rx_cfg1.u64);
+		cvmx_write_csr_node(node, CVMX_ILA_TXX_CFG1(0), tx_cfg1.u64);
+
+		/* Clear and enable error interrupts */
+		for (i = 0; i < 8; i++) {
+			if ((1 << i) & lane_mask) {
+				/* Clear pending interrupts */
+				cvmx_write_csr_node(node, CVMX_ILA_RX_LNEX_INT(i), 0x3ff);
+				/* Enable bad_64b67b, bdry_sync_loss, crc32_err, dskew_fifo_ovfl,
+ *                                    scrm_sync_loss, serdes_lock_loss, stat_msg, ukwn_cntl_word */
+			}
+		}
+
+		//cvmx_dprintf("ILK-LA: Lane alignment complete\n");
+	}
+
+	result.u64 = 0;
+	result.s.link_up = 1;
+	result.s.full_duplex = 1;
+	result.s.speed = cvmx_qlm_get_gbaud_mhz(2) * 64 / 67;
+	result.s.speed *= cvmx_pop(lane_mask);
+
+	return result;
+
+fail:
+	if (rx_cfg1.s.pkt_ena) {
+		/* Disable the interface */
+		rx_cfg1.s.pkt_ena = 0;
+		cvmx_write_csr_node(node, CVMX_ILA_RXX_CFG1(0), rx_cfg1.u64);
+
+		/* Disable error interrupts */
+	}
+
+	return result;
+}
+
+/**
+ * Initialize ILK-LA interface
+ *
+ * @param lane_mask  Lanes to initialize ILK-LA interface.
+ * @return  0 on success and -1 on failure.
+ */
+int cvmx_ila_initialize(int lane_mask)
+{
+	cvmx_ila_rxx_cfg0_t rx_cfg0;
+	cvmx_ila_txx_cfg0_t tx_cfg0;
+	cvmx_ila_rxx_cfg1_t rx_cfg1;
+	cvmx_ila_txx_cfg1_t tx_cfg1;
+	cvmx_ila_txx_cha_xon_t tx_cha_xon;
+	cvmx_ila_ser_cfg_t ser_cfg;
+	int node = cvmx_get_node_num();
+	int lane0 = 0, lane1 = 0;
+	cvmx_helper_link_info_t result;
+	int retry_count = 0;
+
+	ser_cfg.u64 = cvmx_read_csr_node(node, CVMX_ILA_SER_CFG);
+	ser_cfg.s.ser_rxpol_auto = 1;
+	cvmx_write_csr_node(node, CVMX_ILA_SER_CFG, ser_cfg.u64);
+
+	if (cvmx_qlm_get_mode_cn78xx(node, 2) == CVMX_QLM_MODE_ILK)
+		lane0 = 0xf;
+
+	if (cvmx_qlm_get_mode_cn78xx(node, 3) == CVMX_QLM_MODE_ILK)
+		lane1 = 0xf;
+
+	if ((lane_mask & 0xf) != lane0) {
+		cvmx_dprintf("ERROR: Invalid configuration for QLM2\n");
+		return -1;
+	}
+	if (((lane_mask >> 4) & 0xf) == lane1) {
+		cvmx_dprintf("ERROR: Invalid configuration for QLM3\n");
+		return -1;
+	}
+
+	/* Enable RX lanes */
+	rx_cfg0.u64 = cvmx_read_csr_node(node, CVMX_ILA_RXX_CFG0(0));
+	rx_cfg0.s.lane_ena = lane_mask;
+	cvmx_write_csr_node(node, CVMX_ILA_RXX_CFG0(0), rx_cfg0.u64);
+
+	/* Enable TX lanes */
+	tx_cfg0.u64 = cvmx_read_csr_node(node, CVMX_ILA_TXX_CFG0(0));
+	tx_cfg0.s.lane_ena = lane_mask;
+	cvmx_write_csr_node(node, CVMX_ILA_TXX_CFG0(0), tx_cfg0.u64);
+
+	/* Set XON/XOFF state to XON */
+	tx_cha_xon.u64 = cvmx_read_csr_node(node, CVMX_ILA_TXX_CHA_XON(0));
+	tx_cha_xon.s.ch0_xon = 1;
+	tx_cha_xon.s.ch1_xon = 1;
+	cvmx_write_csr_node(node, CVMX_ILA_TXX_CHA_XON(0), tx_cha_xon.u64);
+
+	/* Enable TX packet reception. */
+	tx_cfg1.u64 = cvmx_read_csr_node(node, CVMX_ILA_TXX_CFG1(0));
+	tx_cfg1.s.pkt_ena = 1;
+	cvmx_write_csr_node(node, CVMX_ILA_TXX_CFG1(0), tx_cfg1.u64);	
+
+	result.u64 = 0;
+
+retry:
+	retry_count++;
+	if (retry_count > 10)
+		goto out;
+
+	/* Make sure the link is up, so that packets can be sent */
+	result = __cvmx_ila_link_get(lane_mask);
+
+	/* Small delay before another retry */
+	cvmx_wait_usec(100);
+
+	rx_cfg1.u64 = cvmx_read_csr_node(node, CVMX_ILA_RXX_CFG1(0));
+	if (rx_cfg1.s.pkt_ena == 0)
+		goto retry;
+
+out:
+	if (result.s.link_up)
+		return 0;
+
+	return -1;
+}
+
+int cvmx_ila_disable(void)
+{
+	int node = cvmx_get_node_num();
+	cvmx_ila_rxx_cfg0_t rx_cfg0;
+	cvmx_ila_txx_cfg0_t tx_cfg0;
+
+	/* Disable RX lanes */
+	rx_cfg0.u64 = cvmx_read_csr_node(node, CVMX_ILA_RXX_CFG0(0));
+	rx_cfg0.s.lane_ena = 0;
+	cvmx_write_csr_node(node, CVMX_ILA_RXX_CFG0(0), rx_cfg0.u64);
+
+	/* Disable TX lanes */
+	tx_cfg0.u64 = cvmx_read_csr_node(node, CVMX_ILA_TXX_CFG0(0));
+	tx_cfg0.s.lane_ena = 0;
+	cvmx_write_csr_node(node, CVMX_ILA_TXX_CFG0(0), tx_cfg0.u64);
+
+	return 0;
+}
+
+/**
+ * Enable or disable LA mode in ILK header.
+ *
+ * @param channel channel
+ * @param mode   If set, enable LA mode in ILK header, else disable
+ *
+ * @return ILK header
+ */
+cvmx_ila_header_t cvmx_ila_configure_header(int channel, int mode)
+{
+	cvmx_ila_header_t ila_header;
+
+	ila_header.u64 = 0;
+
+	if (mode) {
+		ila_header.s.la_mode = mode;
+		ila_header.s.ilk_channel = channel;
+	}
+
+	return ila_header;
+}
+			
diff --git a/arch/mips/cavium-octeon/executive/cvmx-ilk.c b/arch/mips/cavium-octeon/executive/cvmx-ilk.c
index 31ee940..7e2952c 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-ilk.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-ilk.c
@@ -221,7 +221,7 @@ int cvmx_ilk_start_interface(int interface, unsigned short lane_mask)
 			/* Make sure QLM is in ILK mode */
 			gserx_cfg.u64 = cvmx_read_csr_node(node, CVMX_GSERX_CFG(qlm));
 			if (gserx_cfg.s.ila)
-				lane_mask_all |= ((1 << 4) - 1) << (4 * (qlm - 4));	
+				lane_mask_all |= ((1 << 4) - 1) << (4 * (qlm - 4));
 		}
 
 		if ((lane_mask_all & lane_mask) != lane_mask) {
@@ -297,7 +297,7 @@ int cvmx_ilk_start_interface(int interface, unsigned short lane_mask)
 		if (cvmx_ilk_use_la_mode(interface, 0)) {
 			cvmx_ilk_txx_cfg1_t ilk_txx_cfg1;
 			cvmx_ilk_rxx_cfg1_t ilk_rxx_cfg1;
-	
+
 			ilk_txx_cfg1.u64 = cvmx_read_csr(CVMX_ILK_TXX_CFG1(interface));
 			ilk_rxx_cfg1.u64 = cvmx_read_csr(CVMX_ILK_RXX_CFG1(interface));
 			ilk_txx_cfg1.s.la_mode = 1;
@@ -321,6 +321,21 @@ int cvmx_ilk_start_interface(int interface, unsigned short lane_mask)
 	cvmx_write_csr_node(node, CVMX_ILK_TXX_CFG0(interface), ilk_txx_cfg0.u64);
 	cvmx_write_csr_node(node, CVMX_ILK_RXX_CFG0(interface), ilk_rxx_cfg0.u64);
 
+	/* For 10.3125Gbs data rate, set SER_LIMIT to 0x3ff for x8 & x12 mode */
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		cvmx_gserx_lane_mode_t lmode0, lmode1;
+		lmode0.u64 = cvmx_read_csr_node(node, CVMX_GSERX_LANE_MODE(5));
+		lmode1.u64 = cvmx_read_csr_node(node, CVMX_GSERX_LANE_MODE(7));
+		if ((lmode0.s.lmode == 5 || lmode1.s.lmode == 5)
+		     && (lane_mask == 0xfff || lane_mask == 0xfff0
+			 || lane_mask == 0xff || lane_mask == 0xff00)) {
+			cvmx_ilk_txx_cfg1_t ilk_txx_cfg1;
+			ilk_txx_cfg1.u64 = cvmx_read_csr_node(node, CVMX_ILK_TXX_CFG1(interface));
+			ilk_txx_cfg1.s.ser_limit = 0x3ff;
+			cvmx_write_csr_node(node, CVMX_ILK_TXX_CFG1(interface), ilk_txx_cfg1.u64);
+		}
+	}
+
 	/* write to local cache. for lane speed, if interface 0 has 8 lanes,
 	 * assume both qlms have the same speed */
 	cvmx_ilk_intf_cfg[node][interface].intf_en = 1;
@@ -490,7 +505,7 @@ int cvmx_ilk_rx_cal_conf(int intf, int cal_depth, cvmx_ilk_cal_entry_t * pent)
 	int res = -1, i;
 	cvmx_ilk_rxx_cfg0_t ilk_rxx_cfg0;
 	int num_entries;
-	int node = (intf >> 4) & 0xf; 
+	int node = (intf >> 4) & 0xf;
 	int interface = intf & 0xf;
 
 	if (!octeon_has_feature(OCTEON_FEATURE_ILK))
@@ -513,7 +528,7 @@ int cvmx_ilk_rx_cal_conf(int intf, int cal_depth, cvmx_ilk_cal_entry_t * pent)
 	if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
 		/* Update the calendar for each channel */
 		if ((cvmx_ilk_use_la_mode(interface, 0) == 0) ||
-		    (cvmx_ilk_use_la_mode(interface, 0) && 
+		    (cvmx_ilk_use_la_mode(interface, 0) &&
 		     cvmx_ilk_la_mode_enable_rx_calendar(interface))) {
 			for (i = 0; i < cal_depth; i++) {
 				__cvmx_ilk_write_rx_cal_entry(interface, i,
@@ -535,7 +550,7 @@ int cvmx_ilk_rx_cal_conf(int intf, int cal_depth, cvmx_ilk_cal_entry_t * pent)
 
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
 		ilk_rxx_cfg0.u64 = cvmx_read_csr_node(node, CVMX_ILK_RXX_CFG0(interface));
-		/* 
+		/*
 		 * Make sure cal_ena is 0 for programming the calender table,
 		 * as per Errata ILK-19398
 		 */
@@ -667,7 +682,7 @@ EXPORT_SYMBOL(cvmx_ilk_cal_setup_rx);
 /**
  * configure calendar for tx
  *
- * @param interface The identifier of the packet interface to configure and
+ * @param intf      The identifier of the packet interface to configure and
  *                  use as a ILK interface. cn68xx has 2 interfaces: ilk0 and
  *                  ilk1.
  *
@@ -717,7 +732,7 @@ int cvmx_ilk_tx_cal_conf(int intf, int cal_depth, cvmx_ilk_cal_entry_t * pent)
 
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
 		ilk_txx_cfg0.u64 = cvmx_read_csr_node(node, CVMX_ILK_TXX_CFG0(interface));
-		/* 
+		/*
 		 * Make sure cal_ena is 0 for programming the calender table,
 		 * as per Errata ILK-19398
 		 */
@@ -743,7 +758,7 @@ int cvmx_ilk_tx_cal_conf(int intf, int cal_depth, cvmx_ilk_cal_entry_t * pent)
 /**
  * enable calendar for tx
  *
- * @param interface The identifier of the packet interface to configure and
+ * @param intf	    The identifier of the packet interface to configure and
  *                  use as a ILK interface. cn68xx has 2 interfaces: ilk0 and
  *                  ilk1.
  *
@@ -804,7 +819,7 @@ int cvmx_ilk_cal_setup_tx(int intf, int cal_depth, cvmx_ilk_cal_entry_t * pent,
 
 EXPORT_SYMBOL(cvmx_ilk_cal_setup_tx);
 
-//#define CVMX_ILK_STATS_ENA 1
+/* #define CVMX_ILK_STATS_ENA 1 */
 #ifdef CVMX_ILK_STATS_ENA
 static void cvmx_ilk_reg_dump_rx(int intf)
 {
@@ -884,7 +899,7 @@ static void cvmx_ilk_reg_dump_rx(int intf)
 		for (i = 0; i < CAL_NUM_DBG; i++) {
 			ilk_rxx_idx_cal.u64 = cvmx_read_csr(CVMX_ILK_RXX_IDX_CAL(interface));
 			cvmx_dprintf("ilk rxx idx cal: 0x%16lx\n", ilk_rxx_idx_cal.u64);
-	
+
 			ilk_rxx_mem_cal0.u64 = cvmx_read_csr(CVMX_ILK_RXX_MEM_CAL0(interface));
 			cvmx_dprintf("ilk rxx mem cal0: 0x%16lx\n", ilk_rxx_mem_cal0.u64);
 			ilk_rxx_mem_cal1.u64 = cvmx_read_csr(CVMX_ILK_RXX_MEM_CAL1(interface));
@@ -948,7 +963,7 @@ static void cvmx_ilk_reg_dump_tx(int intf)
 		for (i = 0; i < CAL_NUM_DBG; i++) {
 			ilk_txx_idx_cal.u64 = cvmx_read_csr(CVMX_ILK_TXX_IDX_CAL(interface));
 			cvmx_dprintf("ilk txx idx cal: 0x%16lx\n", ilk_txx_idx_cal.u64);
-	
+
 			ilk_txx_mem_cal0.u64 = cvmx_read_csr(CVMX_ILK_TXX_MEM_CAL0(interface));
 			cvmx_dprintf("ilk txx mem cal0: 0x%16lx\n", ilk_txx_mem_cal0.u64);
 			ilk_txx_mem_cal1.u64 = cvmx_read_csr(CVMX_ILK_TXX_MEM_CAL1(interface));
@@ -1194,7 +1209,7 @@ int cvmx_ilk_disable(int intf)
 /**
  * Provide interface enable status
  *
- * @param interface The identifier of the packet interface to disable. cn68xx
+ * @param xiface The identifier of the packet xiface to disable. cn68xx
  *                  has 2 interfaces: ilk0 and ilk1.
  *
  * @return Zero, not enabled; One, enabled.
@@ -1241,7 +1256,7 @@ cvmx_ilk_la_nsp_compact_hdr_t cvmx_ilk_enable_la_header(int ipd_port, int mode)
 	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
 	int xiface = cvmx_helper_get_interface_num(ipd_port);
 	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
-	
+
 	int ilk_interface = xi.interface - CVMX_ILK_GBL_BASE();
 	int skip = 0;
 	int crc = 1;
@@ -1352,7 +1367,7 @@ void cvmx_ilk_show_stats(int interface, cvmx_ilk_stats_ctrl_t * pstats)
 				ilk_rxx_idx_stat0.s.clr = pstats->clr_on_rd;
 				cvmx_write_csr(CVMX_ILK_RXX_IDX_STAT0(interface), ilk_rxx_idx_stat0.u64);
 				ilk_rxx_mem_stat0.u64 = cvmx_read_csr(CVMX_ILK_RXX_MEM_STAT0(interface));
-	
+
 				/* get the number of rx bytes */
 				ilk_rxx_idx_stat1.u64 = 0;
 				ilk_rxx_idx_stat1.s.index = *chan_list;
@@ -1369,7 +1384,7 @@ void cvmx_ilk_show_stats(int interface, cvmx_ilk_stats_ctrl_t * pstats)
 				rxx_pkt_cntx.u64 = cvmx_read_csr_node(node, CVMX_ILK_RXX_PKT_CNTX(*chan_list, interface));
 				rxx_byte_cntx.u64 = cvmx_read_csr_node(node, CVMX_ILK_RXX_BYTE_CNTX(*chan_list, interface));
 				cvmx_dprintf("ILK%d Channel%d Rx: %llu packets %llu bytes\n", interface,
-					     *chan_list, 
+					     *chan_list,
 					     (unsigned long long)rxx_pkt_cntx.s.rx_pkt,
 					     (unsigned long long)rxx_byte_cntx.s.rx_bytes);
 			}
@@ -1381,14 +1396,14 @@ void cvmx_ilk_show_stats(int interface, cvmx_ilk_stats_ctrl_t * pstats)
 				ilk_txx_idx_stat0.s.clr = pstats->clr_on_rd;
 				cvmx_write_csr(CVMX_ILK_TXX_IDX_STAT0(interface), ilk_txx_idx_stat0.u64);
 				ilk_txx_mem_stat0.u64 = cvmx_read_csr(CVMX_ILK_TXX_MEM_STAT0(interface));
-	
+
 				/* get the number of tx bytes */
 				ilk_txx_idx_stat1.u64 = 0;
 				ilk_txx_idx_stat1.s.index = *pstats->chan_list;
 				ilk_txx_idx_stat1.s.clr = pstats->clr_on_rd;
 				cvmx_write_csr(CVMX_ILK_TXX_IDX_STAT1(interface), ilk_txx_idx_stat1.u64);
 				ilk_txx_mem_stat1.u64 = cvmx_read_csr(CVMX_ILK_TXX_MEM_STAT1(interface));
-	
+
 				cvmx_dprintf("ILK%d Channel%d Tx: %d packets %d bytes\n", interface,
 					     *chan_list, ilk_txx_mem_stat0.s.tx_pkt, (unsigned int)ilk_txx_mem_stat1.s.tx_bytes);
 			}
@@ -1416,30 +1431,30 @@ void cvmx_ilk_show_stats(int interface, cvmx_ilk_stats_ctrl_t * pstats)
 		ilk_rxx_idx_stat0.s.inc = pstats->chan_step;
 		ilk_rxx_idx_stat0.s.clr = pstats->clr_on_rd;
 		cvmx_write_csr(CVMX_ILK_RXX_IDX_STAT0(interface), ilk_rxx_idx_stat0.u64);
-	
+
 		ilk_rxx_idx_stat1.u64 = 0;
 		ilk_rxx_idx_stat1.s.index = pstats->chan_start;
 		ilk_rxx_idx_stat1.s.inc = pstats->chan_step;
 		ilk_rxx_idx_stat1.s.clr = pstats->clr_on_rd;
 		cvmx_write_csr(CVMX_ILK_RXX_IDX_STAT1(interface), ilk_rxx_idx_stat1.u64);
-	
+
 		ilk_txx_idx_stat0.u64 = 0;
 		ilk_txx_idx_stat0.s.index = pstats->chan_start;
 		ilk_txx_idx_stat0.s.inc = pstats->chan_step;
 		ilk_txx_idx_stat0.s.clr = pstats->clr_on_rd;
 		cvmx_write_csr(CVMX_ILK_TXX_IDX_STAT0(interface), ilk_txx_idx_stat0.u64);
-	
+
 		ilk_txx_idx_stat1.u64 = 0;
 		ilk_txx_idx_stat1.s.index = pstats->chan_start;
 		ilk_txx_idx_stat1.s.inc = pstats->chan_step;
 		ilk_txx_idx_stat1.s.clr = pstats->clr_on_rd;
 		cvmx_write_csr(CVMX_ILK_TXX_IDX_STAT1(interface), ilk_txx_idx_stat1.u64);
-	
+
 		for (i = pstats->chan_start; i <= pstats->chan_end; i += pstats->chan_step) {
 			ilk_rxx_mem_stat0.u64 = cvmx_read_csr(CVMX_ILK_RXX_MEM_STAT0(interface));
 			ilk_rxx_mem_stat1.u64 = cvmx_read_csr(CVMX_ILK_RXX_MEM_STAT1(interface));
 			cvmx_dprintf("ILK%d Channel%d Rx: %d packets %d bytes\n", interface, i, ilk_rxx_mem_stat0.s.rx_pkt, (unsigned int)ilk_rxx_mem_stat1.s.rx_bytes);
-	
+
 			ilk_txx_mem_stat0.u64 = cvmx_read_csr(CVMX_ILK_TXX_MEM_STAT0(interface));
 			ilk_txx_mem_stat1.u64 = cvmx_read_csr(CVMX_ILK_TXX_MEM_STAT1(interface));
 			cvmx_dprintf("ILK%d Channel%d Tx: %d packets %d bytes\n", interface, i, ilk_rxx_mem_stat0.s.rx_pkt, (unsigned int)ilk_rxx_mem_stat1.s.rx_bytes);
@@ -1472,7 +1487,7 @@ void cvmx_ilk_show_stats(int interface, cvmx_ilk_stats_ctrl_t * pstats)
 /**
  * enable or disable loopbacks
  *
- * @param interface The identifier of the packet interface to disable. cn68xx
+ * @param xiface The identifier of the packet xiface to disable. cn68xx
  *                  has 2 interfaces: ilk0 and ilk1.
  * @param enable    Enable or disable loopback
  * @param mode      Internal or external loopback
diff --git a/arch/mips/cavium-octeon/executive/cvmx-ipd.c b/arch/mips/cavium-octeon/executive/cvmx-ipd.c
index 7fe8e4b..a9f0cfb 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-ipd.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-ipd.c
@@ -50,7 +50,6 @@
 #include <asm/octeon/cvmx-pip-defs.h>
 #include <asm/octeon/cvmx-dbg-defs.h>
 #include <asm/octeon/cvmx-sso-defs.h>
-
 #include <asm/octeon/cvmx-fpa1.h>
 #include <asm/octeon/cvmx-wqe.h>
 #include <asm/octeon/cvmx-ipd.h>
@@ -129,8 +128,6 @@ void cvmx_ipd_convert_to_newcfg(cvmx_ipd_config_t ipd_config)
 					 ipd_config.packet_pool.pool_num, ipd_config.packet_pool.buffer_count);
 }
 
-
-
 int cvmx_ipd_set_config(cvmx_ipd_config_t ipd_config)
 {
 	cvmx_ipd_cfg = ipd_config;
@@ -152,15 +149,14 @@ void cvmx_ipd_set_packet_pool_buffer_count(uint64_t buffer_count)
 void cvmx_ipd_set_packet_pool_config(int64_t pool, uint64_t buffer_size,
 				     uint64_t buffer_count)
 {
+	cvmx_ipd_cfg.packet_pool.pool_num = pool;
+	cvmx_ipd_cfg.packet_pool.buffer_size = buffer_size;
+	cvmx_ipd_cfg.packet_pool.buffer_count = buffer_count;
 	if (octeon_has_feature(OCTEON_FEATURE_PKI)) {
 		int node = cvmx_get_node_num();
 		int64_t aura = pool;
 		cvmx_helper_pki_set_dflt_pool(node, pool, buffer_size, buffer_count);
 		cvmx_helper_pki_set_dflt_aura(node, aura, pool, buffer_count);
-	} else {
-		cvmx_ipd_cfg.packet_pool.pool_num = pool;
-		cvmx_ipd_cfg.packet_pool.buffer_size = buffer_size;
-		cvmx_ipd_cfg.packet_pool.buffer_count = buffer_count;
 	}
 }
 EXPORT_SYMBOL(cvmx_ipd_set_packet_pool_config);
@@ -437,6 +433,14 @@ void cvmx_ipd_config(uint64_t mbuff_size,
 	cvmx_ipd_wqe_fpa_queue_t wqe_pool;
 	cvmx_ipd_ctl_status_t ipd_ctl_reg;
 
+	/* Enforce 1st skip minimum if WQE shares the buffer with packet */
+	if (octeon_has_feature(OCTEON_FEATURE_NO_WPTR)) {
+		union cvmx_ipd_ctl_status ctl_status;
+		ctl_status.u64 = cvmx_read_csr(CVMX_IPD_CTL_STATUS);
+		if (ctl_status.s.no_wptr != 0 && first_mbuff_skip < 16)
+			first_mbuff_skip = 16;
+	}
+
 	first_skip.u64 = 0;
 	first_skip.s.skip_sz = first_mbuff_skip;
 	cvmx_write_csr(CVMX_IPD_1ST_MBUFF_SKIP, first_skip.u64);
diff --git a/arch/mips/cavium-octeon/executive/cvmx-l2c.c b/arch/mips/cavium-octeon/executive/cvmx-l2c.c
index 3b646ee..006e63b 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-l2c.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-l2c.c
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2003-2015  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -43,7 +43,7 @@
  * Implementation of the Level 2 Cache (L2C) control,
  * measurement, and debugging facilities.
  *
- * <hr>$Revision: 113624 $<hr>
+ * <hr>$Revision: 115982 $<hr>
  *
  */
 
@@ -196,7 +196,7 @@ int cvmx_l2c_set_hw_way_partition2(uint32_t mask)
 	uint32_t valid_mask;
 	int node = cvmx_get_node_num();
 
-	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX)))
+	if (CVMX_L2C_IOBS < 2)
 		return -1;
 
 	valid_mask = (0x1 << cvmx_l2c_get_num_assoc()) - 1;
@@ -208,10 +208,9 @@ int cvmx_l2c_set_hw_way_partition2(uint32_t mask)
 int cvmx_l2c_get_hw_way_partition2(void)
 {
 	int node = cvmx_get_node_num();
-	if (!(OCTEON_IS_MODEL(OCTEON_CN68XX) || OCTEON_IS_MODEL(OCTEON_CN78XX))) {
-		cvmx_warn("only one IOB on this chip");
+	if (CVMX_L2C_IOBS < 2)
 		return -1;
-	}
+
 	return cvmx_read_csr_node(node, CVMX_L2C_WPAR_IOBX(1)) & 0xffff;
 }
 
@@ -398,9 +397,13 @@ int cvmx_l2c_lock_line(uint64_t addr)
 				 && l2c_tadx_tag.cn70xx.valid
 				 && l2c_tadx_tag.cn70xx.tag == tag)
 				break;
+			else if ((OCTEON_IS_MODEL(OCTEON_CN73XX)
+				  || OCTEON_IS_MODEL(OCTEON_CNF75XX))
+				 && l2c_tadx_tag.cn73xx.tag == tag)
+				break;
 			else if (l2c_tadx_tag.cn78xx.ts != 0
 				 && l2c_tadx_tag.cn78xx.tag == tag)
-				break;
+			        break;
 
 			 /* cvmx_dprintf("caddr=%lx tad=%d tagu64=%lx valid=%x tag=%x \n", caddr,
 			   tad, l2c_tadx_tag.u64, l2c_tadx_tag.cn70xx.valid, l2c_tadx_tag.cn70xx.tag); */
@@ -740,8 +743,8 @@ static union __cvmx_l2c_tag __read_l2_tag(uint64_t assoc, uint64_t index)
 		      "sd    $0, 0(%[dbg_addr])\n\t"	/* Exit debug mode, wait for store */
 		      "ld    $0, 0(%[dbg_addr])\n\t" "cache 9, 0($0)\n\t"	/* Invalidate dcache to discard debug data */
 		      ".set pop":[tag_val] "=r"(tag_val)
-		      : [dbg_addr] "r"(dbg_addr), [dbg_val] "r"(debug_val), [tag_addr] "r"(debug_tag_addr)
-		      : "memory");
+		      :[dbg_addr] "r"(dbg_addr),[dbg_val] "r"(debug_val),[tag_addr] "r"(debug_tag_addr)
+		      :"memory");
 
 	cvmx_local_irq_restore(flags);
 
@@ -786,6 +789,14 @@ union cvmx_l2c_tag cvmx_l2c_get_tag_v2(uint32_t association, uint32_t index, uin
 			tag.s.L = l2c_tadx_tag.cn78xx.lock;
 			tag.s.U = l2c_tadx_tag.cn78xx.used;
 			tag.s.addr = l2c_tadx_tag.cn78xx.tag;
+		} else if (OCTEON_IS_MODEL(OCTEON_CN73XX)
+			   || OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+			if (l2c_tadx_tag.cn73xx.ts != 0)
+				tag.s.V = 1;
+			tag.s.D = l2c_tadx_tag.cn73xx.sblkdty;
+			tag.s.L = l2c_tadx_tag.cn73xx.lock;
+			tag.s.U = l2c_tadx_tag.cn73xx.used;
+			tag.s.addr = l2c_tadx_tag.cn73xx.tag;
 		} else {
 			tag.s.V = l2c_tadx_tag.cn61xx.valid;
 			tag.s.D = l2c_tadx_tag.cn61xx.dirty;
@@ -879,6 +890,14 @@ union cvmx_l2c_tag cvmx_l2c_get_tag(uint32_t association, uint32_t index)
 			tag.s.L = l2c_tadx_tag.cn78xx.lock;
 			tag.s.U = l2c_tadx_tag.cn78xx.used;
 			tag.s.addr = l2c_tadx_tag.cn78xx.tag;
+		} else if (OCTEON_IS_MODEL(OCTEON_CN73XX)
+			   || OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+			if (l2c_tadx_tag.cn73xx.ts == 0)
+				tag.s.V = 1;
+			tag.s.D = l2c_tadx_tag.cn73xx.sblkdty;
+			tag.s.L = l2c_tadx_tag.cn73xx.lock;
+			tag.s.U = l2c_tadx_tag.cn73xx.used;
+			tag.s.addr = l2c_tadx_tag.cn73xx.tag;
 		} else {
 			tag.s.V = l2c_tadx_tag.cn61xx.valid;
 			tag.s.D = l2c_tadx_tag.cn61xx.dirty;
@@ -938,7 +957,9 @@ union cvmx_l2c_tag cvmx_l2c_get_tag(uint32_t association, uint32_t index)
 int cvmx_l2c_address_to_tad(uint64_t addr)
 {
 	uint32_t tad;
-	if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+	if (OCTEON_IS_MODEL(OCTEON_CN68XX)
+	    || OCTEON_IS_MODEL(OCTEON_CN73XX)
+	    || OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
 		cvmx_l2c_ctl_t l2c_ctl;
 		l2c_ctl.u64 = cvmx_read_csr(CVMX_L2C_CTL);
 		if (!l2c_ctl.s.disidxalias) {
@@ -992,18 +1013,22 @@ uint32_t cvmx_l2c_address_to_index(uint64_t addr)
 	}
 
 	if (indxalias) {
-		if (OCTEON_IS_MODEL(OCTEON_CN68XX)) {
+		/* For 4 TADs */
+		if (OCTEON_IS_MODEL(OCTEON_CN68XX)
+		    || OCTEON_IS_MODEL(OCTEON_CN73XX)
+		    || OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
 			uint32_t a_14_12 = (idx / (CVMX_L2C_MEMBANK_SELECT_SIZE / (1 << CVMX_L2C_IDX_ADDR_SHIFT))) & 0x7;
 			idx ^= (idx / cvmx_l2c_get_num_sets()) & 0x3ff;
 			idx ^= a_14_12 & 0x3;
 			idx ^= a_14_12 << 2;
+		/* For 8 TADs */
 		} else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
 			uint32_t a_14_12 = (idx / (CVMX_L2C_MEMBANK_SELECT_SIZE / (1 << CVMX_L2C_IDX_ADDR_SHIFT))) & 0x7;
-			uint64_t above_normal_index = (idx / cvmx_l2c_get_num_sets()) & 0xff;	/* A<27:20> */
-			idx ^= above_normal_index;		  /* XOR in A<27:20> */
-			idx ^= (above_normal_index & 0x1F) << 8;  /* XOR in A<24:20> */
-			idx ^= a_14_12;				  /* XOR in A<14:12> */
-			idx ^= (a_14_12 & 0x3) << 3;		  /* XOR in A<13:12> */
+			uint64_t above_normal_index = (idx / cvmx_l2c_get_num_sets()) & 0xff;	// A<27:20>
+			idx ^= above_normal_index;		  // XOR in A<27:20>
+			idx ^= (above_normal_index & 0x1F) << 8;  // XOR in A<24:20>
+			idx ^= a_14_12;				  // XOR in A<14:12>
+			idx ^= (a_14_12 & 0x3) << 3;		  // XOR in A<13:12>
 		} else if (OCTEON_IS_OCTEON2()
 			   || OCTEON_IS_MODEL(OCTEON_CN70XX)) {
 			uint32_t a_14_12 = (idx / (CVMX_L2C_MEMBANK_SELECT_SIZE / (1 << CVMX_L2C_IDX_ADDR_SHIFT))) & 0x7;
@@ -1033,7 +1058,9 @@ int cvmx_l2c_get_set_bits(void)
 		l2_set_bits = 13;	/* 8192 sets */
 	else if (OCTEON_IS_MODEL(OCTEON_CN56XX)
 		 || OCTEON_IS_MODEL(OCTEON_CN58XX)
-		 || OCTEON_IS_MODEL(OCTEON_CN68XX))
+		 || OCTEON_IS_MODEL(OCTEON_CN68XX)
+		 || OCTEON_IS_MODEL(OCTEON_CN73XX)
+		 || OCTEON_IS_MODEL(OCTEON_CNF75XX))
 		l2_set_bits = 11;	/* 2048 sets */
 	else if (OCTEON_IS_MODEL(OCTEON_CN38XX)
 		 || OCTEON_IS_MODEL(OCTEON_CN63XX)
@@ -1073,7 +1100,10 @@ int cvmx_l2c_get_num_assoc(void)
 		|| OCTEON_IS_MODEL(OCTEON_CN50XX)
 		|| OCTEON_IS_MODEL(OCTEON_CN38XX))
 		l2_assoc = 8;
-	else if (OCTEON_IS_OCTEON2() || OCTEON_IS_MODEL(OCTEON_CN78XX))
+	else if (OCTEON_IS_OCTEON2()
+		 || OCTEON_IS_MODEL(OCTEON_CN78XX)
+		 || OCTEON_IS_MODEL(OCTEON_CN73XX)
+		 || OCTEON_IS_MODEL(OCTEON_CNF75XX))
 		l2_assoc = 16;
 	else if (OCTEON_IS_MODEL(OCTEON_CN31XX)
 		|| OCTEON_IS_MODEL(OCTEON_CN30XX))
@@ -1104,7 +1134,7 @@ int cvmx_l2c_get_num_assoc(void)
 	}
 
 	/* Check to see if part of the cache is disabled */
-	if (OCTEON_IS_OCTEON2() || OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+	if (OCTEON_IS_OCTEON2() || OCTEON_IS_OCTEON3()) {
 		union cvmx_mio_fus_dat3 mio_fus_dat3;
 		union cvmx_l2c_wpar_iobx l2c_wpar_iob;
 
@@ -1131,7 +1161,7 @@ int cvmx_l2c_get_num_assoc(void)
 			l2_assoc = 12;
 		else if (l2c_wpar_iob.s.mask)
 			l2_assoc = cvmx_dpop(~(l2c_wpar_iob.s.mask) & 0xffff);
-	} else if (!OCTEON_IS_OCTEON3()) {
+	} else {
 		union cvmx_l2d_fus3 val;
 		val.u64 = cvmx_read_csr(CVMX_L2D_FUS3);
 		/*
@@ -1243,7 +1273,7 @@ void cvmx_l2c_set_big_size_node(int node, uint64_t mem_size, int mode)
 		}
 
 		/*
-		 * The BIG/HOLE is logic is not supported in pass1 as per
+  		 * The BIG/HOLE is logic is not supported in pass1 as per
 		 * Errata L2C-17736
 		 */
 		if (mode == 0 && OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pcie.c b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
index 4c58a88..5fdfb0d 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pcie.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pcie.c
@@ -42,7 +42,7 @@
  *
  * Interface to PCIe as a host(RC) or target(EP)
  *
- * <hr>$Revision: 114248 $<hr>
+ * <hr>$Revision: 122729 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -65,6 +65,7 @@
 #include <asm/octeon/cvmx-sriox-defs.h>
 #include <asm/octeon/cvmx-helper-jtag.h>
 #include <linux/of.h>
+#include <linux/moduleparam.h>
 
 #ifdef CONFIG_CAVIUM_DECODE_RSL
 #include <asm/octeon/cvmx-error.h>
@@ -92,8 +93,10 @@
 #include "octeon_mem_map.h"
 #ifdef __U_BOOT__
 # include <libfdt.h>
+# include <asm/arch/cvmx-helper-fdt.h>
 #else
 # include "libfdt/libfdt.h"
+# include "libfdt/cvmx-helper-fdt.h"
 #endif
 #endif
 
@@ -190,6 +193,148 @@ uint64_t cvmx_pcie_get_mem_size(int pcie_port)
 
 /**
  * @INTERNAL
+ * Return the QLM number for the PCIE port.
+ *
+ * @param  pcie_port  QLM number to return for.
+ *
+ * @return QLM number.
+ */
+static int __cvmx_pcie_get_qlm(int node, int pcie_port)
+{
+	if (OCTEON_IS_MODEL(OCTEON_CN73XX)) {
+		cvmx_pemx_cfg_t pem_cfg;
+		cvmx_pemx_qlm_t pem_qlm;
+		cvmx_gserx_cfg_t gserx_cfg;
+		switch (pcie_port) {
+		case 0: /* PEM0 */
+			gserx_cfg.u64 = CVMX_READ_CSR(CVMX_GSERX_CFG(0));
+			if (gserx_cfg.s.pcie)
+				return 0; /* PEM0 is on QLM0 and possibly QLM1 */
+			else
+				return -1; /* PEM0 is disabled */
+		case 1: /* PEM1 */
+			pem_cfg.u64 = CVMX_READ_CSR(CVMX_PEMX_CFG(0));
+			gserx_cfg.u64 = CVMX_READ_CSR(CVMX_GSERX_CFG(1));
+			if (!pem_cfg.cn78xx.lanes8 && gserx_cfg.s.pcie)
+				return 1; /* PEM1 is on QLM 1 */
+			else
+				return -1; /* PEM1 is disabled */
+		case 2: /* PEM2 */
+			pem_qlm.u64 = CVMX_READ_CSR(CVMX_PEMX_QLM(2));
+			if (pem_qlm.cn73xx.pemdlmsel == 1) {
+				gserx_cfg.u64 = CVMX_READ_CSR(CVMX_GSERX_CFG(5));
+				if (gserx_cfg.s.pcie)
+					return 5;  /* PEM2 is on DLM5 */
+				else
+					return -1; /* PEM2 is disabled */
+			}
+			gserx_cfg.u64 = CVMX_READ_CSR(CVMX_GSERX_CFG(2));
+			if (gserx_cfg.s.pcie)
+				return 2; /* PEM2 is on QLM2 and possibly QLM3 */
+			else
+				return -1; /* PEM2 is disabled */
+		case 3: /* PEM3 */
+			pem_qlm.u64 = CVMX_READ_CSR(CVMX_PEMX_QLM(3));
+			if (pem_qlm.cn73xx.pemdlmsel == 1) {
+				gserx_cfg.u64 = CVMX_READ_CSR(CVMX_GSERX_CFG(6));
+				if (gserx_cfg.s.pcie)
+					return 6;  /* PEM2 is on DLM5 */
+				else
+					return -1; /* PEM2 is disabled */
+			}
+			pem_cfg.u64 = CVMX_READ_CSR(CVMX_PEMX_CFG(2));
+			gserx_cfg.u64 = CVMX_READ_CSR(CVMX_GSERX_CFG(3));
+			if (!pem_cfg.cn78xx.lanes8 && gserx_cfg.s.pcie)
+				return 3; /* PEM2 is on QLM2 and possibly QLM3 */
+			else
+				return -1; /* PEM2 is disabled */
+		default:
+			cvmx_dprintf("Invalid %d PCIe port\n", pcie_port);
+			return -2;
+		}
+	} else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		cvmx_pemx_cfg_t pem_cfg;
+		cvmx_gserx_cfg_t gserx_cfg;
+		switch (pcie_port) {
+		case 0:
+			gserx_cfg.u64 = CVMX_READ_CSR(CVMX_GSERX_CFG(0));
+			if (gserx_cfg.s.pcie)
+				return 0; /* PEM0 is on QLM0 and possibly QLM1 */
+			else
+				return -1; /* PEM0 is disabled */
+		case 1: /* PEM1 */
+			pem_cfg.u64 = CVMX_READ_CSR(CVMX_PEMX_CFG(0));
+			gserx_cfg.u64 = CVMX_READ_CSR(CVMX_GSERX_CFG(1));
+			if (!pem_cfg.cn78xx.lanes8 && gserx_cfg.s.pcie)
+				return 1; /* PEM1 is on QLM 1 */
+			else
+				return -1; /* PEM1 is disabled */
+		case 2: /* PEM2 */
+			gserx_cfg.u64 = CVMX_READ_CSR(CVMX_GSERX_CFG(2));
+			if (gserx_cfg.s.pcie)
+				return 2; /* PEM2 is on QLM2 and possibly QLM3 */
+			else
+				return -1; /* PEM2 is disabled */
+		case 3: /* PEM3 */
+			{
+				cvmx_gserx_cfg_t gser4_cfg;
+				pem_cfg.u64 = CVMX_READ_CSR(CVMX_PEMX_CFG(2));
+				gserx_cfg.u64 = CVMX_READ_CSR(CVMX_GSERX_CFG(3));
+				gser4_cfg.u64 = CVMX_READ_CSR(CVMX_GSERX_CFG(4));
+				if (pem_cfg.cn78xx.lanes8) {
+					if (gser4_cfg.s.pcie)
+						return 4;  /* PEM3 is on QLM4 */
+					else
+						return -1; /* PEM3 is disabled */
+				} else {
+					if (gserx_cfg.s.pcie)
+						return 3; /* PEM3 is on QLM3 */
+					else if (gser4_cfg.s.pcie)
+						return 4; /* PEM3 is on QLM4 */
+					else
+						return -1; /* PEM3 is disabled */
+				}
+			}
+		default:
+			cvmx_dprintf("Invalid %d PCIe port\n", pcie_port);
+			return -1;
+		}
+	} else if (OCTEON_IS_MODEL(OCTEON_CN70XX)) {
+		enum cvmx_qlm_mode mode1 = cvmx_qlm_get_mode(1);
+		enum cvmx_qlm_mode mode2 = cvmx_qlm_get_mode(2);
+		switch (pcie_port) {
+		case 0: /* PCIe0 can be DLM1 with 1, 2 or 4 lanes */
+			if (mode1 == CVMX_QLM_MODE_PCIE || /* Using DLM 1-2 */
+			    mode1 == CVMX_QLM_MODE_PCIE_1X2 || /* Using DLM 1 */
+			    mode1 == CVMX_QLM_MODE_PCIE_2X1 || /* Using DLM 1, lane 0 */
+			    mode1 == CVMX_QLM_MODE_PCIE_1X1) /* Using DLM 1, lane0, lane 1 not used */
+				return 1;
+			else
+				return -1;
+		case 1: /* PCIe1 can be DLM1 1 lane(1), DLM2 1 lane(0) or 2 lanes(0-1) */
+			if (mode1 == CVMX_QLM_MODE_PCIE_2X1)
+				return 1;
+			else if (mode2 == CVMX_QLM_MODE_PCIE_1X2)
+				return 2;
+			else if (mode2 == CVMX_QLM_MODE_PCIE_2X1)
+				return 2;
+			else
+				return -1;
+		case 2: /* PCIe2 can be DLM2 1 lanes(1) */
+			if (mode2 == CVMX_QLM_MODE_PCIE_2X1)
+				return 2;
+			else
+				return -1;
+		default: /* Only three PEM blocks */
+			return -1;
+		}
+	}
+
+	return -1;
+}
+
+/**
+ * @INTERNAL
  * Initialize the RC config space CSRs
  *
  * @param node      node
@@ -390,8 +535,8 @@ static void __cvmx_pcie_rc_initialize_config_space(int node, int pcie_port)
 				     pciercx_cfg034.u32);
 	}
 
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
-		int qlm = pcie_port;
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX) || OCTEON_IS_MODEL(OCTEON_CN73XX)) {
+		int qlm = __cvmx_pcie_get_qlm(node, pcie_port);
 		int speed = cvmx_qlm_get_gbaud_mhz(qlm);
 		cvmx_pemx_cfg_t pem_cfg;
 		cvmx_pciercx_cfg040_t cfg040;
@@ -407,7 +552,7 @@ static void __cvmx_pcie_rc_initialize_config_space(int node, int pcie_port)
 
 			/* Set the target link speed */
 			cfg040.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
-						CVMX_PCIERCX_CFG040(pcie_port));	
+						CVMX_PCIERCX_CFG040(pcie_port));
 			cfg040.s.tls = 1;
 			CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG040(pcie_port),
 						cfg040.u32);
@@ -419,7 +564,7 @@ static void __cvmx_pcie_rc_initialize_config_space(int node, int pcie_port)
 
 			/* Set the target link speed */
 			cfg040.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
-						CVMX_PCIERCX_CFG040(pcie_port));	
+						CVMX_PCIERCX_CFG040(pcie_port));
 			cfg040.s.tls = 2;
 			CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG040(pcie_port),
 						cfg040.u32);
@@ -431,7 +576,7 @@ static void __cvmx_pcie_rc_initialize_config_space(int node, int pcie_port)
 
 			/* Set the target link speed */
 			cfg040.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
-						CVMX_PCIERCX_CFG040(pcie_port));	
+						CVMX_PCIERCX_CFG040(pcie_port));
 			cfg040.s.tls = 3;
 			CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG040(pcie_port),
 						cfg040.u32);
@@ -439,11 +584,14 @@ static void __cvmx_pcie_rc_initialize_config_space(int node, int pcie_port)
 		default:
 			break;
 		}
-		
+
 		/* Link Width Mode (PCIERCn_CFG452[LME]) */
 		pem_cfg.u64 = CVMX_READ_CSR(CVMX_PEMX_CFG(pcie_port));
-		cfg452.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG452(pcie_port));	
-		cfg452.s.lme = (pem_cfg.cn78xx.lanes8) ? 0xf : 0x7;
+		cfg452.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG452(pcie_port));
+		if (qlm >= 5)
+			cfg452.s.lme = 0x3;
+		else
+			cfg452.s.lme = (pem_cfg.cn78xx.lanes8) ? 0xf : 0x7;
 		CVMX_PCIE_CFGX_WRITE(pcie_port, CVMX_PCIERCX_CFG452(pcie_port), cfg452.u32);
 	}
 
@@ -613,6 +761,39 @@ static void __cvmx_increment_ba(cvmx_sli_mem_access_subidx_t * pmas)
 		pmas->cn63xx.ba++;
 }
 
+/*
+ * milliseconds to retry PCIe cfg-space access:
+ * Value 32(unscaled) was recommended in HRM, but may be too small for
+ * some PCIe devices. This 200mS default should cover most devices,
+ * but can be extended by bootparam cvmx-pcie.cfg_timeout, or reduced
+ * to speed boot if it is known that no devices need so much time.
+ */
+static int cfg_timeout = 200;
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+module_param(cfg_timeout, int, 0644);
+MODULE_PARM_DESC(cfg_timeout, "PCIe config-space i/o timeout in mS,"
+	" to accomodate slow-to-start devices");
+#endif
+static int cfg_retries(void)
+{
+	static int cfg_ticks = -1;
+
+	if (cfg_ticks < 0) {
+		uint64_t nS = cfg_timeout * 1000000;
+		const int ceiling = 0xfffff;
+
+#ifdef CVMX_BUILD_FOR_LINUX_KERNEL
+		cfg_ticks = nS / (octeon_get_io_clock_rate() >> 16);
+#else
+		cfg_ticks = nS / (cvmx_clock_get_rate(CVMX_CLOCK_SCLK) >> 16);
+#endif
+
+		if (cfg_ticks > ceiling)
+			cfg_ticks = ceiling;
+	}
+	return cfg_ticks;
+}
+
 /**
  * Initialize a PCIe gen 1 port for use in host(RC) mode. It doesn't enumerate
  * the bus.
@@ -667,8 +848,7 @@ retry:
 	 * then PCIe1. '1' == round robin.
 	 */
 	npei_ctl_status.s.arb = 1;
-	/* Allow up to 0x20 config retries */
-	npei_ctl_status.s.cfg_rtry = 0x20;
+	npei_ctl_status.s.cfg_rtry = cfg_retries();
 	/* CN52XX pass1.x has an errata where P0_NTAGS and P1_NTAGS don't
 	 * reset
 	 */
@@ -1023,15 +1203,18 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int node, int pcie_port)
 	cvmx_pciercx_cfg448_t pciercx_cfg448;
 
 	if (OCTEON_IS_OCTEON3()) {
-		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_PEMX_ON(pcie_port), cvmx_pemx_on_t, pemoor, ==, 1, 100000)) {
+		if (CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_PEMX_ON(pcie_port),
+					       cvmx_pemx_on_t, pemoor, ==, 1,
+					       100000)) {
 			cvmx_printf("%d:PCIe: Port %d PEM not on, skipping\n", node, pcie_port);
 			return -1;
 		}
 	}
 
-	/* Remember if the link should try Gen3. This is needed for the CN78XX
-	pass 1.x workaround below */
-	pciercx_cfg031.u32 = CVMX_PCIE_CFGX_READ(pcie_port, CVMX_PCIERCX_CFG031(pcie_port));
+	/* Remember if the link should be Gen3. This is needed for the CN78XX
+	   pass 1.x workaround below */
+	pciercx_cfg031.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
+						 CVMX_PCIERCX_CFG031(pcie_port));
 	try_gen3 = (pciercx_cfg031.s.mls == 3);
 
 	/* Errata (GSER-21178) PCIe gen3 doesn't work */
@@ -1049,13 +1232,13 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int node, int pcie_port)
 	pem_ctl_status.u64 = CVMX_READ_CSR(CVMX_PEMX_CTL_STATUS(pcie_port));
 	pem_ctl_status.s.lnk_enb = 1;
 	CVMX_WRITE_CSR(CVMX_PEMX_CTL_STATUS(pcie_port), pem_ctl_status.u64);
-//printf("try_gen3 = %d, lnk_en = 0x%llx\n", try_gen3, CVMX_READ_CSR(CVMX_PEMX_CTL_STATUS(pcie_port)));
 
 	/* Wait for the link to come up */
 	start_cycle = cvmx_get_cycle();
 	do {
-		if (cvmx_get_cycle() - start_cycle > cvmx_clock_get_rate(CVMX_CLOCK_CORE))
+		if (cvmx_get_cycle() - start_cycle > cvmx_clock_get_rate(CVMX_CLOCK_CORE)) {
 			return -1;
+		}
 		cvmx_wait(10000);
 		pciercx_cfg032.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 							 CVMX_PCIERCX_CFG032(pcie_port));
@@ -1099,7 +1282,7 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int node, int pcie_port)
 		high_qlm = (pem_cfg.cn78xx.lanes8) ? low_qlm+1 : low_qlm;
 
 		/* Toggle cfg_rx_dll_locken_ovvrd_en and rx_resetn_ovrrd_en across
-		all QM lanes in use */
+		   all QM lanes in use */
 		for (qlm = low_qlm; qlm <= high_qlm; qlm++) {
 			for (lane = 0; lane < 4; lane++) {
 				cvmx_gserx_lanex_rx_misc_ovrrd_t misc_ovrrd;
@@ -1127,7 +1310,8 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int node, int pcie_port)
 			}
 		}
 
-		/* Wait for the link to come up (hopefully Gen3) and link training to be complete */
+		/* Wait for the link to come up (hopefully Gen3) and link
+		   training to be complete */
 		start_cycle = cvmx_clock_get_count(CVMX_CLOCK_CORE);
 		do {
 			if (cvmx_clock_get_count(CVMX_CLOCK_CORE) - start_cycle > cvmx_clock_get_rate(CVMX_CLOCK_CORE))
@@ -1166,50 +1350,6 @@ static int __cvmx_pcie_rc_initialize_link_gen2(int node, int pcie_port)
 }
 
 /**
- * @INTERNAL
- * Return the QLM number for the PCIE port.
- *
- * @param  pcie_port  QLM number to return for.
- *
- * @return QLM number.
- */
-static int __cvmx_pcie_get_qlm(int pcie_port)
-{
-	if (OCTEON_IS_MODEL(OCTEON_CN70XX)) {
-		enum cvmx_qlm_mode mode1 = cvmx_qlm_get_mode(1);
-		enum cvmx_qlm_mode mode2 = cvmx_qlm_get_mode(2);
-		switch (pcie_port) {
-		case 0: /* PCIe0 can be DLM1 with 1, 2 or 4 lanes */
-			if (mode1 == CVMX_QLM_MODE_PCIE || /* Using DLM 1-2 */
-			    mode1 == CVMX_QLM_MODE_PCIE_1X2 || /* Using DLM 1 */
-			    mode1 == CVMX_QLM_MODE_PCIE_2X1 || /* Using DLM 1, lane 0 */
-			    mode1 == CVMX_QLM_MODE_PCIE_1X1) /* Using DLM 1, lane0, lane 1 not used */
-				return 1;
-			else
-				return -1;
-		case 1: /* PCIe1 can be DLM1 1 lane(1), DLM2 1 lane(0) or 2 lanes(0-1) */
-			if (mode1 == CVMX_QLM_MODE_PCIE_2X1)
-				return 1;
-			else if (mode2 == CVMX_QLM_MODE_PCIE_1X2)
-				return 2;
-			else if (mode2 == CVMX_QLM_MODE_PCIE_2X1)
-				return 2;
-			else
-				return -1;
-		case 2: /* PCIe2 can be DLM2 1 lanes(1) */
-			if (mode2 == CVMX_QLM_MODE_PCIE_2X1)
-				return 2;
-			else
-				return -1;
-		default: /* Only three PEM blocks */
-			return -1;
-		}
-	}
-
-	return -1;
-}
-
-/**
  * Initialize a PCIe gen 2 port for use in host(RC) mode. It doesn't enumerate
  * the bus.
  *
@@ -1232,7 +1372,7 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	cvmx_sli_mem_access_ctl_t sli_mem_access_ctl;
 	cvmx_sli_mem_access_subidx_t mem_access_subid;
 	cvmx_pemx_bar1_indexx_t bar1_index;
-	uint64_t ciu_soft_prst_reg, rst_ctl_reg;
+	uint64_t ciu_soft_prst_reg, ciu_soft_prst_reg_alt, rst_ctl_reg;
 	int ep_mode;
 	int qlm;
 	int node = (pcie_port >> 4) & 0x3;
@@ -1242,7 +1382,7 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	static void *fdt_addr = 0;
 #endif
 	pcie_port &= 0x3;
-	qlm = pcie_port;
+	qlm = __cvmx_pcie_get_qlm(node, pcie_port);
 
 	if (pcie_port >= CVMX_PCIE_PORTS) {
 		//cvmx_dprintf("Invalid PCIe%d port\n", pcie_port);
@@ -1256,6 +1396,7 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 				    pcie_port);
 			return -1;
 		}
+		mode = cvmx_qlm_get_mode(qlm);
 	} else if (octeon_has_feature(OCTEON_FEATURE_PCIE)) {
 		/* Requires reading the MIO_QLMX_CFG register to figure
 		   out the port type. */
@@ -1330,10 +1471,13 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	 */
 	if (OCTEON_IS_OCTEON3()) {
 		ciu_soft_prst_reg = CVMX_RST_SOFT_PRSTX(pcie_port);
+		ciu_soft_prst_reg_alt = CVMX_RST_SOFT_PRSTX(pcie_port ^ 1);
 		rst_ctl_reg = CVMX_RST_CTLX(pcie_port);
 	} else {
 		ciu_soft_prst_reg = (pcie_port) ?
 				 CVMX_CIU_SOFT_PRST1 : CVMX_CIU_SOFT_PRST;
+		ciu_soft_prst_reg_alt = (pcie_port ^ 1) ?
+				 CVMX_CIU_SOFT_PRST1 : CVMX_CIU_SOFT_PRST;
 		rst_ctl_reg = CVMX_MIO_RST_CTLX(pcie_port);
 	}
 
@@ -1360,6 +1504,7 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 		cvmx_printf("%d:PCIe: Port %d in endpoint mode.\n", node, pcie_port);
 		return -1;
 	}
+
 #if 0
 	/* Enable this code to force PCIe link for x1 lane card. */
 	__cvmx_qlm_pcie_cfg_rxd_set_tweak(qlm, 0);
@@ -1411,16 +1556,14 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 		fdt_addr = __cvmx_phys_addr_to_ptr(cvmx_sysinfo_get()->fdt_addr,
 						   OCTEON_FDT_MAX_SIZE);
 	if (fdt_addr && mode == CVMX_QLM_MODE_PCIE_2X1) {
-		uint32_t *prop;
 		int offset;
 
 		offset = fdt_path_offset(fdt_addr, "/soc");
 		if (offset >= 0) {
-			prop = (uint32_t *)fdt_getprop(fdt_addr, offset,
-						       "cavium,connected-pcie-reset-2x1",
-						       NULL);
-			if (prop)
-				connected_pcie_reset = fdt32_to_cpu(*prop);
+			connected_pcie_reset =
+				cvmx_fdt_get_int(fdt_addr, offset,
+						 "cavium,connected-pcie-reset-2x1",
+						 -1);
 		}
 	}
 #else
@@ -1439,52 +1582,28 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	   followed by PCIe0. */
 	switch (connected_pcie_reset) {
 	case 0:
-		if (pcie_port == 1 &&
-		    (CVMX_READ_CSR(CVMX_MIO_QLMX_CFG(1)) & 0x3) == 1) {
-			ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST);
-			if (ciu_soft_prst.s.soft_prst == 0) {
-				/* Reset the port */
-				ciu_soft_prst.s.soft_prst = 1;
-				CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST,
-					ciu_soft_prst.u64);
-				/* Wait until pcie resets the ports. */
-				cvmx_wait_usec(2000);
-				CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST1,
-					ciu_soft_prst.u64);
-				/* Wait until pcie resets the ports. */
-				cvmx_wait_usec(2000);
-			}
-			ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST);
-			ciu_soft_prst.s.soft_prst = 0;
-			CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST, ciu_soft_prst.u64);
-			ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST1);
-			ciu_soft_prst.s.soft_prst = 0;
-			CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST1, ciu_soft_prst.u64);
-		}
-		break;
 	case 1:
-		if (pcie_port == 0 &&
-		    (CVMX_READ_CSR(CVMX_MIO_QLMX_CFG(1)) & 0x3) == 1) {
-			ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST1);
+		if ((pcie_port != connected_pcie_reset) &&
+		    (mode == CVMX_QLM_MODE_PCIE_2X1)) {
+			ciu_soft_prst.u64 = CVMX_READ_CSR(ciu_soft_prst_reg);
 			if (ciu_soft_prst.s.soft_prst == 0) {
 				/* Reset the port */
 				ciu_soft_prst.s.soft_prst = 1;
-				CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST1,
-					ciu_soft_prst.u64);
-				/* Wait until pcie resets the ports. */
-				cvmx_wait_usec(2000);
-				CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST,
-					ciu_soft_prst.u64);
+				CVMX_WRITE_CSR(ciu_soft_prst_reg,
+					       ciu_soft_prst.u64);
+				CVMX_WRITE_CSR(ciu_soft_prst_reg_alt,
+					       ciu_soft_prst.u64);
 				/* Wait until pcie resets the ports. */
 				cvmx_wait_usec(2000);
+
 			}
-			ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST1);
-			ciu_soft_prst.s.soft_prst = 0;
-			CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST1, ciu_soft_prst.u64);
-			ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST);
-			ciu_soft_prst.s.soft_prst = 0;
-			CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST, ciu_soft_prst.u64);
 		}
+		ciu_soft_prst.u64 = CVMX_READ_CSR(ciu_soft_prst_reg);
+		ciu_soft_prst.s.soft_prst = 0;
+		CVMX_WRITE_CSR(ciu_soft_prst_reg, ciu_soft_prst.u64);
+		ciu_soft_prst.u64 = CVMX_READ_CSR(ciu_soft_prst_reg_alt);
+		ciu_soft_prst.s.soft_prst = 0;
+		CVMX_WRITE_CSR(ciu_soft_prst_reg_alt, ciu_soft_prst.u64);
 		break;
 	case -1:
 	default:
@@ -1509,10 +1628,24 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	/* Wait for PCIe reset to complete */
 	cvmx_wait_usec(1000);
 
+
 	/* Set MPLL multiplier as per Errata 20669. */
 	if (OCTEON_IS_MODEL(OCTEON_CN70XX)) {
-		int qlm = __cvmx_pcie_get_qlm(pcie_port);
-		__cvmx_qlm_set_mult(qlm, 2500, 56);
+		int qlm = __cvmx_pcie_get_qlm(0, pcie_port);
+		int old_mult;
+		uint64_t meas_refclock = cvmx_qlm_measure_clock(qlm);
+		if (meas_refclock > 99000000 && meas_refclock < 101000000)
+			old_mult = 35;
+		else if (meas_refclock > 124000000 && meas_refclock < 126000000)
+			old_mult = 56;
+		else if (meas_refclock > 156000000 && meas_refclock < 156500000)
+			old_mult = 45;
+		else {
+			cvmx_dprintf("%s: Invalid reference clock for qlm %d\n",
+				     __func__, qlm);
+			return -1;
+		}
+		__cvmx_qlm_set_mult(qlm, 2500, old_mult);
 	}
 
 	/* Check and make sure PCIe came out of reset. If it doesn't the board
@@ -1530,8 +1663,8 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	if (pemx_bist_status.u64)
 		cvmx_printf("%d:PCIe: BIST FAILED for port %d (0x%016llx)\n",
 			    node, pcie_port, CAST64(pemx_bist_status.u64));
-	/* BIST_STATUS2 is not present on 78xx */
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+	/* BIST_STATUS2 is not present on some of Octeon3 models. */
+	if (OCTEON_IS_OCTEON3() && !OCTEON_IS_MODEL(OCTEON_CN70XX))
 		pemx_bist_status2.u64 = 0;
 	else
 		pemx_bist_status2.u64 = CVMX_READ_CSR(CVMX_PEMX_BIST_STATUS2(pcie_port));
@@ -1557,7 +1690,7 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 		/* Some gen1 devices don't handle the gen 2 training correctly.
 		 * Disable gen2 and try again with only gen1
 		 */
-		if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		if (OCTEON_IS_OCTEON3() && !OCTEON_IS_MODEL(OCTEON_CN70XX)) {
 			cvmx_printf("%d:PCIe: Link timeout on port %d, probably the slot is empty\n",
 				    node, pcie_port);
 			return -1;
@@ -1609,7 +1742,7 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 	if (OCTEON_IS_MODEL(OCTEON_CN63XX) ||
 	    OCTEON_IS_MODEL(OCTEON_CN66XX) ||
 	    OCTEON_IS_MODEL(OCTEON_CN68XX) ||
-	    OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+	    (OCTEON_IS_OCTEON3() && !OCTEON_IS_MODEL(OCTEON_CN70XX))) {
 		/* Disable the peer to peer forwarding register. This must be
 		 * setup by the OS after it enumerates the bus and assigns
 		 * addresses to the PCIe busses
@@ -1664,16 +1797,22 @@ static int __cvmx_pcie_rc_initialize_gen2(int pcie_port)
 		bar1_index.s.addr_idx += (((1ull << 28) / 16ull) >> 22);
 	}
 
-	/* Value is recommended in CSR files */
+	/* Wait for 200ms */
 	pemx_ctl_status.u64 = CVMX_READ_CSR(CVMX_PEMX_CTL_STATUS(pcie_port));
-	pemx_ctl_status.cn63xx.cfg_rtry = 32;
+	pemx_ctl_status.cn63xx.cfg_rtry = cfg_retries();
 	CVMX_WRITE_CSR(CVMX_PEMX_CTL_STATUS(pcie_port), pemx_ctl_status.u64);
 
+	/* Here is the second part of the config retry changes. Wait for 700ms
+	   after setting up the link before continuing. PCIe says the devices
+	   may need up to 900ms to come up. 700ms plus 200ms from above gives 
+	   us a total of 900ms */
+	cvmx_wait_usec(700000);
+
 	/* Display the link status */
 	pciercx_cfg032.u32 = CVMX_PCIE_CFGX_READ(pcie_port,
 						 CVMX_PCIERCX_CFG032(pcie_port));
-	cvmx_printf("%d:PCIe: Port %d link active, %d lanes, speed gen%d\n",
-		    node, pcie_port, pciercx_cfg032.s.nlw, pciercx_cfg032.s.ls);
+	cvmx_printf("%d:PCIe: Port %d link active, %d lanes\n",
+		    node, pcie_port, pciercx_cfg032.s.nlw);
 
 	return 0;
 }
@@ -1709,6 +1848,8 @@ int cvmx_pcie_rc_initialize(int pcie_port)
  */
 int cvmx_pcie_rc_shutdown(int pcie_port)
 {
+	uint64_t ciu_soft_prst_reg;
+	cvmx_ciu_soft_prst_t ciu_soft_prst;
 	int node = (pcie_port >> 4) & 0x3;
 	pcie_port &= 0x3;
 #if !defined(CVMX_BUILD_FOR_LINUX_KERNEL) || defined(CONFIG_CAVIUM_DECODE_RSL)
@@ -1727,19 +1868,18 @@ int cvmx_pcie_rc_shutdown(int pcie_port)
 			cvmx_dprintf("PCIe: Port %d shutdown timeout\n",
 				     pcie_port);
 	}
-
-	/* Force reset */
-	if (pcie_port) {
-		cvmx_ciu_soft_prst_t ciu_soft_prst;
-		ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST1);
-		ciu_soft_prst.s.soft_prst = 1;
-		CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST1, ciu_soft_prst.u64);
+	if (OCTEON_IS_OCTEON3()) {
+		ciu_soft_prst_reg = CVMX_RST_SOFT_PRSTX(pcie_port);
 	} else {
-		cvmx_ciu_soft_prst_t ciu_soft_prst;
-		ciu_soft_prst.u64 = CVMX_READ_CSR(CVMX_CIU_SOFT_PRST);
-		ciu_soft_prst.s.soft_prst = 1;
-		CVMX_WRITE_CSR(CVMX_CIU_SOFT_PRST, ciu_soft_prst.u64);
+		ciu_soft_prst_reg = (pcie_port) ?
+				    CVMX_CIU_SOFT_PRST1 : CVMX_CIU_SOFT_PRST;
 	}
+
+	/* Force reset */
+	ciu_soft_prst.u64 = CVMX_READ_CSR(ciu_soft_prst_reg);
+	ciu_soft_prst.s.soft_prst = 1;
+	CVMX_WRITE_CSR(ciu_soft_prst_reg, ciu_soft_prst.u64);
+
 	return 0;
 }
 
@@ -2210,10 +2350,11 @@ int cvmx_pcie_is_host_mode(int pcie_port)
 {
 	int node = (pcie_port >> 4) & 0x3;
 	pcie_port &= 0x3;
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX)
+	    || OCTEON_IS_MODEL(OCTEON_CN73XX)) {
 		cvmx_pemx_strap_t strap;
 		strap.u64 = CVMX_READ_CSR(CVMX_PEMX_STRAP(pcie_port));
-		return (strap.cn78xx.pimode != 3);
+		return (strap.cn78xx.pimode == 3);
 	} else if (OCTEON_IS_MODEL(OCTEON_CN70XX)) {
 		cvmx_rst_ctlx_t rst_ctl;
 		rst_ctl.u64 = cvmx_read_csr(CVMX_RST_CTLX(pcie_port));
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c b/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c
index 10a18f3..78d64d0 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pki-resources.c
@@ -204,7 +204,7 @@ int cvmx_pki_cluster_grp_free(int node, int cl_grp)
  */
 int cvmx_pki_cluster_alloc(int node, int num_clusters, uint64_t *cluster_mask)
 {
-	int cluster = 0;
+	unsigned cluster = 0;
 	int clusters[CVMX_PKI_NUM_CLUSTER];
 
 	if (node >= CVMX_MAX_NODES) {
@@ -246,7 +246,7 @@ int cvmx_pki_cluster_alloc(int node, int num_clusters, uint64_t *cluster_mask)
  */
 int cvmx_pki_cluster_free(int node, uint64_t cluster_mask)
 {
-	int cluster = 0;
+	unsigned cluster = 0;
 	if (cluster_mask > 0) {
 		while (cluster < CVMX_PKI_NUM_CLUSTER) {
 			if (cluster_mask & (0x01L << cluster)) {
@@ -393,7 +393,8 @@ int cvmx_pki_qpg_entry_free(int node, int base_offset, int count)
 void __cvmx_pki_global_rsrc_free(int node)
 {
 	int cnt;
-	int cluster, bank;
+	unsigned  cluster;
+	int  bank;
 
 	cnt = CVMX_PKI_NUM_CLUSTER_GROUP;
 	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_CLUSTER_GRP(node), 0, cnt) == -1) {
@@ -492,3 +493,31 @@ int cvmx_pki_bpid_free(int node, int bpid)
 	return 0;
 }
 
+int cvmx_pki_mtag_idx_alloc(int node, int idx)
+{
+	if (cvmx_create_global_resource_range(CVMX_GR_TAG_MTAG_IDX(node), CVMX_PKI_NUM_MTAG_IDX)) {
+		cvmx_printf("ERROR: Failed to create MTAG-IDX global resource\n");
+		return -1;
+	}
+	if (idx >= 0) {
+		idx = cvmx_reserve_global_resource_range(CVMX_GR_TAG_MTAG_IDX(node), idx, idx, 1);
+		if (idx == -1) {
+			cvmx_dprintf("INFO: MTAG index %d is already reserved\n", (int)idx);
+			return CVMX_RESOURCE_ALREADY_RESERVED;
+		}
+	} else {
+		idx = cvmx_allocate_global_resource_range(CVMX_GR_TAG_MTAG_IDX(node), idx, 1, 1);
+		if (idx == -1) {
+			cvmx_printf("ERROR: Failed to allocate MTAG index\n");
+			return CVMX_RESOURCE_ALLOC_FAILED;
+		}
+	}
+	return idx;
+}
+
+void cvmx_pki_mtag_idx_free(int node, int idx)
+{
+	if (cvmx_free_global_resource_range_with_base(CVMX_GR_TAG_MTAG_IDX(node), idx, 1) == -1)
+		cvmx_printf("ERROR Failed to release MTAG index %d\n", (int)idx);
+}
+
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pki.c b/arch/mips/cavium-octeon/executive/cvmx-pki.c
index 574bf69..a7233f9 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pki.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pki.c
@@ -49,6 +49,7 @@
 #include <asm/octeon/cvmx-pki.h>
 #include <asm/octeon/cvmx-fpa3.h>
 #include <asm/octeon/cvmx-pki-cluster.h>
+#include <asm/octeon/cvmx-pki-resources.h>
 #else
 #include "cvmx.h"
 #include "cvmx-version.h"
@@ -57,6 +58,7 @@
 #include "cvmx-pki.h"
 #include "cvmx-fpa3.h"
 #include "cvmx-pki-cluster.h"
+#include "cvmx-pki-resources.h"
 #endif
 
 
@@ -167,7 +169,7 @@ void cvmx_pki_read_global_config(int node, struct cvmx_pki_global_config *gbl_cf
 	cvmx_pki_tag_secret_t tag_secret_reg;
 	cvmx_pki_frm_len_chkx_t frm_len_chk;
 	cvmx_pki_buf_ctl_t buf_ctl;
-	int cl_grp;
+	unsigned cl_grp;
 	int id;
 
 	stat_ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_STAT_CTL);
@@ -235,7 +237,7 @@ void cvmx_pki_write_global_config(int node, struct cvmx_pki_global_config *gbl_c
 {
 	cvmx_pki_stat_ctl_t stat_ctl;
 	cvmx_pki_buf_ctl_t buf_ctl;
-	int cl_grp;
+	unsigned cl_grp;
 
 	for (cl_grp = 0; cl_grp < CVMX_PKI_NUM_CLUSTER_GROUP; cl_grp++)
 		cvmx_pki_attach_cluster_to_group(node, cl_grp, gbl_cfg->cluster_mask[cl_grp]);
@@ -319,7 +321,7 @@ int cvmx_pki_read_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config
  */
 int cvmx_pki_write_pkind_config(int node, int pkind, struct cvmx_pki_pkind_config *pkind_cfg)
 {
-	int cluster = 0;
+	unsigned cluster = 0;
 	uint64_t cluster_mask;
 
 	cvmx_pki_pkindx_icgsel_t pkind_clsel;
@@ -375,7 +377,9 @@ int cvmx_pki_write_pkind_config(int node, int pkind, struct cvmx_pki_pkind_confi
 	return 0;
 }
 
- /** This function reads parameters associated with tag configuration in hardware.
+ /**
+ * This function reads parameters associated with tag configuration in hardware.
+ * Only first cluster in the group is used.
  * @param node		node number.
  * @param style		style to configure tag for
  * @param cluster_mask	Mask of clusters to configure the style for.
@@ -384,15 +388,18 @@ int cvmx_pki_write_pkind_config(int node, int pkind, struct cvmx_pki_pkind_confi
 void cvmx_pki_read_tag_config(int node, int style, uint64_t cluster_mask,
 	struct cvmx_pki_style_tag_cfg *tag_cfg)
 {
+	int mask, tag_idx, index;
 	cvmx_pki_clx_stylex_cfg2_t style_cfg2_reg;
 	cvmx_pki_clx_stylex_alg_t style_alg_reg;
+	cvmx_pki_stylex_tag_sel_t tag_sel;
+	cvmx_pki_tag_incx_ctl_t tag_ctl;
+	cvmx_pki_tag_incx_mask_t tag_mask;
 	int cluster = __builtin_ffsll(cluster_mask) - 1;
 
-	style_cfg2_reg.u64 = cvmx_read_csr_node(node,
-		CVMX_PKI_CLX_STYLEX_CFG2(style, cluster));
-	style_alg_reg.u64 = cvmx_read_csr_node(node,
-		CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
+	style_cfg2_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster));
+	style_alg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
 
+	/* 7-Tuple Tag: */
 	tag_cfg->tag_fields.layer_g_src = style_cfg2_reg.s.tag_src_lg;
 	tag_cfg->tag_fields.layer_f_src = style_cfg2_reg.s.tag_src_lf;
 	tag_cfg->tag_fields.layer_e_src = style_cfg2_reg.s.tag_src_le;
@@ -415,10 +422,29 @@ void cvmx_pki_read_tag_config(int node, int style, uint64_t cluster_mask,
 	tag_cfg->tag_fields.mpls_label = style_alg_reg.s.tag_mpls0;
 	tag_cfg->tag_fields.input_port = style_alg_reg.s.tag_prt;
 
-	/** TO_DO get mask tag*/
+	/* Custom-Mask Tag: */
+	tag_sel.u64 = cvmx_read_csr_node(node, CVMX_PKI_STYLEX_TAG_SEL(style));
+	for (mask = 0; mask < 4; mask++) {
+		tag_cfg->mask_tag[mask].enable |= (style_cfg2_reg.s.tag_inc & (1 << mask)) != 0;
+		switch (mask) {
+		case 0: tag_idx = tag_sel.s.tag_idx0; break;
+		case 1: tag_idx = tag_sel.s.tag_idx1; break;
+		case 2: tag_idx = tag_sel.s.tag_idx2; break;
+		case 3: tag_idx = tag_sel.s.tag_idx3; break;
+		}
+		index = tag_idx * 4 + mask;
+		tag_mask.u64 = cvmx_read_csr_node(node, CVMX_PKI_TAG_INCX_MASK(index));
+		tag_cfg->mask_tag[mask].val = tag_mask.s.en;
+ 		tag_ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_TAG_INCX_CTL(index));
+		tag_cfg->mask_tag[mask].base = tag_ctl.s.ptr_sel;
+		tag_cfg->mask_tag[mask].offset = tag_ctl.s.offset;
+	}
 }
 
- /** This function writes/configures parameters associated with tag configuration in hardware.
+ /**
+ * This function writes/configures parameters associated with tag configuration in hardware.
+ * In Custom-Mask Tagging, all four masks use the same base index to access Tag Control and
+ * Tag Mask registers.
  * @param node	              node number.
  * @param style		      style to configure tag for
  * @param cluster_mask	      Mask of clusters to configure the style for.
@@ -427,12 +453,17 @@ void cvmx_pki_read_tag_config(int node, int style, uint64_t cluster_mask,
 void cvmx_pki_write_tag_config(int node, int style, uint64_t cluster_mask,
 			       struct cvmx_pki_style_tag_cfg *tag_cfg)
 {
+	int mask, index, tag_idx, mtag_en = 0;
+	unsigned cluster = 0;
 	cvmx_pki_clx_stylex_cfg2_t style_cfg2_reg;
 	cvmx_pki_clx_stylex_alg_t style_alg_reg;
-	int cluster = 0;
+	cvmx_pki_tag_incx_ctl_t tag_ctl;
+	cvmx_pki_tag_incx_mask_t tag_mask;
+	cvmx_pki_stylex_tag_sel_t tag_sel;
 
 	while (cluster < CVMX_PKI_NUM_CLUSTER) {
 		if (cluster_mask & (0x01L << cluster)) {
+			/* 7-Tuple Tag: */
 			style_cfg2_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster));
 			style_cfg2_reg.s.tag_src_lg = tag_cfg->tag_fields.layer_g_src;
 			style_cfg2_reg.s.tag_src_lf = tag_cfg->tag_fields.layer_f_src;
@@ -446,11 +477,9 @@ void cvmx_pki_write_tag_config(int node, int style, uint64_t cluster_mask,
 			style_cfg2_reg.s.tag_dst_ld = tag_cfg->tag_fields.layer_d_dst;
 			style_cfg2_reg.s.tag_dst_lc = tag_cfg->tag_fields.layer_c_dst;
 			style_cfg2_reg.s.tag_dst_lb = tag_cfg->tag_fields.layer_b_dst;
-			cvmx_write_csr_node(node,
-				CVMX_PKI_CLX_STYLEX_CFG2(style, cluster),
-				style_cfg2_reg.u64);
-			style_alg_reg.u64 = cvmx_read_csr_node(node,
-				CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
+			cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster), style_cfg2_reg.u64);
+
+			style_alg_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
 			style_alg_reg.s.tag_vni = tag_cfg->tag_fields.tag_vni;
 			style_alg_reg.s.tag_gtp = tag_cfg->tag_fields.tag_gtp;
 			style_alg_reg.s.tag_spi = tag_cfg->tag_fields.tag_spi;
@@ -462,10 +491,50 @@ void cvmx_pki_write_tag_config(int node, int style, uint64_t cluster_mask,
 			style_alg_reg.s.tag_prt = tag_cfg->tag_fields.input_port;
 			cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster), style_alg_reg.u64);
 
-			/* TO_DO add mask tag */
+			/* Custom-Mask Tag (Part 1): */
+			for (mask = 0; mask < 4; mask++) {
+				if (tag_cfg->mask_tag[mask].enable)
+					mtag_en++;
+			}
+			if (mtag_en) {
+				style_cfg2_reg.u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster));
+				style_cfg2_reg.s.tag_inc = 0;
+				for (mask = 0; mask < 4; mask++) {
+					if (tag_cfg->mask_tag[mask].enable)
+						style_cfg2_reg.s.tag_inc |= 1 << mask;
+				}
+				cvmx_write_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster), style_cfg2_reg.u64);
+			}
 		}
 		cluster++;
 	}
+	/* Custom-Mask Tag (Part 2): */
+	if (mtag_en) {
+		if ((tag_idx = cvmx_pki_mtag_idx_alloc(node, -1)) < 0)
+			return;
+
+		tag_sel.u64 = cvmx_read_csr_node(node, CVMX_PKI_STYLEX_TAG_SEL(style));
+		for (mask = 0; mask < 4; mask++) {
+			if (tag_cfg->mask_tag[mask].enable) {
+				switch (mask) {
+				case 0: tag_sel.s.tag_idx0 = tag_idx; break;
+				case 1: tag_sel.s.tag_idx1 = tag_idx; break;
+				case 2: tag_sel.s.tag_idx2 = tag_idx; break;
+				case 3: tag_sel.s.tag_idx3 = tag_idx; break;
+				}
+				index = tag_idx * 4 + mask;
+				tag_mask.u64 = cvmx_read_csr_node(node, CVMX_PKI_TAG_INCX_MASK(index));
+				tag_mask.s.en = tag_cfg->mask_tag[mask].val;
+				cvmx_write_csr_node(node, CVMX_PKI_TAG_INCX_MASK(index), tag_mask.u64);
+
+   				tag_ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_TAG_INCX_CTL(index));
+				tag_ctl.s.ptr_sel = tag_cfg->mask_tag[mask].base;
+				tag_ctl.s.offset = tag_cfg->mask_tag[mask].offset;
+				cvmx_write_csr_node(node, CVMX_PKI_TAG_INCX_CTL(index), tag_ctl.u64);
+			}
+		}
+		cvmx_write_csr_node(node, CVMX_PKI_STYLEX_TAG_SEL(style), tag_sel.u64);
+	}
 }
 
 /**
@@ -557,7 +626,7 @@ void cvmx_pki_write_style_config(int node, uint64_t style, uint64_t cluster_mask
 	cvmx_pki_clx_stylex_cfg2_t style_cfg2_reg;
 	cvmx_pki_clx_stylex_alg_t style_alg_reg;
 	cvmx_pki_stylex_buf_t     style_buf_reg;
-	int cluster = 0;
+	unsigned cluster = 0;
 
 	while (cluster < CVMX_PKI_NUM_CLUSTER) {
 		if (cluster_mask & (0x01L << cluster)) {
@@ -679,7 +748,7 @@ int cvmx_pki_pcam_write_entry(int node, int index, uint64_t cluster_mask,
 	struct cvmx_pki_pcam_input input, struct cvmx_pki_pcam_action action)
 {
 	int bank;
-	int cluster = 0;
+	unsigned cluster = 0;
 	cvmx_pki_clx_pcamx_termx_t	pcam_term;
 	cvmx_pki_clx_pcamx_matchx_t	pcam_match;
 	cvmx_pki_clx_pcamx_actionx_t	pcam_action;
@@ -759,13 +828,32 @@ int cvmx_pki_enable_aura_qos(int node, int aura, bool ena_red,
  */
 int cvmx_pki_write_aura_bpid(int node, int aura, int bpid)
 {
+	int i, cnt, ena_bp;
 	cvmx_pki_aurax_cfg_t pki_aura_cfg;
 
 	if (aura >= CVMX_PKI_NUM_AURA || bpid >= CVMX_PKI_NUM_BPID) {
-		cvmx_dprintf("ERROR: PKI config aura_bp aura = %d bpid = %d",
-			aura, bpid);
+		cvmx_dprintf("ERROR: aura=%d or bpid=%d is out or range\n", aura, bpid);
 		return -1;
 	}
+	if(OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		/* Workaround for Errata PKI-24364:
+		 * Inform about assigning the same BPID to multiple auras
+		 * having different ENA_BP.
+		 */
+		pki_aura_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_AURAX_CFG(aura));
+		ena_bp = pki_aura_cfg.s.ena_bp;
+		for (i = 0, cnt = 1; i < CVMX_PKI_NUM_AURA; i++) {
+			if (i == aura)
+				continue;
+			pki_aura_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_AURAX_CFG(i));
+			if (pki_aura_cfg.s.bpid == bpid && pki_aura_cfg.s.ena_bp != ena_bp)
+				cnt++;
+		}
+		if (cnt > 1)
+			cvmx_dprintf("WARNING: BPID(%d) is used by %d AURAs.\n"
+				"\tEnable|disable backpressure for all AURAs on this BPID.\n",
+				bpid, cnt);
+	}
 	pki_aura_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_AURAX_CFG(aura));
 	pki_aura_cfg.s.bpid = bpid;
 	cvmx_write_csr_node(node, CVMX_PKI_AURAX_CFG(aura), pki_aura_cfg.u64);
@@ -869,7 +957,7 @@ void cvmx_pki_set_little_endian(int node, uint64_t style)
 void cvmx_pki_endis_fcs_check(int node, int pknd, bool fcs_chk, bool fcs_strip)
 {
 	int style;
-	int cluster = 0;
+	unsigned cluster = 0;
 	cvmx_pki_clx_pkindx_style_t pkind_style;
 	cvmx_pki_clx_stylex_cfg_t style_cfg;
 
@@ -901,7 +989,7 @@ void cvmx_pki_endis_l2_errs(int node, int pknd, bool l2len_err,
 			 bool maxframe_err, bool minframe_err)
 {
 	int style;
-	int cluster = 0;
+	unsigned cluster = 0;
 	cvmx_pki_clx_pkindx_style_t pkind_style;
 	cvmx_pki_clx_stylex_cfg_t style_cfg;
 
@@ -928,7 +1016,7 @@ void cvmx_pki_endis_l2_errs(int node, int pknd, bool l2len_err,
 void cvmx_pki_dis_frame_len_chk(int node, int pknd)
 {
 	int style;
-	int cluster = 0;
+	unsigned cluster = 0;
 	cvmx_pki_clx_pkindx_style_t pkind_style;
 	cvmx_pki_clx_stylex_cfg_t style_cfg;
 
@@ -1052,7 +1140,7 @@ void cvmx_pki_show_valid_pcam_entries(int node)
  */
 void cvmx_pki_show_pkind_attributes(int node, int pkind)
 {
-	int cluster = 0;
+	unsigned cluster = 0;
 	int index;
 	cvmx_pki_pkindx_icgsel_t pkind_clsel;
 	cvmx_pki_clx_pkindx_style_t pkind_cfg_style;
@@ -1099,5 +1187,516 @@ void cvmx_pki_show_pkind_attributes(int node, int pkind)
 	}
 }
 
+#ifdef CVMX_DUMP_PKI
+/*
+ * Show PKI integrated configuration.
+ * See function prototype in cvmx-pki.h
+ */
+int cvmx_pki_config_dump(unsigned node)
+{
+#define PKI_PRN_HEADLEN   28
+#define PKI_PRN_DATALEN   52
+#define PKI_PRN_LINELEN   (PKI_PRN_HEADLEN + PKI_PRN_DATALEN)
+#define PKI_PRN_DPLEN(__n)  (PKI_PRN_DATALEN / __n)
+
+#ifndef MAX
+#define MAX(__a, __b) (((__a) > (__b)) ? (__a) : (__b))
+#endif
+
+#define DLMPRINT(__format, ...) \
+	do { \
+		int __n; \
+		sprintf(lines[1], __format, ## __VA_ARGS__); \
+		__n = PKI_PRN_LINELEN - strlen(lines[1]) - 1; \
+		memset(lines[0], '-', __n);  lines[0][__n] = '\0'; \
+		cvmx_printf("%s %s\n", lines[0], lines[1]); \
+	} while (0)
+
+#define NMPRINT(__num, __mask, __index, __level, __header, __format, __arg) \
+	do { \
+		int __step = PKI_PRN_DATALEN / __num; \
+		int __offs = __level * 2; \
+		cvmx_printf("%*s%-*s", __offs, "", PKI_PRN_HEADLEN - __offs, __header); \
+		for (__index = 0; __index < __num; __index++) { \
+			if ((1 << __index) & __mask) \
+				cvmx_printf(__format, __step, __arg); \
+			else \
+				cvmx_printf("%*s", __step, "--"); \
+		} \
+		cvmx_printf("\n"); \
+	} while (0)
+
+#define NSPRINT(__num, __index, __action) \
+	do { \
+		for (__index = 0; __index < __num; __index++) \
+			(__action); \
+	} while (0)
+
+#define NMCMPEQ(__cmp, __data, __num, __mask, __index) ({ \
+	int __rc = 0; \
+	uint64_t __prev = __cmp; \
+	do { \
+		for (__index = 0; __index < __num; __index++) { \
+			if ((1 << __index) & __mask) \
+				if (__data != __prev) __rc += 1; \
+		} \
+	} while (0); \
+	__rc; \
+})
+
+	void printfl(int level, char *name, const char *format, ...) {
+		char dbuf[PKI_PRN_DATALEN + 1];
+		int offs;
+		va_list args;
+		va_start(args, format);
+		offs = level * 2;
+		vsnprintf(dbuf, PKI_PRN_DATALEN + 1, format, args);
+		cvmx_printf("%*s%-*s%*s\n", offs, "", PKI_PRN_HEADLEN - offs, name, PKI_PRN_DATALEN, dbuf);
+		va_end(args);
+	}
+	void int2cstr(char *buf, int data, int nbits) {
+		char *lbits[8] = {"G","F", "E", "D", "C", "B", "?", "?"};
+		buf[0] = '\0';
+		for (nbits--; nbits >= 0; nbits--, data >>= 1)
+			strcat(buf, (data & 1) ? lbits[nbits & 0x7] : "-");
+	}
+	char *qpgqos_map[8] = {
+		[0x0] = "NONE",
+		[0x1] = "VLAN",
+		[0x2] = "MPLS",
+		[0x3] = "DSA_SRC",
+		[0x4] = "DIFF",
+		[0x5] = "HIGIG",
+		[0x6] = "Undef",
+		[0x7] = "Undef"
+	};
+	char *tagtype_map[4] = {
+		[0x0] = "Ordered",
+		[0x1] = "Atomic",
+		[0x2] = "Untagged",
+		[0x3] = "Empty"
+	};
+	char *mtagptr_map[16] = {
+		[0]  = "SOP",
+		[1]  = "Undef",
+		[2]  = "Undef",
+		[3]  = "Undef",
+		[4]  = "Undef",
+		[5]  = "Undef",
+		[6]  = "Undef",
+		[7]  = "Undef",
+		[8]  = "LA",
+		[9]  = "LB",
+		[10] = "LC",
+		[11] = "LD",
+		[12] = "LE",
+		[13] = "LF",
+		[14] = "LG",
+		[15] = "VL"
+	};
+	int nclusters = CVMX_PKI_NUM_CLUSTER;
+	cvmx_pki_tag_secret_t secret;
+	cvmx_pki_buf_ctl_t ctl;
+	cvmx_pki_gbl_pen_t pen;
+	cvmx_pki_icgx_cfg_t cgcfg;
+	cvmx_pki_sft_rst_t rst;
+	int mask, ibase, n, k, i, __i;
+	int pkind, cluster, style, group;
+	uint32_t crc32, pcrc32;
+	char lines[4][128];
+
+	/* Show Global Configuration. */
+	ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_BUF_CTL);
+	rst.u64 = cvmx_read_csr_node(node, CVMX_PKI_SFT_RST);
+	pen.u64 = cvmx_read_csr_node(node, CVMX_PKI_GBL_PEN);
+	secret.u64 = cvmx_read_csr_node(node, CVMX_PKI_TAG_SECRET);
+	memset(lines[0], '*', PKI_PRN_LINELEN);  lines[0][PKI_PRN_LINELEN] = '\0';
+	cvmx_printf("\n%s\n", lines[0]);
+	cvmx_printf("   PKI Configuration (Node %d)\n", node);
+	cvmx_printf("%s\n", lines[0]);
+	printfl(0, "PKI Enabled/Active", "%d/%d", ctl.s.pki_en, rst.s.active);
+	printfl(0, "Packet buffering", "%*s", PKI_PRN_DATALEN, ctl.s.pkt_off ? "Disabled" : "Enabled");
+	printfl(0, "FPA buffer policy", "%*s", PKI_PRN_DATALEN, ctl.s.fpa_wait ? "Wait" : "Drop");
+	printfl(0, "BPID backpressure", "%*s", PKI_PRN_DATALEN, ctl.s.pbp_en ? "Enabled" : "Disabled");
+	printfl(0, "", "%*s%*s%*s%*s", PKI_PRN_DPLEN(4), "DST6", PKI_PRN_DPLEN(4), "SRC6",
+		PKI_PRN_DPLEN(4), "DST", PKI_PRN_DPLEN(4), "SRC");
+	printfl(0, "Tag secret words (hex)", "%*x%*x%*x%*x", PKI_PRN_DPLEN(4), secret.s.dst6,
+		PKI_PRN_DPLEN(4), secret.s.src6, PKI_PRN_DPLEN(4), secret.s.dst,
+		PKI_PRN_DPLEN(4), secret.s.src);
+	cvmx_printf("%-30s %4s %4s %4s %4s %4s %4s %4s %4s %4s %4s\n", "",
+		"VIRT", "CLG", "CL2", "L4", "IL3", "L3", "MPLS", "FULC", "DSA", "HG");
+	cvmx_printf("%-30s %4d %4d %4d %4d %4d %4d %4d %4d %4d %4d\n", "Parsing enabled",
+		pen.s.virt_pen, pen.s.clg_pen, pen.s.cl2_pen, pen.s.l4_pen, pen.s.il3_pen,
+		pen.s.l3_pen, pen.s.mpls_pen, pen.s.fulc_pen, pen.s.dsa_pen, pen.s.hg_pen);
+	/* Show PKINDs.*/
+	for (pkind = 0, pcrc32 = 0, ibase = 0; pkind < CVMX_PKI_NUM_PKIND; pkind++) {
+		cvmx_pki_pkindx_icgsel_t cgsel;
+		cvmx_pki_clx_pkindx_style_t pkstyle[CVMX_PKI_NUM_CLUSTER];
+		cvmx_pki_clx_pkindx_cfg_t pkcfg[CVMX_PKI_NUM_CLUSTER];
+		cvmx_pki_clx_pkindx_skip_t pkskip[CVMX_PKI_NUM_CLUSTER];
+		cvmx_pki_clx_pkindx_l2_custom_t pkl2cust[CVMX_PKI_NUM_CLUSTER];
+		cvmx_pki_clx_pkindx_lg_custom_t pklgcust[CVMX_PKI_NUM_CLUSTER];
+
+		cgsel.u64 = cvmx_read_csr_node(node, CVMX_PKI_PKINDX_ICGSEL(pkind));
+		cgcfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_ICGX_CFG(cgsel.s.icg));
+		mask = cgcfg.s.clusters;
+		if (mask == 0)
+			continue;
+
+		CVMX_MT_CRC_POLYNOMIAL(0x1edc6f41);
+		CVMX_MT_CRC_IV(0xffffffff);
+		CVMX_MT_CRC_DWORD(cgsel.u64 & 0x3ull);
+		for (cluster = 0; cluster < nclusters; cluster++) {
+			if (((1 << cluster) & mask) == 0)
+				continue;
+			pkstyle[cluster].u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_STYLE(pkind, cluster));
+			pkcfg[cluster].u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_CFG(pkind, cluster));
+			pkskip[cluster].u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_SKIP(pkind, cluster));
+			pkl2cust[cluster].u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_L2_CUSTOM(pkind, cluster));
+			pklgcust[cluster].u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PKINDX_LG_CUSTOM(pkind, cluster));
+			CVMX_MT_CRC_DWORD(pkstyle[cluster].u64 & ((1ull << 16) - 1));
+			CVMX_MT_CRC_DWORD(pkcfg[cluster].u64 & ((1ull << 16) - 1));
+			CVMX_MT_CRC_DWORD(pkskip[cluster].u64 & ((1ull << 16) - 1));
+			CVMX_MT_CRC_DWORD(pkl2cust[cluster].u64 & ((1ull << 16) - 1));
+			CVMX_MT_CRC_DWORD(pklgcust[cluster].u64 & ((1ull << 8) - 1));
+		}
+		CVMX_MF_CRC_IV(crc32);
+		if (crc32 == pcrc32)
+			continue;
+		if (pkind > 0 && (pkind - 1) != ibase)
+			cvmx_printf("\nPKIND(s) %02d-%02d -- same as PKIND %02d\n", pkind - 1, ibase + 1, ibase);
+		pcrc32 = crc32;
+		ibase = pkind;
+        
+		DLMPRINT("PKIND %02d:", pkind);
+		NSPRINT(nclusters, __i, sprintf(lines[__i], "Cluster%d", __i));
+		NMPRINT(nclusters, mask, __i, 0, "", "%*s", lines[__i]);
+		cvmx_printf("Mapping:\n");
+		printfl(1, "Cluster Group", "%*d", PKI_PRN_DATALEN, cgsel.s.icg);
+		cvmx_printf("Parsing:\n");
+		NMPRINT(nclusters, mask, __i, 1, "Initial Style", "%*d", pkstyle[__i].s.style);
+		NSPRINT(nclusters, __i, sprintf(lines[__i], "%c%c%c%c%c%c%c",
+			(pkstyle[__i].s.pm & (1 << 0)) ? '-' : 'A', (pkstyle[__i].s.pm & (1 << 1)) ? '-' : 'B',
+			(pkstyle[__i].s.pm & (1 << 2)) ? '-' : 'C', (pkstyle[__i].s.pm & (1 << 3)) ? '-' : 'D',
+			(pkstyle[__i].s.pm & (1 << 4)) ? '-' : 'E', (pkstyle[__i].s.pm & (1 << 5)) ? '-' : 'F',
+			(pkstyle[__i].s.pm & (1 << 6)) ? '-' : 'G'));
+		NMPRINT(nclusters, mask, __i, 1, "Initial Parse Mode", "%*s", lines[__i]);
+		NMPRINT(nclusters, mask, __i, 1, "INST skip", "%*d", pkskip[__i].s.inst_skip);
+		if (NMCMPEQ(0, pkcfg[__i].s.inst_hdr, nclusters, mask, __i) != 0)
+			NMPRINT(nclusters, mask, __i, 1, "INST Header present", "%*s", pkcfg[__i].s.inst_hdr ? "Yes" : "No");
+		NMPRINT(nclusters, mask, __i, 1, "FCS skip", "%*d", pkskip[__i].s.fcs_skip);
+		NMPRINT(nclusters, mask, __i, 1, "FCS present", "%*s", pkcfg[__i].s.fcs_pres ? "On":"Off");
+		if (NMCMPEQ(0, pkl2cust[__i].s.valid, nclusters, mask, __i) != 0) {
+			NMPRINT(nclusters, mask, __i, 1, "L2 custom match", "%*s", pkl2cust[__i].s.valid ? "On":"Off");
+			NMPRINT(nclusters, mask, __i, 1, "L2 Custom offset", "%*d", pkl2cust[__i].s.offset);
+		}
+		if (NMCMPEQ(0, pkcfg[__i].s.lg_custom, nclusters, mask, __i) != 0) {
+			NMPRINT(nclusters, mask, __i, 1, "LG custom match", "%*s", pkcfg[__i].s.lg_custom ? "On":"Off");
+			NMPRINT(nclusters, mask, __i, 1, "LG Custom offset", "%*d", pklgcust[__i].s.offset);
+		}
+		if (NMCMPEQ(0, pkcfg[__i].s.mpls_en, nclusters, mask, __i) != 0)
+			NMPRINT(nclusters, mask, __i, 1, "MPLS parsing", "%*s", pkcfg[__i].s.mpls_en ? "On":"Off");
+		if (NMCMPEQ(0, pkcfg[__i].s.dsa_en, nclusters, mask, __i) != 0)
+			NMPRINT(nclusters, mask, __i, 1, "DSA parsing", "%*s", pkcfg[__i].s.dsa_en ? "On":"Off");
+		if (NMCMPEQ(0, pkcfg[__i].s.hg_en, nclusters, mask, __i) != 0)
+			NMPRINT(nclusters, mask, __i, 1, "HG parsing", "%*s", pkcfg[__i].s.hg_en ? "On":"Off");
+		if (NMCMPEQ(0, pkcfg[__i].s.hg2_en, nclusters, mask, __i) != 0)
+			NMPRINT(nclusters, mask, __i, 1, "HG2 parsing", "%*s", pkcfg[__i].s.hg2_en ? "On":"Off");
+		if (NMCMPEQ(0, pkcfg[__i].s.fulc_en, nclusters, mask, __i) != 0)
+			NMPRINT(nclusters, mask, __i, 1, "Fulcrum Header parsing", "%*s", pkcfg[__i].s.fulc_en ? "On":"Off");
+	}
+	if ((pkind - 1) != ibase)
+		cvmx_printf("\nPKIND(s) %02d-%02d -- same as PKIND %02d\n", pkind - 1, ibase + 1, ibase);
+
+	/* Show Styles.*/
+	for (style = 0, pcrc32 = 0, ibase = 0; style < CVMX_PKI_NUM_FINAL_STYLE; style++) {
+		cvmx_pki_stylex_buf_t stbuf;
+		cvmx_pki_stylex_tag_sel_t tagsel;
+		cvmx_pki_stylex_tag_mask_t tagmask;
+		cvmx_pki_clx_stylex_cfg_t stcfg[CVMX_PKI_NUM_CLUSTER];
+		cvmx_pki_clx_stylex_cfg2_t stcfg2[CVMX_PKI_NUM_CLUSTER];
+		cvmx_pki_clx_stylex_alg_t stalg[CVMX_PKI_NUM_CLUSTER];
+
+		CVMX_MT_CRC_POLYNOMIAL(0x1edc6f41);
+		CVMX_MT_CRC_IV(0xffffffff);
+		stbuf.u64 = cvmx_read_csr_node(node, CVMX_PKI_STYLEX_BUF(style));
+		CVMX_MT_CRC_DWORD(stbuf.u64 & ((1ull << 33) - 1));
+		tagsel.u64 = cvmx_read_csr_node(node, CVMX_PKI_STYLEX_TAG_SEL(style));
+		CVMX_MT_CRC_DWORD(tagsel.u64 & (0x7ull << 24 | 0x7ull << 16 | 0x7ull << 8 | 0x7ull));
+		tagmask.u64 = cvmx_read_csr_node(node, CVMX_PKI_STYLEX_TAG_MASK(style));
+		CVMX_MT_CRC_DWORD(tagmask.u64 & ((1ull << 16) - 1));
+		mask = 0;
+		for (cluster = 0; cluster < nclusters; cluster++) {
+			stcfg[cluster].u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG(style, cluster));
+			stcfg2[cluster].u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_CFG2(style, cluster));
+			stalg[cluster].u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_STYLEX_ALG(style, cluster));
+			CVMX_MT_CRC_DWORD(stcfg[cluster].u64 & 0x7FFF07FF);
+			CVMX_MT_CRC_DWORD(stcfg2[cluster].u64 & ((1ull << 32) - 1));
+			CVMX_MT_CRC_DWORD(stalg[cluster].u64 & ((1ull << 32) - 1));
+			mask |= 1 << cluster;
+		}
+		CVMX_MF_CRC_IV(crc32);
+		if (crc32 == pcrc32)
+			continue;
+		if (style > 0 && (style - 1) != ibase)
+			cvmx_printf("\nSTYLE(s) %02d-%02d -- same as STYLE %02d\n", style - 1, ibase + 1, ibase);
+		pcrc32 = crc32;
+		ibase = style;
+
+		DLMPRINT("STYLE %02d:", style);
+		NSPRINT(nclusters, __i, sprintf(lines[__i], "Cluster%d", __i));
+		NMPRINT(nclusters, mask, __i, 0, "", "%*s", lines[__i]);
+		cvmx_printf("Buffering:\n");
+		stbuf.u64 = cvmx_read_csr_node(node, CVMX_PKI_STYLEX_BUF(style));
+		printfl(1, "WQE header", "%s", stbuf.s.wqe_hsz ? "WORD0..5" : "WORD0..4");
+		printfl(1, "WQE and Data buffers", "%s", stbuf.s.dis_wq_dat ? "Separate" : "Same");
+		printfl(1, "WQE Skip", "%d", stbuf.s.wqe_skip);
+		printfl(1, "First Skip", "%d", stbuf.s.first_skip);
+		printfl(1, "Later Skip", "%d", stbuf.s.later_skip);
+		printfl(1, "OPC Mode", "%d", stbuf.s.opc_mode);
+		NMPRINT(nclusters, mask, __i, 0, "Strip FCS", "%*s", stcfg[__i].s.fcs_strip ? "Yes":"No");
+		NMPRINT(nclusters, mask, __i, 0, "Drop", "%*d", stcfg[__i].s.drop);
+		NMPRINT(nclusters, mask, __i, 0, "No Drop", "%*d", stcfg[__i].s.nodrop);
+		NMPRINT(nclusters, mask, __i, 0, "Raw Drop", "%*d", stcfg[__i].s.rawdrp);
+		cvmx_printf("Tag:\n");
+		NMPRINT(nclusters, mask, __i, 1, "Tag Type", "%*s", tagtype_map[stalg[__i].s.tt]);
+		if (NMCMPEQ(0, ((stcfg2[__i].u64 >> 18) & 0x3F), 4, mask, __i) != 0) {
+			NSPRINT(nclusters, __i, int2cstr(lines[__i], (stcfg2[__i].u64 >> 18) & 0x3F, 6));
+			NMPRINT(nclusters, mask, __i, 1, "T-Tag <= Source", "%*s", lines[__i]);
+		}
+		if (NMCMPEQ(0, ((stcfg2[__i].u64 >> 12) & 0x3F), 4, mask, __i) != 0) {
+			NSPRINT(nclusters, __i, int2cstr(lines[__i], (stcfg2[__i].u64 >> 12) & 0x3F, 6));
+			NMPRINT(nclusters, mask, __i, 1, "T-Tag <= Dest", "%*s", lines[__i]);
+		}
+		if (NMCMPEQ(0, stalg[__i].s.tag_pctl, 4, mask, __i) != 0)
+			NMPRINT(nclusters, mask, __i, 1, "T-Tag <= Proto", "%*s", stalg[__i].s.tag_pctl ? "On":"Off");
+		if (NMCMPEQ(0, stalg[__i].s.tag_vs0, 4, mask, __i) != 0)
+			NMPRINT(nclusters, mask, __i, 1, "T-Tag <= VLAN0", "%*s", stalg[__i].s.tag_vs0 ? "On":"Off");
+		if (NMCMPEQ(0, stalg[__i].s.tag_vs1, 4, mask, __i) != 0)
+			NMPRINT(nclusters, mask, __i, 1, "T-Tag <= VLAN1", "%*s", stalg[__i].s.tag_vs1 ? "On":"Off");
+		if (NMCMPEQ(0, stalg[__i].s.tag_prt, 4, mask, __i) != 0)
+			NMPRINT(nclusters, mask, __i, 1, "T-Tag <= Port", "%*s", stalg[__i].s.tag_prt ? "On":"Off");
+		if (NMCMPEQ(0, stcfg2[__i].s.tag_inc, 4, mask, __i) != 0) {
+			cvmx_pki_tag_incx_ctl_t tagctl;
+			cvmx_pki_tag_incx_mask_t incmask;
+			int tagx, clx = __builtin_ffsll(mask) - 1;
+			cvmx_printf("%*s%s\n", 2, "", "M-Tag = Base:Offset:Mask (name:dec:hex)");
+			for (i = 0; i < 4; i++) {
+				if ((stcfg2[clx].s.tag_inc & (1 << i)) == 0)
+					continue;
+				switch (i) {
+				case 0: tagx = tagsel.s.tag_idx0 * 4 + 0; break;
+				case 1: tagx = tagsel.s.tag_idx1 * 4 + 1; break;
+				case 2: tagx = tagsel.s.tag_idx2 * 4 + 2; break;
+				case 3: tagx = tagsel.s.tag_idx3 * 4 + 3; break;
+				}
+				tagctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_TAG_INCX_CTL(tagx));
+				incmask.u64 = cvmx_read_csr_node(node, CVMX_PKI_TAG_INCX_MASK(tagx));
+				sprintf(lines[0], "M-Tag%d", i);
+				printfl(2, lines[0], "%s:%d:%016llx", mtagptr_map[tagctl.s.ptr_sel], tagctl.s.offset, incmask.s.en);
+			}
+		}
+		if (NMCMPEQ(0, stcfg2[__i].s.tag_masken, 4, mask, __i) != 0) {
+			printfl(1, "Tag Mask (hex)", "%*x", PKI_PRN_DATALEN, tagmask.s.mask);
+		}
+		cvmx_printf("QPG:\n");
+		NMPRINT(nclusters, mask, __i, 1, "QOS Algo", "%*s", qpgqos_map[stalg[__i].s.qpg_qos]);
+		NMPRINT(nclusters, mask, __i, 1, "QPG Base (dec)", "%*d", stcfg[__i].s.qpg_base);
+		if (NMCMPEQ(0, stalg[__i].s.qpg_port_msb, 4, mask, __i) != 0) {
+			NMPRINT(nclusters, mask, __i, 1, "Port MSB (hex)", "%*x", stalg[__i].s.qpg_port_msb);
+			NMPRINT(nclusters, mask, __i, 1, "Port Shift (dec)", "%*d", stalg[__i].s.qpg_port_sh);
+		}
+		NMPRINT(nclusters, mask, __i, 1, "QPG => PortAdder", "%*s", stcfg[__i].s.qpg_dis_padd ? "Off":"On");
+		NMPRINT(nclusters, mask, __i, 1, "QPG => Aura", "%*s", stcfg[__i].s.qpg_dis_aura ? "Off":"On");
+		NMPRINT(nclusters, mask, __i, 1, "QPG => Group", "%*s", stcfg[__i].s.qpg_dis_grp ? "Off":"On");
+		NMPRINT(nclusters, mask, __i, 1, "WQE[TAG] => Group", "%*s", stcfg[__i].s.qpg_dis_grptag ? "Off":"On");
+	}
+	if (style > 0 && (style - 1) != ibase)
+		cvmx_printf("\nSTYLE(s) %02d-%02d -- same as STYLE %02d\n", style - 1, ibase + 1, ibase);
+
+	/* Show Cluster Groups. */
+	for (group = 0; group < (int)CVMX_PKI_NUM_CLUSTER_GROUP; group++) {
+		cgcfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_ICGX_CFG(group));
+		mask = cgcfg.s.clusters;
+		if (mask == 0)
+			continue;
+
+		DLMPRINT("CLUSTER GROUP %d:", group);
+		cvmx_printf("Action = {PMC(hex) : +STYLE(dec) : PF(hex) : SETTY(dec) : ADVANCE(dec)}\n");
+		printfl(0, "Parsing", "%s", cgcfg.s.pena ? "Enabled" : "Disabled");
+		printfl(0, "Entry", "%*s%*s", PKI_PRN_DPLEN(2), "PCAM0", PKI_PRN_DPLEN(2), "PCAM1");
+		for (cluster = 0; cluster < nclusters; cluster++) {
+			cvmx_pki_clx_pcamx_termx_t term[CVMX_PKI_NUM_PCAM_BANK];
+			cvmx_pki_clx_pcamx_matchx_t	match[CVMX_PKI_NUM_PCAM_BANK];
+			cvmx_pki_clx_pcamx_actionx_t action[CVMX_PKI_NUM_PCAM_BANK];
+			int pcam, entry;
+
+			if (((1 << cluster) & mask) == 0)
+				continue;
+
+			cvmx_printf("---------------- Cluster %d\n", cluster);
+			for (entry = 0; entry < CVMX_PKI_NUM_PCAM_ENTRY; entry++) {
+				for (pcam = 0; pcam < CVMX_PKI_NUM_PCAM_BANK; pcam++) {
+					term[pcam].u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_TERMX(cluster, pcam, entry));
+					match[pcam].u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_MATCHX(cluster, pcam, entry));
+					action[pcam].u64 = cvmx_read_csr_node(node, CVMX_PKI_CLX_PCAMX_ACTIONX(cluster, pcam, entry));
+				}
+				if ((term[0].s.valid + term[1].s.valid) == 0)
+					continue;
+				sprintf(lines[0], "[%d]", entry);
+				printfl(0, lines[0], "");
+				NSPRINT(2, __i, sprintf(lines[__i], "%x:%x", term[__i].s.term1,
+					term[__i].s.term1 ^ term[__i].s.term0));
+				NMPRINT(2, 0x3, __i, 1, "Term:Mask (hex)", "%*s", lines[__i]);
+				NSPRINT(2, __i, sprintf(lines[__i], "%x:%x", term[__i].s.style1,
+					term[__i].s.style1 ^ term[__i].s.style0));
+				NMPRINT(2, 0x3, __i, 1, "Style:Mask (hex)", "%*s", lines[__i]);
+				NSPRINT(2, __i, sprintf(lines[__i], "%x:%x", match[__i].s.data1,
+					match[__i].s.data1 ^ match[__i].s.data0));
+				NMPRINT(2, 0x3, __i, 1, "Data:Mask (hex)", "%*s", lines[__i]);
+				NSPRINT(2, __i, sprintf(lines[__i], "%x:%d:%x:%d:%d", action[__i].s.pmc,
+					action[__i].s.style_add, action[__i].s.pf,
+					action[__i].s.setty, action[__i].s.advance));
+				NMPRINT(2, 0x3, __i, 1, "Action", "%*s", lines[__i]);
+			}
+		}
+	}
+	DLMPRINT("QPG TABLE:");
+	cvmx_printf("(Only entries with non-zero values are shown.)\n");
+	n = (PKI_PRN_LINELEN) / 8;
+	cvmx_printf("%-*s%*s%*s%*s%*s%*s%*s%*s\n", n, "QPG", n, "PADD", n, "GRP_OK",
+		n, "GRP_BAD", n, "GTAG_OK", n, "GTAG_BAD", n, "NODE", n, "LAURA");
+	for (i = 0; i < CVMX_PKI_NUM_QPG_ENTRY; i++) {
+		cvmx_pki_qpg_tblx_t qpgt;
+		qpgt.u64 = cvmx_read_csr_node(node, CVMX_PKI_QPG_TBLX(i));
+		if (qpgt.u64 != 0) {
+			sprintf(lines[0], "%d", i);
+			cvmx_printf("%-*s%*d%*d%*d%*d%*d%*d%*d\n", n, lines[0], n, qpgt.s.padd,
+				n, qpgt.s.grp_ok, n, qpgt.s.grp_bad, n, qpgt.s.grptag_ok,
+				n, qpgt.s.grptag_bad, n, qpgt.s.aura_node, n, qpgt.s.laura);
+		}
+	}
+	DLMPRINT("AURA TABLE:");
+	cvmx_printf("(Only entries with non-zero values are shown.)\n");
+	n = (PKI_PRN_LINELEN) / 8;
+	cvmx_printf("%-*s%*s%*s%*s%*s%*s%*s%*s\n", n, "Aura", n, "", n, "",
+		n, "PKT_ADD", n, "ENA_RED", n, "ENA_DROP", n, "ENA_BP", n, "BPID");
+	for (i = 0; i < CVMX_PKI_NUM_AURA; i++) {
+		cvmx_pki_aurax_cfg_t acfg;
+		acfg.u64 = cvmx_read_csr_node(node, CVMX_PKI_AURAX_CFG(i));
+		if (acfg.u64 != 0) {
+			sprintf(lines[0], "%d", i);
+			cvmx_printf("%-*s%*s%*s%*d%*d%*d%*d%*d\n", n, lines[0], n, "", n, "",
+				n, acfg.s.pkt_add, n, acfg.s.ena_red, n, acfg.s.ena_drop,
+				n, acfg.s.ena_bp, n, acfg.s.bpid);
+		}
+	}
+	DLMPRINT("CHANNEL TABLE:");
+	cvmx_printf("(Only entries with non-zero values are shown.)\n");
+	n = (PKI_PRN_LINELEN) / 8;
+	cvmx_printf("%-*s%*s%*s\n", n * 6, "Channel", n, "", n, "BPID");
+	for (i = 0; i < CVMX_PKI_NUM_CHANNEL; i++) {
+		cvmx_pki_chanx_cfg_t chan;
+		chan.u64 = cvmx_read_csr_node(node, CVMX_PKI_CHANX_CFG(i));
+		if (chan.s.imp == 1 && chan.s.bpid != 0) {
+
+			if (i >= 0 && i < 64) /* LBK*/
+				sprintf(lines[1], "LBK:%d", i);
+			else if (i >= 0x100 && i < (0x100 + 128)) { /* DPI*/
+				k = i - 0x100;
+				if (OCTEON_IS_MODEL(OCTEON_CN78XX) && i >= (0x100 + 64))
+					continue;
+				sprintf(lines[1], "DPI:%d", k);
+			}
+			else if (i >= 0x240 && i < (0x240 + 2 * 2)) { /* SRIO*/
+				k = i - 0x240;
+				sprintf(lines[1], "SRIO%d:%d", k / 2, k % 2);
+			}
+			else if (i >= 0x400 && i < (0x400 + 2 * 256)) { /* ILK */
+				k = i - 0x400;
+				sprintf(lines[1], "ILK%d:%d", k / 256, k % 256);
+			}
+			else if (i >= 0x800 && i < (0x800 + 6 * 4 * 16)) { /* BGX */
+				k = i - 0x800;
+				sprintf(lines[1], "BGX%d:Port%d:%d", k / (4 * 16),
+					(k % (4 * 16)) / 16, (k % (4 * 16)) % 16);
+			}
+			else
+				continue;
+			sprintf(lines[0], "%d(0x%x)", i, i);
+			cvmx_printf("%-*s%-*s%*s%*d\n", n * 2, lines[0], n * 2, lines[1], n * 3, "", n, chan.s.bpid);
+		}
+	}
+	return 0;
+}
 
+/*
+ * Show PKI integrated statistics.
+ * See function prototype in cvmx-pki.h
+ */
+int cvmx_pki_stats_dump(unsigned node)
+{
+	int i;
+	int64_t n;
+	cvmx_pki_stat_ctl_t ctl;
+	struct cvmx_pki_port_stats stats;
+
+	cvmx_dprintf("PKI Statistics on Node %d:\n", node);
+	ctl.u64 = cvmx_read_csr_node(node, CVMX_PKI_STAT_CTL);
+
+	for (i = 0; i < CVMX_PKI_NUM_PKIND; i++) {
+		cvmx_pki_get_stats(node, i, &stats);
+		if (stats.inb_octets > 0) {
+			cvmx_dprintf("PKIND %d\n", i);
+			cvmx_dprintf("%26s %14lld\n", "Packets:",
+				(long long)stats.inb_packets);
+			if (stats.inb_errors > 0)
+				cvmx_dprintf("%26s %14lld\n", "Err.packets:",
+					(long long)stats.inb_errors);
+			if (ctl.s.mode == 0x1) {
+				if (stats.octets > 0) {
+					cvmx_dprintf("STYLE %d\n", i);
+					cvmx_dprintf("%26s %14lld\n", "Packets:",
+						(long long)stats.packets);
+				}
+			}
+			if (stats.len_64_packets > 0)
+				cvmx_dprintf("%26s %14lld\n", "1..63 packets:",
+					(long long)stats.len_64_packets);
+			if (stats.len_65_127_packets > 0)
+				cvmx_dprintf("%26s %14lld\n", "64..127 packets:",
+					(long long)stats.len_65_127_packets);
+			if (stats.len_128_255_packets > 0)
+				cvmx_dprintf("%26s %14lld\n", "128..255 packets:",
+					(long long)stats.len_128_255_packets);
+			if (stats.len_256_511_packets > 0)
+				cvmx_dprintf("%26s %14lld\n", "256..511 packets:",
+					(long long)stats.len_256_511_packets);
+			if (stats.len_512_1023_packets > 0)
+				cvmx_dprintf("%26s %14lld\n", "512..1023 packets:",
+					(long long)stats.len_512_1023_packets);
+			if (stats.len_1024_1518_packets > 0)
+				cvmx_dprintf("%26s %14lld\n", "1024..1518 packets:",
+					(long long)stats.len_1024_1518_packets);
+			if (stats.len_1519_max_packets > 0)
+				cvmx_dprintf("%26s %14lld\n", ">1518 packets:",
+					(long long)stats.len_1519_max_packets);
+			if (stats.pci_raw_packets > 0)
+				cvmx_dprintf("%26s %14lld\n", "Raw packets:",
+					(long long)stats.pci_raw_packets);
+			if (stats.dropped_packets > 0)
+				cvmx_dprintf("%26s %14lld\n", "Dropped packets:",
+					(long long)stats.dropped_packets);
+			n = stats.fcs_align_err_packets;
+			n += stats.runt_crc_packets;
+			n += stats.runt_packets;
+			n += stats.oversize_crc_packets;
+			n += stats.oversize_packets;
+			if (n > 0)
+				cvmx_dprintf("%26s %14lld\n", "Err.packets:", (long long)n);
+		}
+	}
+	return 0;
+}
+#endif /* CVMX_DUMP_PKI */
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko.c b/arch/mips/cavium-octeon/executive/cvmx-pko.c
index b2f6088..e5a68f2 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko.c
@@ -1,4 +1,4 @@
-/***********************license start***************
+/***********************license start*************** 
  * Copyright (c) 2003-2014  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
@@ -84,7 +84,7 @@ static const int debug = 0;
  */
 
 #define pko_for_each_port(__p)					\
-	for (__p = 0; __p < CVMX_HELPER_CFG_MAX_PKO_PORT; __p++)	\
+    for (__p = 0; __p < CVMX_HELPER_CFG_MAX_PKO_PORT; __p++)	\
 	if (__cvmx_helper_cfg_pko_queue_base(__p) != CVMX_HELPER_CFG_INVALID_VALUE)
 
 
@@ -222,7 +222,7 @@ EXPORT_SYMBOL(cvmx_pko_get_num_queues);
 void cvmx_pko_show_queue_map(void)
 {
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
-		cvmx_dprintf("%s: not supported on this chip\n", __FUNCTION__);
+		cvmx_dprintf("%s: not supported on this chip\n",__FUNCTION__);
 		return;
 	} else if (octeon_has_feature(OCTEON_FEATURE_PKND)) {
 		int port;
@@ -238,7 +238,7 @@ void cvmx_pko_show_queue_map(void)
 		int port;
 		int pko_output_ports;
 		pko_output_ports = 40;
-		cvmx_dprintf("pko queue info\n");
+		cvmx_dprintf("pko queue info \n");
 		for (port = 0; port < pko_output_ports; port++) {
 			cvmx_dprintf("%3d=%3d-%3d ",
 				port,
@@ -449,8 +449,8 @@ void cvmx_pko_hw_init(uint8_t pool, unsigned bufsize)
 					union cvmx_pko_reg_engine_storagex engine_storage;
 
 #define PKO_ASSIGN_ENGINE_STORAGE(index)                        \
-		engine_storage.s.engine##index =                        \
-		__cvmx_pko_memory_per_engine_o68(16 * i + (index))
+        engine_storage.s.engine##index =                        \
+            __cvmx_pko_memory_per_engine_o68(16 * i + (index))
 
 					engine_storage.u64 = 0;
 					PKO_ASSIGN_ENGINE_STORAGE(0);
@@ -601,7 +601,7 @@ cvmx_pko_return_value_t cvmx_pko_config_port(int port, int base_queue,
 		cvmx_dprintf("%s: port=%d queue=%d-%d pri %#x %#x %#x %#x\n",
 			__func__,
 			port, base_queue, (base_queue+num_queues-1),
-			priority[0], priority[1], priority[2], priority[3]);
+			priority[0],priority[1], priority[2], priority[3]);
 
 	/* The need to handle ILLEGAL_PID port argument
 	 * is obsolete now, the code here can be simplified.
@@ -750,7 +750,7 @@ cvmx_pko_return_value_t cvmx_pko_config_port(int port, int base_queue,
 					return (CVMX_PKO_NO_MEMORY);
 				case CVMX_CMD_QUEUE_ALREADY_SETUP:
 					cvmx_dprintf("ERROR: %s: "
-					"Port already setup. port=%d\n",
+					"Port already setup. port=%d \n",
 					__func__, (int) port);
 					return (CVMX_PKO_PORT_ALREADY_SETUP);
 				case CVMX_CMD_QUEUE_INVALID_PARAM:
@@ -801,7 +801,7 @@ static cvmx_pko_return_value_t cvmx_pko2_config_port(short ipd_port, int base_qu
 
 	if (debug)
 		cvmx_dprintf("%s: ipd_port %d pko_iport %d qbase %d qnum %d\n",
-		__func__, ipd_port, pko_port, base_queue, num_queues);
+		__func__, ipd_port, pko_port, base_queue, num_queues );
 
 	static_priority_base = -1;
 	static_priority_end = -1;
@@ -907,10 +907,10 @@ static cvmx_pko_return_value_t cvmx_pko2_config_port(short ipd_port, int base_qu
 			if (cmd_res != CVMX_CMD_QUEUE_SUCCESS) {
 				switch (cmd_res) {
 				case CVMX_CMD_QUEUE_NO_MEMORY:
-					cvmx_dprintf("ERROR: %s: Unable to allocate output buffer\n", __func__);
+					cvmx_dprintf("ERROR: %s: Unable to allocate output buffer\n",__func__);
 					break;
 				case CVMX_CMD_QUEUE_ALREADY_SETUP:
-					cvmx_dprintf("ERROR: %s: Port already setup\n", __func__);
+					cvmx_dprintf("ERROR: %s: Port already setup\n",__func__);
 					break;
 				case CVMX_CMD_QUEUE_INVALID_PARAM:
 				default:
@@ -1020,7 +1020,7 @@ int cvmx_pko_rate_limit_bits(int port, uint64_t bits_s, int burst)
  *       order. It is not MP-safe and caller should guarantee
  *       atomicity.
  */
-void cvmx_pko_get_port_status(uint64_t ipd_port, uint64_t clear, cvmx_pko_port_status_t *status)
+void cvmx_pko_get_port_status(uint64_t ipd_port, uint64_t clear, cvmx_pko_port_status_t * status)
 {
 	cvmx_pko_reg_read_idx_t pko_reg_read_idx;
 	cvmx_pko_mem_count0_t pko_mem_count0;
@@ -1102,13 +1102,13 @@ EXPORT_SYMBOL(cvmx_pko_get_port_status);
  * @param queue is the queue identifier to be queried
  * @return the number of commands pending transmission or -1 on error
  */
-int cvmx_pko_queue_pend_count(cvmx_cmd_queue_id_t queue)
+int cvmx_pko_queue_pend_count( cvmx_cmd_queue_id_t queue)
 {
 	int count;
 
 	if (octeon_has_feature(OCTEON_FEATURE_CN78XX_WQE)) {
 		int node = cvmx_get_node_num();
-		count = cvmx_pko3_dq_query(node, queue);
+		count = cvmx_pko3_dq_query(node,queue);
 	} else {
 		count = cvmx_cmd_queue_length(CVMX_CMD_QUEUE_PKO(queue));
 	}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c b/arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c
index fc1f464..fb97816 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko3-queue.c
@@ -36,8 +36,6 @@
  * CORRESPONDENCE TO DESCRIPTION. THE ENTIRE  RISK ARISING OUT OF USE OR
  * PERFORMANCE OF THE SOFTWARE LIES WITH YOU.
  ***********************license end**************************************/
-
-
 /*
  * File version info: $Rev:$
  *
@@ -47,26 +45,25 @@
 #include <linux/module.h>
 #include <asm/octeon/cvmx.h>
 #include <asm/octeon/cvmx-pko3.h>
+#include <asm/octeon/cvmx-pko3-resources.h>
+#include <asm/octeon/cvmx-pko3-queue.h>
 #include <asm/octeon/cvmx-helper-pko3.h>
 #include <asm/octeon/cvmx-bootmem.h>
 #include <asm/octeon/cvmx-clock.h>
 #else
 #include "cvmx.h"
 #include "cvmx-pko3.h"
+#include "cvmx-pko3-resources.h"
+#include "cvmx-pko3-queue.h"
 #include "cvmx-helper-pko3.h"
 #include "cvmx-bootmem.h"
 #endif
 
-
-
 /* Smalles Round-Robin quantum to use +1 */
 #define	CVMX_PKO3_RR_QUANTUM_MIN	0x10
 
 static int debug = 0;	/* 1 for basic, 2 for detailed trace */
 
-/* Minimum MTU assumed for shaping configuration */
-static unsigned __pko3_min_mtu = 9080;	/* Could be per-port in the future */
-
 struct cvmx_pko3_dq {
 #ifdef __BIG_ENDIAN_BITFIELD
 	unsigned	dq_count :6;	/* Number of descriptor queues */
@@ -98,7 +95,7 @@ int cvmx_pko3_get_queue_base(int ipd_port)
 	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
 
 	/* get per-node table */
-	if(__cvmx_pko3_dq_table == NULL)
+	if (cvmx_unlikely(__cvmx_pko3_dq_table == NULL))
 		__cvmx_pko3_dq_table_setup();
 
 	i = CVMX_PKO3_SWIZZLE_IPD ^ xp.port;
@@ -106,8 +103,11 @@ int cvmx_pko3_get_queue_base(int ipd_port)
 	/* get per-node table */
 	dq_table = __cvmx_pko3_dq_table + CVMX_PKO3_IPD_NUM_MAX * xp.node;
 
-	if(dq_table[i].dq_count > 0)
+	if (cvmx_likely(dq_table[i].dq_count > 0))
 		ret = xp.node << 10 | dq_table[i].dq_base;
+	else if (debug)
+		cvmx_printf("ERROR: %s: no queues for ipd_port=%#x\n",
+			__func__, ipd_port);
 
 	return ret;
 }
@@ -121,7 +121,7 @@ int cvmx_pko3_get_queue_num(int ipd_port)
 	struct cvmx_xport xp = cvmx_helper_ipd_port_to_xport(ipd_port);
 
 	/* get per-node table */
-	if(__cvmx_pko3_dq_table == NULL)
+	if (cvmx_unlikely(__cvmx_pko3_dq_table == NULL))
 		__cvmx_pko3_dq_table_setup();
 
 	i = CVMX_PKO3_SWIZZLE_IPD ^ xp.port;
@@ -129,13 +129,40 @@ int cvmx_pko3_get_queue_num(int ipd_port)
 	/* get per-node table */
 	dq_table = __cvmx_pko3_dq_table + CVMX_PKO3_IPD_NUM_MAX * xp.node;
 
-	if(dq_table[i].dq_count > 0)
+	if (cvmx_likely(dq_table[i].dq_count > 0))
 		ret = dq_table[i].dq_count;
+	else if (debug)
+		cvmx_dprintf("ERROR: %s: no queues for ipd_port=%#x\n",
+			__func__, ipd_port);
 
 	return ret;
 }
 
 /**
+ * Get L1/Port Queue number assigned to interface port.
+ *
+ * @param xiface is interface number.
+ * @param index is port index.
+ */
+int cvmx_pko3_get_port_queue(int xiface, int index)
+{
+	int queue;
+	cvmx_pko_l1_sqx_topology_t qtop;
+	int mac = __cvmx_pko3_get_mac_num(xiface, index);
+	int nqueues = cvmx_pko3_num_level_queues(CVMX_PKO_PORT_QUEUES);
+	cvmx_xiface_t xi = cvmx_helper_xiface_to_node_interface(xiface);
+
+	for (queue = 0; queue < nqueues; queue++) {
+		qtop.u64 = cvmx_read_csr_node(xi.node, CVMX_PKO_L1_SQX_TOPOLOGY(queue));
+		if (qtop.s.link == mac)
+			break;
+	}
+	if (queue >= nqueues)
+		return -1;
+	return queue;
+}
+
+/**
  * @INTERNAL
  *
  * Initialize port/dq table contents
@@ -170,6 +197,9 @@ int __cvmx_pko3_dq_table_setup(void)
 		"cvmx_pko3_global_dq_table",
 		__cvmx_pko3_dq_table_init);
 
+	if (debug)
+		cvmx_dprintf("%s: dq_table_ptr=%p\n", __func__, ptr);
+
 	if(ptr == NULL)
 		return -1;
 
@@ -179,9 +209,9 @@ int __cvmx_pko3_dq_table_setup(void)
 
 /*
  * @INTERNAL
- * Register a range of Descriptor Queues wth an interface port
+ * Register a range of Descriptor Queues with an interface port
  *
- * This function poulates the DQ-to-IPD translation table
+ * This function populates the DQ-to-IPD translation table
  * used by the application to retreive the DQ range (typically ordered
  * by priority) for a given IPD-port, which is either a physical port,
  * or a channel on a channelized interface (i.e. ILK).
@@ -193,7 +223,7 @@ int __cvmx_pko3_dq_table_setup(void)
  * @param dq_count is the number of consecutive Descriptor Queues leading
  *        the same channel or port.
  *
- * Only a consecurive range of Descriptor Queues can be associated with any
+ * Only a consecutive range of Descriptor Queues can be associated with any
  * given channel/port, and usually they are ordered from most to least
  * in terms of scheduling priority.
  *
@@ -217,7 +247,7 @@ int __cvmx_pko3_ipd_dq_register(int xiface, int index,
 		int p;
 		p = cvmx_helper_get_ipd_port(xiface, index);
 		if (p < 0) {
-			cvmx_dprintf("ERROR: %s: xiface %#x has no IPD port\n",
+			cvmx_printf("ERROR: %s: xiface %#x has no IPD port\n",
 			__func__, xiface);
 			return -1;
 		}
@@ -235,12 +265,12 @@ int __cvmx_pko3_ipd_dq_register(int xiface, int index,
 	dq_table = __cvmx_pko3_dq_table + CVMX_PKO3_IPD_NUM_MAX * xi.node;
 
 	if(debug)
-		cvmx_dprintf("%s: ipd=%#x ix=%#x dq %u cnt %u\n",
+		cvmx_dprintf("%s: ipd_port=%#x ix=%#x dq %u cnt %u\n",
 			__func__, ipd_port, i, dq_base, dq_count);
 
 	/* Check the IPD port has not already been configured */
 	if(dq_table[i].dq_count > 0 ) {
-		cvmx_dprintf("%s: ERROR: IPD %#x already registered\n",
+		cvmx_printf("%s: ERROR: IPD %#x already registered\n",
 			__func__, ipd_port);
 		return -1;
 	}
@@ -273,7 +303,7 @@ int __cvmx_pko3_ipd_dq_unregister(int xiface, int index)
 		int p;
 		p = cvmx_helper_get_ipd_port(xiface, index);
 		if (p < 0) {
-			cvmx_dprintf("ERROR: %s: xiface %#x has no IPD port\n",
+			cvmx_printf("ERROR: %s: xiface %#x has no IPD port\n",
 			__func__, xiface);
 			return -1;
 		}
@@ -292,13 +322,13 @@ int __cvmx_pko3_ipd_dq_unregister(int xiface, int index)
 	dq_table = __cvmx_pko3_dq_table + CVMX_PKO3_IPD_NUM_MAX * xi.node;
 
 	if (dq_table[i].dq_count == 0) {
-		cvmx_dprintf("%s:ipd=%#x already released\n",
+		cvmx_printf("WARNING: %s:ipd=%#x already released\n",
 			__func__, ipd_port);
 		return -1;
 	}
 
 	if(debug)
-		cvmx_dprintf("%s:ipd=%#x release dq %u cnt %u\n",
+		cvmx_dprintf("%s:ipd_port=%#x release dq %u cnt %u\n",
 			     __func__, ipd_port,
 			     dq_table[i].dq_base,
 			     dq_table[i].dq_count);
@@ -313,14 +343,14 @@ int __cvmx_pko3_ipd_dq_unregister(int xiface, int index)
  * Convert normal CHAN_E (i.e. IPD port) value to compressed channel form
  * that is used to populate PKO_LUT.
  *
- * Note: This code may be CN78XX specific, not the same for all PKO3
- * implementations.
+ * Note: This code may be model specific.
  */
-static uint16_t cvmx_pko3_chan_2_xchan(uint16_t ipd_port)
+static int cvmx_pko3_chan_2_xchan(uint16_t ipd_port)
 {
 	uint16_t xchan;
 	uint8_t off;
-	static const uint8_t xchan_base[16] = {
+	static const uint8_t *xchan_base = NULL;
+	static const uint8_t xchan_base_cn78xx[16] = {
 		/* IPD 0x000 */ 0x3c0 >> 4,	/* LBK */
 		/* IPD 0x100 */ 0x380 >> 4,	/* DPI */
 		/* IPD 0x200 */ 0xfff >> 4,	/* not used */
@@ -338,11 +368,58 @@ static uint16_t cvmx_pko3_chan_2_xchan(uint16_t ipd_port)
 		/* IPD 0xe00 */ 0xfff >> 4,	/* not used */
 		/* IPD 0xf00 */ 0xfff >> 4	/* not used */
 	};
+	static const uint8_t xchan_base_cn73xx[16] = {
+		/* IPD 0x000 */ 0x0c0 >> 4,	/* LBK */
+		/* IPD 0x100 */ 0x100 >> 4,	/* DPI */
+		/* IPD 0x200 */ 0xfff >> 4,	/* not used */
+		/* IPD 0x300 */ 0xfff >> 4,	/* not used */
+		/* IPD 0x400 */ 0xfff >> 4,	/* not used */
+		/* IPD 0x500 */ 0xfff >> 4,	/* not used */
+		/* IPD 0x600 */ 0xfff >> 4,	/* not used */
+		/* IPD 0x700 */ 0xfff >> 4,	/* not used */
+		/* IPD 0x800 */ 0x000 >> 4,	/* BGX0 */
+		/* IPD 0x900 */ 0x040 >> 4,	/* BGX1 */
+		/* IPD 0xa00 */ 0x080 >> 4,	/* BGX2 */
+		/* IPD 0xb00 */ 0xfff >> 4,	/* not used */
+		/* IPD 0xc00 */ 0xfff >> 4,	/* not used */
+		/* IPD 0xd00 */ 0xfff >> 4,	/* not used */
+		/* IPD 0xe00 */ 0xfff >> 4,	/* not used */
+		/* IPD 0xf00 */ 0xfff >> 4	/* not used */
+	};
+	// FIXME: These values are wild guess, check with CSR again!
+	static const uint8_t xchan_base_cn75xx[16] = {
+		/* IPD 0x000 */ 0x040 >> 4,	/* LBK */
+		/* IPD 0x100 */ 0x080 >> 4,	/* DPI */
+		/* IPD 0x200 */ 0xeee >> 4,	/* SRIO0  noop */
+		/* IPD 0x300 */ 0xfff >> 4,	/* not used */
+		/* IPD 0x400 */ 0xfff >> 4,	/* not used */
+		/* IPD 0x500 */ 0xfff >> 4,	/* not used */
+		/* IPD 0x600 */ 0xfff >> 4,	/* not used */
+		/* IPD 0x700 */ 0xfff >> 4,	/* not used */
+		/* IPD 0x800 */ 0x000 >> 4,	/* BGX0 */
+		/* IPD 0x900 */ 0x020 >> 4,	/* BGX1 */
+		/* IPD 0xa00 */ 0xfff >> 4,	/* not used */
+		/* IPD 0xb00 */ 0xfff >> 4,	/* not used */
+		/* IPD 0xc00 */ 0xfff >> 4,	/* not used */
+		/* IPD 0xd00 */ 0xfff >> 4,	/* not used */
+		/* IPD 0xe00 */ 0xfff >> 4,	/* not used */
+		/* IPD 0xf00 */ 0xfff >> 4	/* not used */
+	};
+
+        if (OCTEON_IS_MODEL(OCTEON_CN73XX))
+		xchan_base = xchan_base_cn73xx;
+        if (OCTEON_IS_MODEL(OCTEON_CNF75XX))
+		xchan_base = xchan_base_cn75xx;
+        if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		xchan_base = xchan_base_cn78xx;
+
+	if (xchan_base == NULL)
+		return -1;
 
 	xchan = ipd_port >> 8;
 
-	/* ILKx has 8 bits logical channels, others just 6 */
-	if (((xchan & 0xfe) == 0x04))
+	/* ILKx, DPI has 8 bits logical channels, others just 6 */
+	if (((xchan & 0xfe) == 0x04) || xchan == 0x01)
 		off = ipd_port & 0xff;
 	else
 		off = ipd_port & 0x3f;
@@ -350,7 +427,9 @@ static uint16_t cvmx_pko3_chan_2_xchan(uint16_t ipd_port)
 	xchan = xchan_base[ xchan & 0xF ];
 
 	if(xchan == 0xff)
-		return 0xffff;
+		return -1;	/* Invalid IPD_PORT */
+	else if (xchan == 0xee)
+		return -2;	/* LUT not used */
 	else
 		return (xchan << 4) | off;
 }
@@ -373,7 +452,7 @@ void cvmx_pko3_map_channel(unsigned node,
 {
 	union cvmx_pko_l3_l2_sqx_channel sqx_channel;
 	cvmx_pko_lutx_t lutx;
-	uint16_t xchan;
+	int xchan;
 
 	sqx_channel.u64 = cvmx_read_csr_node(node,
 		CVMX_PKO_L3_L2_SQX_CHANNEL(l2_l3_q_num));
@@ -386,9 +465,13 @@ void cvmx_pko3_map_channel(unsigned node,
 	/* Convert CHAN_E into compressed channel */
 	xchan =  cvmx_pko3_chan_2_xchan(channel);
 
-	if(xchan & 0xf000) {
-		cvmx_dprintf("%s: ERROR: channel %#x not recognized\n",
-			__func__, channel);
+	if (debug)
+		cvmx_dprintf("%s: ipd_port=%#x xchan=%#x\n", __func__, channel, xchan);
+
+	if(xchan < 0) {
+		if (xchan == -1)
+			cvmx_printf("%s: ERROR: channel %#x not recognized\n",
+				__func__, channel);
 		return;
 	}
 
@@ -412,22 +495,16 @@ void cvmx_pko3_map_channel(unsigned node,
  *
  * @param node is to specify the node to which this configuration is applied.
  * @param port_queue is the port queue number to be configured.
- * @param child_base is the first child queue number in the static prioriy childs.
- * @param child_rr_prio is the round robin childs priority.
  * @param mac_num is the mac number of the mac that will be tied to this port_queue.
  */
-static void cvmx_pko_configure_port_queue(int node, int port_queue,
-					 int child_base, int child_rr_prio,
-					 int mac_num)
+static void cvmx_pko_configure_port_queue(int node, int port_queue, int mac_num)
 {
 	cvmx_pko_l1_sqx_topology_t pko_l1_topology;
 	cvmx_pko_l1_sqx_shape_t pko_l1_shape;
 	cvmx_pko_l1_sqx_link_t pko_l1_link;
 
 	pko_l1_topology.u64 = 0;
-	pko_l1_topology.s.prio_anchor = child_base;
 	pko_l1_topology.s.link = mac_num;
-	pko_l1_topology.s.rr_prio = child_rr_prio;
 	cvmx_write_csr_node(node, CVMX_PKO_L1_SQX_TOPOLOGY(port_queue), pko_l1_topology.u64);
 
 	pko_l1_shape.u64 = 0;
@@ -445,17 +522,34 @@ static void cvmx_pko_configure_port_queue(int node, int port_queue,
  * in hardware.
  *
  * @param node is to specify the node to which this configuration is applied.
- * @param queue is the level2 queue number to be configured.
- * @param parent_queue is the parent queue at next level for this l2 queue.
+ * @param queue is the level3 queue number to be configured.
+ * @param parent_queue is the parent queue at next level for this l3 queue.
  * @param prio is this queue's priority in parent's scheduler.
  * @param rr_quantum is this queue's round robin quantum value.
- * @return returns none.
+ * @param child_base is the first child queue number in the static prioriy childs.
+ * @param child_rr_prio is the round robin childs priority.
  */
 static void cvmx_pko_configure_l2_queue(int node, int queue, int parent_queue,
-					       int prio, int rr_quantum)
+					       int prio, int rr_quantum,
+					       int child_base, int child_rr_prio)
 {
-	cvmx_pko_l2_sqx_schedule_t pko_sq_sched;
-	cvmx_pko_l2_sqx_topology_t pko_sq_topology;
+	cvmx_pko_l3_sqx_schedule_t pko_sq_sched;
+	cvmx_pko_l3_sqx_topology_t pko_child_topology;
+	cvmx_pko_l2_sqx_topology_t pko_parent_topology;
+
+	/* parent topology configuration */
+	pko_parent_topology.u64 = cvmx_read_csr_node(node,
+			CVMX_PKO_L1_SQX_TOPOLOGY(parent_queue));
+	pko_parent_topology.s.prio_anchor = child_base;
+	pko_parent_topology.s.rr_prio = child_rr_prio;
+	cvmx_write_csr_node(node,
+			CVMX_PKO_L1_SQX_TOPOLOGY(parent_queue),
+			pko_parent_topology.u64);
+
+	if (debug>1) cvmx_dprintf("CVMX_PKO_L1_SQX_TOPOLOGY(%u): "
+		"PRIO_ANCHOR=%u PARENT=%u\n",
+		parent_queue, pko_parent_topology.s.prio_anchor,
+		pko_parent_topology.s.parent);
 
 	/* scheduler configuration for this sq in the parent queue */
 	pko_sq_sched.u64 = 0;
@@ -463,13 +557,14 @@ static void cvmx_pko_configure_l2_queue(int node, int queue, int parent_queue,
 	pko_sq_sched.s.rr_quantum = rr_quantum;
 	cvmx_write_csr_node(node, CVMX_PKO_L2_SQX_SCHEDULE(queue), pko_sq_sched.u64);
 
-	/* topology configuration */
-	pko_sq_topology.u64 = 0;
-	pko_sq_topology.s.parent = parent_queue;
-	cvmx_write_csr_node(node, CVMX_PKO_L2_SQX_TOPOLOGY(queue), pko_sq_topology.u64);
+	/* child topology configuration */
+	pko_child_topology.u64 = 0;
+	pko_child_topology.s.parent = parent_queue;
+	cvmx_write_csr_node(node, CVMX_PKO_L2_SQX_TOPOLOGY(queue), pko_child_topology.u64);
 
 }
 
+
 /*
  * @INTERNAL
  * This function configures level 3 queues scheduling and topology parameters
@@ -633,22 +728,35 @@ static void cvmx_pko_configure_dq(int node, int dq, int parent_queue,
 	cvmx_pko_dqx_topology_t pko_dq_topology;
 	cvmx_pko_l5_sqx_topology_t pko_parent_topology;
 	cvmx_pko_dqx_wm_ctl_t pko_dq_wm_ctl;
+	unsigned long long parent_topology_reg;
+	char lvl;
 
 	if (debug)
 		cvmx_dprintf("%s: dq %u parent %u child_base %u\n",
 			     __func__, dq, parent_queue, child_base);
 
+	if (__cvmx_pko3_sq_lvl_max() == CVMX_PKO_L5_QUEUES) {
+		parent_topology_reg = CVMX_PKO_L5_SQX_TOPOLOGY(parent_queue);
+		lvl = 5;
+	} else if (__cvmx_pko3_sq_lvl_max() == CVMX_PKO_L3_QUEUES) {
+		parent_topology_reg = CVMX_PKO_L3_SQX_TOPOLOGY(parent_queue);
+		lvl = 3;
+	} else
+		return;
+
+	if (debug)
+		cvmx_dprintf("%s: parent_topology_reg=%#llx\n",
+			__func__, parent_topology_reg);
+
 	/* parent topology configuration */
-	pko_parent_topology.u64 = cvmx_read_csr_node(node,
-			CVMX_PKO_L5_SQX_TOPOLOGY(parent_queue));
+	pko_parent_topology.u64 = cvmx_read_csr_node(node, parent_topology_reg);
 	pko_parent_topology.s.prio_anchor = child_base;
 	pko_parent_topology.s.rr_prio = child_rr_prio;
-	cvmx_write_csr_node(node,
-			CVMX_PKO_L5_SQX_TOPOLOGY(parent_queue),
+	cvmx_write_csr_node(node, parent_topology_reg,
 			pko_parent_topology.u64);
 
-	if (debug>1) cvmx_dprintf("CVMX_PKO_L5_SQX_TOPOLOGY(%u): "
-		"PRIO_ANCHOR=%u PARENT=%u\n",
+	if (debug>1) cvmx_dprintf("CVMX_PKO_L%d_SQX_TOPOLOGY(%u): "
+		"PRIO_ANCHOR=%u PARENT=%u\n", lvl,
 		parent_queue, pko_parent_topology.s.prio_anchor,
 		pko_parent_topology.s.parent);
 
@@ -689,21 +797,42 @@ static void cvmx_pko_configure_dq(int node, int dq, int parent_queue,
  * The initial content of the table will be setup in accordance
  * to the specific SoC model and its implemented resources
  */
-static const struct {
-	unsigned sq_level_base,
-		sq_level_count;
-	/* 4 function pointers for L3 .. L6=DQ */
-	void (*cfg_sq_func[])(
+struct pko3_cfg_tab_s {
+	/* function pointer for to configure the given level, last=DQ */
+	struct {
+		uint8_t parent_level;
+		void (*cfg_sq_func)(
 			int node, int queue, int parent_queue,
 			int prio, int rr_quantum,
 			int child_base, int child_rr_prio);
-} __cvmx_pko3_sq_config_table = {
-	3, 4,
+	//XXX for debugging exagerated size
+	} lvl[256];
+};
+
+
+static const struct pko3_cfg_tab_s pko3_cn78xx_cfg = {
+	{
+	[CVMX_PKO_L2_QUEUES] =
+		{CVMX_PKO_PORT_QUEUES, cvmx_pko_configure_l2_queue },
+	[CVMX_PKO_L3_QUEUES] =
+		{CVMX_PKO_L2_QUEUES, cvmx_pko_configure_l3_queue },
+	[CVMX_PKO_L4_QUEUES] =
+		{CVMX_PKO_L3_QUEUES, cvmx_pko_configure_l4_queue },
+	[CVMX_PKO_L5_QUEUES] =
+		{CVMX_PKO_L4_QUEUES, cvmx_pko_configure_l5_queue },
+	[CVMX_PKO_DESCR_QUEUES] =
+		{CVMX_PKO_L5_QUEUES, cvmx_pko_configure_dq }
+	}
+};
+
+static const struct pko3_cfg_tab_s pko3_cn73xx_cfg = {
 	{
-	cvmx_pko_configure_l3_queue,
-	cvmx_pko_configure_l4_queue,
-	cvmx_pko_configure_l5_queue,
-	cvmx_pko_configure_dq
+	[CVMX_PKO_L2_QUEUES] =
+		{CVMX_PKO_PORT_QUEUES, cvmx_pko_configure_l2_queue },
+	[CVMX_PKO_L3_QUEUES] =
+		{CVMX_PKO_L2_QUEUES, cvmx_pko_configure_l3_queue },
+	[CVMX_PKO_DESCR_QUEUES] =
+		{CVMX_PKO_L3_QUEUES, cvmx_pko_configure_dq }
 	}
 };
 
@@ -717,85 +846,20 @@ static const struct {
  *
  * @param node on which to operate
  * @param mac_num is the LMAC number to that is associated with the Port Queue,
- * @param which is identical to the Port Queue number that is configured
- * @param child_base is the number of the first L2 SQ attached to the PQ
- * @param child_count is the number of L2 SQ children to attach to PQ
- * @param stat_prio_count is the priority setting for the children L2 SQs
- *
- * If <stat_prio_count> is -1, the L2 children will have equal Round-Robin
- * relationship with eachother. If <stat_prio_count> is 0, all L2 children
- * will be arranged in Weighted-Round-Robin, with the first having the most
- * precedence. If <stat_prio_count> is between 1 and 8, it indicates how
- * many children will have static priority settings (with the first having
- * the most precedence), with the remaining L2 children having WRR scheduling.
+ * @param pq_num is the number of the L1 PQ attached to the MAC
  *
  * @returns 0 on success, -1 on failure.
- *
- * Note: this function supports the configuration of node-local unit.
  */
-int cvmx_pko3_pq_config_children(unsigned node, unsigned mac_num,
-			 unsigned child_base,
-			unsigned child_count, int stat_prio_count)
+int cvmx_pko3_pq_config(unsigned node, unsigned mac_num, unsigned pq_num)
 {
-	unsigned pq_num;
-	unsigned rr_quantum, rr_count;
-	unsigned child, prio, rr_prio;
-
-	/* L1/PQ number is 1-to-1 from MAC number */
-	pq_num = mac_num;
-
-	/* First static priority is 0 - wuth the most precedence */
-	prio = 0;
-
-	if (stat_prio_count > (signed) child_count)
-		stat_prio_count = child_count;
-
-	/* Valid PRIO field is 0..9, limit maximum static priorities */
-	if (stat_prio_count > 9)
-		stat_prio_count = 9;
-
-	/* Special case of a single child */
-	if (child_count == 1) {
-		rr_count = 0;
-		rr_prio = 0xF;
-	/* Special case for Fair-RR */
-	} else if (stat_prio_count < 0) {
-		rr_count = child_count;
-		rr_prio = 0;
-	} else {
-		rr_count = child_count - stat_prio_count;
-		rr_prio = stat_prio_count;
-	}
-
-	/* Compute highest RR_QUANTUM */
-	if (stat_prio_count > 0)
-		rr_quantum = CVMX_PKO3_RR_QUANTUM_MIN * rr_count;
-	else
-		rr_quantum = CVMX_PKO3_RR_QUANTUM_MIN;
+	char b1[10];
 
 	if(debug)
-		cvmx_dprintf("%s: L1/PQ%u MAC%u child_base %u rr_pri %u\n",
-		__func__, pq_num, mac_num, child_base, rr_prio);
-
-	cvmx_pko_configure_port_queue(node,
-		pq_num, child_base, rr_prio, mac_num);
-
-
-	for(child = child_base; child < (child_base + child_count); child ++) {
-		if (debug)
-			cvmx_dprintf("%s: "
-				"L2/SQ%u->PQ%u prio %u rr_quantum %#x\n",
-				__func__,
-				child, pq_num, prio, rr_quantum);
+		cvmx_dprintf("%s: MAC%u -> %s\n",
+			__func__, mac_num,
+			 __cvmx_pko3_sq_str(b1, CVMX_PKO_PORT_QUEUES, pq_num));
 
-		cvmx_pko_configure_l2_queue(node,
-			child, pq_num, prio, rr_quantum);
-
-		if (prio < rr_prio)
-			prio ++;
-		else if (stat_prio_count > 0)
-			rr_quantum -= CVMX_PKO3_RR_QUANTUM_MIN;
-	} /* for child */
+	cvmx_pko_configure_port_queue(node, pq_num, mac_num);
 
 	return 0;
 }
@@ -811,7 +875,7 @@ int cvmx_pko3_pq_config_children(unsigned node, unsigned mac_num,
  * when multiple children are assigned a single parent.
  *
  * @param node on which to operate
- * @param parent_level is the level of the parent queue, 2 to 5.
+ * @param child_level  is the level of the child queue
  * @param parent_queue is the number of the parent Scheduler Queue
  * @param child_base is the number of the first child SQ or DQ to assign to
  * @param child_count is the number of consecutive children to assign
@@ -828,23 +892,41 @@ int cvmx_pko3_pq_config_children(unsigned node, unsigned mac_num,
  *
  * Note: this function supports the configuration of node-local unit.
  */
-int cvmx_pko3_sq_config_children(unsigned int node, unsigned parent_level,
+int cvmx_pko3_sq_config_children(unsigned int node,
+			enum cvmx_pko3_level_e child_level,
 			unsigned parent_queue, unsigned child_base,
 			unsigned child_count, int stat_prio_count)
 {
-	unsigned child_level;
+	enum cvmx_pko3_level_e parent_level;
+	unsigned num_elem = 0;
 	unsigned rr_quantum, rr_count;
 	unsigned child, prio, rr_prio;
-	unsigned func_idx;
+	const struct pko3_cfg_tab_s *cfg_tbl = NULL;
+	char b1[10], b2[10];
 
-	child_level = parent_level + 1;
+        if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		num_elem = NUM_ELEMENTS(pko3_cn78xx_cfg.lvl);
+		cfg_tbl = &pko3_cn78xx_cfg;
+	}
+        if (OCTEON_IS_MODEL(OCTEON_CN73XX) || OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+		num_elem = NUM_ELEMENTS(pko3_cn73xx_cfg.lvl);
+		cfg_tbl = &pko3_cn73xx_cfg;
+	}
 
-	if (child_level < __cvmx_pko3_sq_config_table.sq_level_base ||
-	    child_level >= __cvmx_pko3_sq_config_table.sq_level_base +
-			__cvmx_pko3_sq_config_table.sq_level_count)
+	if (cfg_tbl == NULL || child_level >= num_elem) {
+		cvmx_printf("ERROR: %s: model or level %#x invalid\n",
+			__func__, child_level);
 		return -1;
+	}
 
-	func_idx = child_level - __cvmx_pko3_sq_config_table.sq_level_base;
+	parent_level = cfg_tbl->lvl[child_level].parent_level;
+
+	if (cfg_tbl->lvl[child_level].cfg_sq_func == NULL ||
+	    cfg_tbl->lvl[child_level].parent_level == 0) {
+		cvmx_printf("ERROR: %s: queue level %#x invalid\n",
+			__func__, child_level);
+		return -1;
+	}
 
 	/* First static priority is 0 - top precedence */
 	prio = 0;
@@ -876,21 +958,22 @@ int cvmx_pko3_sq_config_children(unsigned int node, unsigned parent_level,
 		rr_quantum = CVMX_PKO3_RR_QUANTUM_MIN;
 
 	if(debug)
-		cvmx_dprintf("%s: Parent L%u/SQ%u child_base %u rr_pri %u\n",
-		__func__, parent_level, parent_queue, child_base, rr_prio);
+		cvmx_dprintf("%s: Parent %s child_base %u rr_pri %u\n",
+		__func__, __cvmx_pko3_sq_str(b1, parent_level, parent_queue),
+		child_base, rr_prio);
 
 	/* Parent is configured with child */
 
 	for(child = child_base; child < (child_base + child_count); child ++) {
 		if (debug)
 			cvmx_dprintf("%s: "
-				"L%u/SQ%u->L%u/SQ%u prio %u rr_quantum %#x\n",
-				__func__,
-				child_level, child,
-				parent_level, parent_queue,
-				prio, rr_quantum);
+			    "Child %s of %s prio %u rr_quantum %#x\n",
+			    __func__,
+			    __cvmx_pko3_sq_str(b1, child_level, child),
+			    __cvmx_pko3_sq_str(b2, parent_level, parent_queue),
+			    prio, rr_quantum);
 
-		__cvmx_pko3_sq_config_table.cfg_sq_func[func_idx](
+		cfg_tbl->lvl[child_level].cfg_sq_func(
 			node, child, parent_queue, prio, rr_quantum,
 			child_base, rr_prio);
 
@@ -957,14 +1040,13 @@ static int cvmx_pko3_shaper_rate_compute(unsigned long tclk,
 
 	/* Find the biggest divider that has the short float fit */
 	for (div_exp = 0; div_exp <= max_exp; div_exp++) {
-		tmp = (rate_tocks << div_exp) / tclk;
+		tmp = ((rate_tocks << div_exp) + (tclk / 2)) / tclk;
 		if (tmp > fmax) {
 			if (div_exp > 0)
 				div_exp --;
 			break;
 		}
 	}
-
 	/* Make sure divider, rate are within valid range */
 	if (div_exp > max_exp) {
 		/* Minimum reached */
@@ -974,7 +1056,6 @@ static int cvmx_pko3_shaper_rate_compute(unsigned long tclk,
 		if ((rate_tocks / tclk) > fmax)
 			rate_tocks = fmax * tclk;
 	}
-
 	/* Store common divider */
 	reg->s.rate_divider_exponent = div_exp;
 
@@ -983,12 +1064,8 @@ static int cvmx_pko3_shaper_rate_compute(unsigned long tclk,
 	 * should not be less than RATE/Tclk
 	 */
 
-	/* Find the minimum burst size needed for rate */
-	min_burst = (rate_tocks << div_exp) / tclk;
-
-	/* Override with minimum MTU (could become per-port cfg) */
-	if (min_burst < __pko3_min_mtu)
-		min_burst = __pko3_min_mtu;
+	/* Find the minimum burst size needed for rate (burst ~ 4x rate) */
+	min_burst = (rate_tocks << (div_exp + 4)) / tclk;
 
 	/* Apply the minimum */
 	if (burst_tocks < min_burst)
@@ -1004,16 +1081,15 @@ static int cvmx_pko3_shaper_rate_compute(unsigned long tclk,
 	tmp = (burst_tocks << 8);
 	CVMX_SHOFT_FROM_U64(tmp, mant, exp);
 	reg->s.burst_mantissa = mant;
-	reg->s.burst_exponent = exp - 8;
+	reg->s.burst_exponent = exp - 8 - 1;
 
 	if (debug)
 		cvmx_dprintf("%s: RATE=%llu BURST=%llu DIV_EXP=%d\n",
 			__func__,
 			CVMX_SHOFT_TO_U64(reg->s.rate_mantissa,
-					reg->s.rate_exponent),
+				reg->s.rate_exponent),
 			CVMX_SHOFT_TO_U64(reg->s.burst_mantissa,
-					reg->s.burst_exponent),
-			div_exp);
+				(reg->s.burst_exponent + 1)), div_exp);
 
 	/* Validate the resulting rate */
 	rate_v = CVMX_SHOFT_TO_U64(reg->s.rate_mantissa,
@@ -1024,7 +1100,7 @@ static int cvmx_pko3_shaper_rate_compute(unsigned long tclk,
 	rate_v >>= div_exp;
 
 	burst_v = CVMX_SHOFT_TO_U64(reg->s.burst_mantissa,
-				reg->s.burst_exponent);
+				(reg->s.burst_exponent + 1));
 	/* Convert in additional bytes as in argument */
 	burst_v = burst_v << (tock_bytes_exp);
 
@@ -1041,7 +1117,10 @@ static int cvmx_pko3_shaper_rate_compute(unsigned long tclk,
 			__func__, rate_v, burst_v);
 
 	rate_v = (rate_v * 1000000ULL) / rate_kbips;
-	burst_v = (burst_v * 1000000ULL) / burst_bytes;
+	if (burst_bytes > 0)
+		burst_v = (burst_v * 1000000ULL) / burst_bytes;
+	else
+		burst_v = 0;
 
 	if (debug)
 		cvmx_dprintf("%s: error rate=%llu burst=%llu ppm\n",
@@ -1073,11 +1152,11 @@ static int cvmx_pko3_shaper_rate_compute(unsigned long tclk,
  * is returned as a positive integer.
  */
 int cvmx_pko3_port_cir_set(unsigned node, unsigned pq_num,
-		unsigned long rate_kbips, unsigned burst_bytes)
+		unsigned long rate_kbips, unsigned burst_bytes, int adj_bytes)
 {
-	const unsigned time_wheel_turn = 96; /* S-Clock cycles */
 	unsigned long tclk;
 	cvmx_pko_l1_sqx_cir_t sqx_cir;
+	cvmx_pko_l1_sqx_shape_t shape;
 	int rc;
 
 	if (debug)
@@ -1088,31 +1167,29 @@ int cvmx_pko3_port_cir_set(unsigned node, unsigned pq_num,
 
 	/* When rate == 0, disable the shaper */
 	if( rate_kbips == 0ULL) {
-		/* Disable shaping */
 		sqx_cir.s.enable = 0;
-		cvmx_write_csr_node(node,
-			CVMX_PKO_L1_SQX_CIR(pq_num), sqx_cir.u64);
+		cvmx_write_csr_node(node, CVMX_PKO_L1_SQX_CIR(pq_num), sqx_cir.u64);
 		return 0;
 	}
-
 	/* Compute time-wheel frequency */
-	tclk = cvmx_clock_get_rate_node(node, CVMX_CLOCK_SCLK)/
-		time_wheel_turn;
+	tclk = cvmx_pko3_pq_tw_clock_rate_node(node);
 
 	/* Compute shaper values */
-	rc = cvmx_pko3_shaper_rate_compute(tclk, &sqx_cir,
-		rate_kbips, burst_bytes);
-
-	/* Refuse to set register if insane rates, 25% = 250,000 PPM  */
-	if (rc < 250000)
-		return rc;
-
-	/* Enable shaping */
-	sqx_cir.s.enable = 1;
+	rc = cvmx_pko3_shaper_rate_compute(tclk, &sqx_cir, rate_kbips, burst_bytes);
 
 	/* Apply new settings */
+	sqx_cir.s.enable = 1;
+	sqx_cir.s. rate_divider_exponent = sqx_cir.s. rate_divider_exponent;
+	sqx_cir.s. rate_mantissa  = sqx_cir.s. rate_mantissa;
+	sqx_cir.s. rate_exponent  = sqx_cir.s. rate_exponent;
+	sqx_cir.s. burst_mantissa = sqx_cir.s. burst_mantissa;
+	sqx_cir.s. burst_exponent = sqx_cir.s. burst_exponent - 1;
 	cvmx_write_csr_node(node, CVMX_PKO_L1_SQX_CIR(pq_num), sqx_cir.u64);
 
+	shape.u64 = cvmx_read_csr_node(node, CVMX_PKO_L1_SQX_SHAPE(pq_num));
+	shape.s.adjust = adj_bytes;
+	cvmx_write_csr_node(node, CVMX_PKO_L1_SQX_SHAPE(pq_num), shape.u64);
+
 	return rc;
 }
 
@@ -1134,14 +1211,12 @@ int cvmx_pko3_port_cir_set(unsigned node, unsigned pq_num,
 int cvmx_pko3_dq_cir_set(unsigned node, unsigned dq_num,
 		unsigned long rate_kbips, unsigned burst_bytes)
 {
-	const unsigned time_wheel_turn = 768; /* S-Clock cycles */
 	unsigned long tclk;
 	cvmx_pko_l1_sqx_cir_t sqx_cir;
 	cvmx_pko_dqx_cir_t dqx_cir;
 	int rc;
 
-	dq_num &= (1<<10)-1;
-
+	dq_num &= cvmx_pko3_num_level_queues(CVMX_PKO_DESCR_QUEUES) - 1;
 	if (debug)
 		cvmx_dprintf("%s: dq=%u rate=%lu kbps, burst=%u bytes\n",
 			__func__, dq_num, rate_kbips, burst_bytes);
@@ -1150,34 +1225,23 @@ int cvmx_pko3_dq_cir_set(unsigned node, unsigned dq_num,
 
 	/* When rate == 0, disable the shaper */
 	if( rate_kbips == 0ULL) {
-		/* Disable shaping */
 		dqx_cir.s.enable = 0;
-		cvmx_write_csr_node(node,
-			CVMX_PKO_DQX_CIR(dq_num), dqx_cir.u64);
+		cvmx_write_csr_node(node, CVMX_PKO_DQX_CIR(dq_num), dqx_cir.u64);
 		return 0;
 	}
-
 	/* Compute time-wheel frequency */
-	tclk = cvmx_clock_get_rate_node(node, CVMX_CLOCK_SCLK)/
-		time_wheel_turn;
+	tclk = cvmx_pko3_dq_tw_clock_rate_node(node);
 
 	/* Compute shaper values */
-	rc = cvmx_pko3_shaper_rate_compute(tclk, &sqx_cir,
-		rate_kbips, burst_bytes);
-
-	/* Refuse to set register if insane rates, 25% = 250,000 PPM  */
-	if (rc < 250000)
-		return rc;
+	rc = cvmx_pko3_shaper_rate_compute(tclk, &sqx_cir, rate_kbips, burst_bytes);
 
-	/* Enable shaping */
+	/* Apply new settings */
 	dqx_cir.s.enable = 1;
 	dqx_cir.s. rate_divider_exponent = sqx_cir.s. rate_divider_exponent;
 	dqx_cir.s. rate_mantissa  = sqx_cir.s. rate_mantissa;
 	dqx_cir.s. rate_exponent  = sqx_cir.s. rate_exponent;
 	dqx_cir.s. burst_mantissa = sqx_cir.s. burst_mantissa;
-	dqx_cir.s. burst_exponent = sqx_cir.s. burst_exponent ;
-
-	/* Apply new settings */
+	dqx_cir.s. burst_exponent = sqx_cir.s. burst_exponent - 1;
 	cvmx_write_csr_node(node, CVMX_PKO_DQX_CIR(dq_num), dqx_cir.u64);
 
 	return rc;
@@ -1201,13 +1265,12 @@ int cvmx_pko3_dq_cir_set(unsigned node, unsigned dq_num,
 int cvmx_pko3_dq_pir_set(unsigned node, unsigned dq_num,
 		unsigned long rate_kbips, unsigned burst_bytes)
 {
-	const unsigned time_wheel_turn = 768; /* S-Clock cycles */
 	unsigned long tclk;
 	cvmx_pko_l1_sqx_cir_t sqx_cir;
 	cvmx_pko_dqx_pir_t dqx_pir;
 	int rc;
 
-	dq_num &= (1<<10)-1;
+	dq_num &= cvmx_pko3_num_level_queues(CVMX_PKO_DESCR_QUEUES) - 1;
 	if (debug)
 		cvmx_dprintf("%s: dq=%u rate=%lu kbps, burst=%u bytes\n",
 			__func__, dq_num, rate_kbips, burst_bytes);
@@ -1216,34 +1279,23 @@ int cvmx_pko3_dq_pir_set(unsigned node, unsigned dq_num,
 
 	/* When rate == 0, disable the shaper */
 	if( rate_kbips == 0ULL) {
-		/* Disable shaping */
 		dqx_pir.s.enable = 0;
-		cvmx_write_csr_node(node,
-			CVMX_PKO_DQX_PIR(dq_num), dqx_pir.u64);
+		cvmx_write_csr_node(node, CVMX_PKO_DQX_PIR(dq_num), dqx_pir.u64);
 		return 0;
 	}
-
 	/* Compute time-wheel frequency */
-	tclk = cvmx_clock_get_rate_node(node, CVMX_CLOCK_SCLK)/
-		time_wheel_turn;
+	tclk = cvmx_pko3_dq_tw_clock_rate_node(node);
 
 	/* Compute shaper values */
-	rc = cvmx_pko3_shaper_rate_compute(tclk, &sqx_cir,
-		rate_kbips, burst_bytes);
+	rc = cvmx_pko3_shaper_rate_compute(tclk, &sqx_cir, rate_kbips, burst_bytes);
 
-	/* Refuse to set register if insane rates, 25% = 250,000 PPM  */
-	if (rc < 250000)
-		return rc;
-
-	/* Enable shaping */
+	/* Apply new settings */
 	dqx_pir.s.enable = 1;
 	dqx_pir.s. rate_divider_exponent = sqx_cir.s. rate_divider_exponent;
 	dqx_pir.s. rate_mantissa  = sqx_cir.s. rate_mantissa;
 	dqx_pir.s. rate_exponent  = sqx_cir.s. rate_exponent;
 	dqx_pir.s. burst_mantissa = sqx_cir.s. burst_mantissa;
-	dqx_pir.s. burst_exponent = sqx_cir.s. burst_exponent ;
-
-	/* Apply new settings */
+	dqx_pir.s. burst_exponent = sqx_cir.s. burst_exponent - 1;
 	cvmx_write_csr_node(node, CVMX_PKO_DQX_PIR(dq_num), dqx_pir.u64);
 
 	return rc;
@@ -1276,30 +1328,27 @@ void cvmx_pko3_dq_red(unsigned node, unsigned dq_num, red_action_t red_act,
 {
 	cvmx_pko_dqx_shape_t dqx_shape;
 
-	dq_num &= (1<<10)-1;
-
+	dq_num &= cvmx_pko3_num_level_queues(CVMX_PKO_DESCR_QUEUES) - 1;
 	dqx_shape.u64 = 0;
 
-        if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
 		if (len_adjust < 0)
 			len_adjust = 0;
 	}
-
-        dqx_shape.s.adjust = len_adjust;
+	dqx_shape.s.adjust = len_adjust;
 
 	switch(red_act) {
-		default:
-		case CVMX_PKO3_SHAPE_RED_STALL:
-			dqx_shape.s.red_algo = 0x0;
-			break;
-		case CVMX_PKO3_SHAPE_RED_DISCARD:
-			dqx_shape.s.red_algo = 0x3;
-			break;
-		case CVMX_PKO3_SHAPE_RED_PASS:
-			dqx_shape.s.red_algo = 0x1;
-			break;
-		}
-
+	default:
+	case CVMX_PKO3_SHAPE_RED_STALL:
+		dqx_shape.s.red_algo = 0x0;
+		break;
+	case CVMX_PKO3_SHAPE_RED_DISCARD:
+		dqx_shape.s.red_algo = 0x3;
+		break;
+	case CVMX_PKO3_SHAPE_RED_PASS:
+		dqx_shape.s.red_algo = 0x1;
+		break;
+	}
 	cvmx_write_csr_node(node, CVMX_PKO_DQX_SHAPE(dq_num), dqx_shape.u64);
 }
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko3-resources.c b/arch/mips/cavium-octeon/executive/cvmx-pko3-resources.c
index d3043af..ec4a47e 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko3-resources.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko3-resources.c
@@ -46,12 +46,14 @@
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <linux/module.h>
 #include <asm/octeon/cvmx.h>
+#include <asm/octeon/cvmx-bootmem.h>
 #include <asm/octeon/cvmx-pko3.h>
 #include "asm/octeon/cvmx-global-resources.h"
 #include <asm/octeon/cvmx-pko3-resources.h>
 #include "asm/octeon/cvmx-range.h"
 #else
 #include "cvmx.h"
+#include "cvmx-bootmem.h"
 #include "cvmx-pko3.h"
 #include "cvmx-global-resources.h"
 #include "cvmx-pko3-resources.h"
@@ -66,7 +68,13 @@
 #define CVMX_GR_TAG_PKO_DESCR_QUEUES(x)  cvmx_get_gr_tag('c','v','m','_','p','k','o','d','e','q','_',(x+'0'),'.','.','.','.')
 #define CVMX_GR_TAG_PKO_PORT_INDEX(x)  	 cvmx_get_gr_tag('c','v','m','_','p','k','o','p','i','d','_',(x+'0'),'.','.','.','.')
 
-const int cvmx_pko_num_queues_78XX[CVMX_PKO_NUM_QUEUE_LEVELS] = 
+/*
+ * @INRWENAL
+ * Per-DQ parameters, current and maximum queue depth counters
+ */
+CVMX_SHARED cvmx_pko3_dq_params_t  *__cvmx_pko3_dq_params[CVMX_MAX_NODES];
+
+static const short cvmx_pko_num_queues_78XX[256] = 
 {
 	[CVMX_PKO_PORT_QUEUES] = 32,
 	[CVMX_PKO_L2_QUEUES] = 512,
@@ -76,15 +84,47 @@ const int cvmx_pko_num_queues_78XX[CVMX_PKO_NUM_QUEUE_LEVELS] =
 	[CVMX_PKO_DESCR_QUEUES] = 1024
 };
 
-static inline int __cvmx_pko3_get_num_queues(int level)
+static const short cvmx_pko_num_queues_73XX[256] = 
 {
-	if(OCTEON_IS_MODEL(OCTEON_CN78XX))
-		return cvmx_pko_num_queues_78XX[level];
-	return -1;
+	[CVMX_PKO_PORT_QUEUES] = 16,
+	[CVMX_PKO_L2_QUEUES] = 256,
+	[CVMX_PKO_L3_QUEUES] = 256,
+	[CVMX_PKO_L4_QUEUES] = 0,
+	[CVMX_PKO_L5_QUEUES] = 0,
+	[CVMX_PKO_DESCR_QUEUES] = 256
+};
+
+int cvmx_pko3_num_level_queues(enum cvmx_pko3_level_e level)
+{
+	unsigned nq = 0, ne = 0;
+
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		ne = NUM_ELEMENTS(cvmx_pko_num_queues_78XX);
+		nq =  cvmx_pko_num_queues_78XX[level];
+	}
+	if (OCTEON_IS_MODEL(OCTEON_CN73XX) || OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+		ne = NUM_ELEMENTS(cvmx_pko_num_queues_73XX);
+		nq =  cvmx_pko_num_queues_73XX[level];
+	}
+
+	if (nq == 0 || level >= ne) {
+		cvmx_printf("ERROR: %s: queue level %#x invalid\n",
+			__func__, level);
+		return -1;
+	}
+
+	return nq;
 }
 
-static inline struct global_resource_tag __cvmx_pko_get_queues_resource_tag(int node, int queue_level)
+static inline struct global_resource_tag
+__cvmx_pko_get_queues_resource_tag(int node, enum cvmx_pko3_level_e queue_level)
 {
+	if (cvmx_pko3_num_level_queues(queue_level) == 0) {
+		cvmx_printf("ERROR: %s: queue level %#x invalid\n",
+				__func__, queue_level);
+		return CVMX_GR_TAG_INVALID;
+	}
+
 	switch(queue_level) {
 		case CVMX_PKO_PORT_QUEUES:
 			return CVMX_GR_TAG_PKO_PORT_QUEUES(node);
@@ -99,6 +139,8 @@ static inline struct global_resource_tag __cvmx_pko_get_queues_resource_tag(int
 		case CVMX_PKO_DESCR_QUEUES:
 			return CVMX_GR_TAG_PKO_DESCR_QUEUES(node);
 		default:
+			cvmx_printf("ERROR: %s: queue level %#x invalid\n",
+				__func__, queue_level);
 			return CVMX_GR_TAG_INVALID;
 	}
 }
@@ -148,7 +190,7 @@ int cvmx_pko_alloc_global_resource(struct global_resource_tag tag, int base_queu
 int cvmx_pko_alloc_queues(int node, int level, int owner, int base_queue, int num_queues)
 {
 	struct global_resource_tag tag = __cvmx_pko_get_queues_resource_tag(node, level);
-	int max_num_queues = __cvmx_pko3_get_num_queues(level);
+	int max_num_queues = cvmx_pko3_num_level_queues(level);
 
 	return cvmx_pko_alloc_global_resource(tag, base_queue, owner, num_queues, max_num_queues);
 }
@@ -170,4 +212,43 @@ int cvmx_pko_free_queues(int node, int level, int owner)
 }
 EXPORT_SYMBOL(cvmx_pko_free_queues);
 
+/**
+ * @INTERNAL
+ *
+ * Initialize the pointer to the descriptor queue parameter table.
+ * The table is one named block per node, and may be shared between
+ * applications.
+ */
+int __cvmx_pko3_dq_param_setup(unsigned node)
+{
+	cvmx_pko3_dq_params_t  *pParam;
+	char block_name[] = "cvmx_pko3_dq_param_0";
+	unsigned i;
+
+	pParam = __cvmx_pko3_dq_params[node];
+	if (pParam != NULL)
+		return 0;
+
+	/* Adjust block name with node# */
+	i = strlen(block_name);
+	block_name[i-1] += node;
+
+	/* Get number of descriptor queues for sizing the table */
+	i = cvmx_pko3_num_level_queues(CVMX_PKO_DESCR_QUEUES);
+
+	pParam = cvmx_bootmem_alloc_named_range_once(
+		/* size */
+		sizeof(cvmx_pko3_dq_params_t) * i,
+		/* min_addr, max_addr, align */
+		0ull, 0ull, sizeof(cvmx_pko3_dq_params_t),
+		block_name, NULL);
+
+	if (pParam == NULL)
+		return -1;
+
+	__cvmx_pko3_dq_params[node] = pParam;
+
+	return 0;
+}
+
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-pko3.c b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
index 4f1876d..83772a9 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-pko3.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-pko3.c
@@ -45,12 +45,14 @@
 #include <asm/octeon/cvmx-clock.h>
 #include <asm/octeon/cvmx-hwpko.h>
 #include <asm/octeon/cvmx-pko3.h>
+#include <asm/octeon/cvmx-pko3-resources.h>
 #include <asm/octeon/cvmx-helper-pko3.h>
 #else
 #include "cvmx.h"
 #include "cvmx-hwpko.h"	/* For legacy support */
-#include "cvmx-pko3.h"
 #include "cvmx-fpa3.h"
+#include "cvmx-pko3.h"
+#include "cvmx-pko3-resources.h"
 #include "cvmx-helper-pko3.h"
 #include <errno.h>
 #endif
@@ -67,8 +69,6 @@ static const bool __native_le = 1;
 	if(debug)		\
 	cvmx_dprintf("%s=%#llx\n",#reg,(long long)cvmx_read_csr_node(node,reg))
 
-CVMX_TLS int32_t __cvmx_pko3_dq_depth[1024];
-
 static int cvmx_pko_setup_macs(int node);
 
 /*
@@ -131,18 +131,23 @@ int cvmx_pko3_hw_init_global(int node, uint16_t aura)
 	cvmx_pko_enable_t pko_enable;
 	cvmx_pko_dpfi_status_t dpfi_status;
 	cvmx_pko_status_t pko_status;
+	cvmx_pko_shaper_cfg_t shaper_cfg;
 	uint64_t cycles;
 	const unsigned timeout = 100;	/* 100 milliseconds */
 
 	if (node != (aura >> 10))
-		cvmx_dprintf("WARNING: AURA vs PKO node mismatch\n");
+		cvmx_printf("WARNING: AURA vs PKO node mismatch\n");
 
 	pko_enable.u64 = cvmx_read_csr_node(node, CVMX_PKO_ENABLE);
 	if (pko_enable.s.enable) {
-		cvmx_dprintf("WARNING: %s: PKO already enabled on node %u\n",
+		cvmx_printf("WARNING: %s: PKO already enabled on node %u\n",
 			__func__, node);
 		return 0;
 	}
+	/* Enable color awareness. */
+	shaper_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKO_SHAPER_CFG);
+	shaper_cfg.s.color_aware = 1;
+	cvmx_write_csr_node(node, CVMX_PKO_SHAPER_CFG, shaper_cfg.u64);
 
 	/* Clear FLUSH command to be sure */
 	pko_flush.u64 = 0;
@@ -174,7 +179,7 @@ int cvmx_pko3_hw_init_global(int node, uint16_t aura)
 
 	if (!pko_status.s.pko_rdy) {
 		dpfi_status.u64 = cvmx_read_csr_node(node, CVMX_PKO_DPFI_STATUS);
-		cvmx_dprintf("ERROR: %s: PKO DFPI failed, "
+		cvmx_printf("ERROR: %s: PKO DFPI failed, "
 			"PKO_STATUS=%#llx DPFI_STATUS=%#llx\n", __func__,
 			(unsigned long long) pko_status.u64,
 			(unsigned long long) dpfi_status.u64);
@@ -204,7 +209,7 @@ int cvmx_pko3_hw_init_global(int node, uint16_t aura)
 	if (pko_status.s.pko_rdy)
 		return 0;
 
-	cvmx_dprintf("ERROR: %s: failed, PKO_STATUS=%#llx\n", __func__,
+	cvmx_printf("ERROR: %s: failed, PKO_STATUS=%#llx\n", __func__,
 		(unsigned long long) pko_status.u64);
 	return -1;
 }
@@ -222,11 +227,12 @@ int cvmx_pko3_hw_disable(int node)
 	uint64_t cycles;
 	const unsigned timeout = 10;	/* 10 milliseconds */
 	unsigned mac_num, fifo, i;
+	unsigned null_mac_num, null_fifo_num, fifo_grp_count, pq_count;
 
 	(void) pko_status;
 
 	/* Wait until there are no in-flight packets */
-	for(i = mac_num = 0; mac_num < CVMX_PKO_MAX_MACS; mac_num++) {
+	for(i = mac_num = 0; mac_num < __cvmx_pko3_num_macs(); mac_num++) {
 		cvmx_pko_ptfx_status_t ptf_status;
 		ptf_status.u64 =
 			cvmx_read_csr_node(node, CVMX_PKO_PTFX_STATUS(mac_num));
@@ -238,51 +244,63 @@ int cvmx_pko3_hw_disable(int node)
 		if (ptf_status.s.mac_num == 0x1f)
 			continue;
 		if (ptf_status.s.in_flight_cnt != 0) {
-			cvmx_dprintf("%s: MAC %d in-flight %d\n",
+			cvmx_printf("WARNING: %s: MAC %d in-flight %d\n",
 				__func__, mac_num, ptf_status.s.in_flight_cnt);
 			mac_num --;
 			cvmx_wait(1000);
 		}
 	}
 
-	//XXX- try to disable PKO first, then flush the DPFI
 	/* disable PKO - all packets should be out by now */
 	pko_enable.u64 = 0;
 	pko_enable.s.enable = 0;
 	cvmx_write_csr_node(node, CVMX_PKO_ENABLE, pko_enable.u64);
 
-	/* Reset L1_SQ */
-	for(i = 0; i < 32; i++) {
+	/* Assign NULL MAC# for L1/SQ disabled state */
+        if(OCTEON_IS_MODEL(OCTEON_CN73XX) || OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+		null_mac_num = 0x0f;
+		null_fifo_num = 0x1f;
+		fifo_grp_count = 4;
+		pq_count = 16;
+	} else {
+		null_mac_num = 0x1c;
+		null_fifo_num = 0x1f;
+		fifo_grp_count = 8;
+		pq_count = 32;
+	}
+
+	/* Reset L1_PQ */
+	for(i = 0; i < pq_count; i++) {
 		cvmx_pko_l1_sqx_topology_t pko_l1_topology;
 		cvmx_pko_l1_sqx_shape_t pko_l1_shape;
 		cvmx_pko_l1_sqx_link_t pko_l1_link;
 		pko_l1_topology.u64 = 0;
-		pko_l1_topology.s.link = 0x1c;
+		pko_l1_topology.s.link = null_mac_num;
 		cvmx_write_csr_node(node, CVMX_PKO_L1_SQX_TOPOLOGY(i),
 			pko_l1_topology.u64);
 
 		pko_l1_shape.u64 = 0;
-		pko_l1_shape.s.link = 0x1c;
+		pko_l1_shape.s.link = null_mac_num;
 		cvmx_write_csr_node(node, CVMX_PKO_L1_SQX_SHAPE(i), pko_l1_shape.u64);
 
 		pko_l1_link.u64 = 0;
-		pko_l1_link.s.link = 0x1c;
+		pko_l1_link.s.link = null_mac_num;
 		cvmx_write_csr_node(node, CVMX_PKO_L1_SQX_LINK(i), pko_l1_link.u64);
 
 	}
 
 	/* Reset all MAC configurations */
-	for(mac_num = 0; mac_num < CVMX_PKO_MAX_MACS; mac_num++) {
+	for(mac_num = 0; mac_num < __cvmx_pko3_num_macs(); mac_num++) {
 		cvmx_pko_macx_cfg_t pko_mac_cfg;
 
 		pko_mac_cfg.u64 = 0;
-		pko_mac_cfg.s.fifo_num = 0x1f;
+		pko_mac_cfg.s.fifo_num = null_fifo_num;
 		cvmx_write_csr_node(node, CVMX_PKO_MACX_CFG(mac_num),
 			pko_mac_cfg.u64);
 	}
 
 	/* Reset all FIFO groups */
-	for(fifo = 0; fifo < 8; fifo++) {
+	for(fifo = 0; fifo < fifo_grp_count; fifo++) {
 		cvmx_pko_ptgfx_cfg_t pko_ptgfx_cfg;
 
 		pko_ptgfx_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKO_PTGFX_CFG(fifo));
@@ -326,7 +344,7 @@ int cvmx_pko3_hw_disable(int node)
 	CVMX_DUMP_REGX(CVMX_PKO_DPFI_FLUSH);
 
 	if (dpfi_status.s.cache_flushed == 0) {
-		cvmx_dprintf("%s: ERROR: timeout waiting for PKO3 ptr flush\n",
+		cvmx_printf("%s: ERROR: timeout waiting for PKO3 ptr flush\n",
 			__FUNCTION__);
 		return -1;
 	}
@@ -334,6 +352,32 @@ int cvmx_pko3_hw_disable(int node)
 	return 0;
 }
 
+/*
+ * Configure Channel credit level in PKO.
+ *
+ * @param node is to specify the node to which this configuration is applied.
+ * @param level specifies the level at which pko channel queues will be configured,
+ * @return returns 0 if successful and -1 on failure.
+ */
+int cvmx_pko3_channel_credit_level(int node, enum cvmx_pko3_level_e level)
+{
+	union cvmx_pko_channel_level channel_level;
+
+	channel_level.u64 = 0;
+
+	if (level == CVMX_PKO_L2_QUEUES)
+		channel_level.s.cc_level = 0;
+	else if (level == CVMX_PKO_L3_QUEUES)
+		channel_level.s.cc_level = 1;
+	else
+		return -1;
+
+	cvmx_write_csr_node(node, CVMX_PKO_CHANNEL_LEVEL, channel_level.u64);
+
+	return 0;
+
+}
+
 /** Open configured descriptor queues before queueing packets into them.
  *
  * @param node is to specify the node to which this configuration is applied.
@@ -344,10 +388,13 @@ int cvmx_pko_dq_open(int node, int dq)
 {
 	cvmx_pko_query_rtn_t pko_status;
 	pko_query_dqstatus_t dqstatus;
+	cvmx_pko3_dq_params_t *pParam;
 
 	if(debug)
 		cvmx_dprintf("%s: DEBUG: dq %u\n", __FUNCTION__, dq);
 
+	__cvmx_pko3_dq_param_setup(node);
+
 	pko_status = __cvmx_pko3_do_dma(node, dq, NULL, 0, CVMX_PKO_DQ_OPEN);
 
 	dqstatus = pko_status.s.dqstatus;
@@ -355,12 +402,20 @@ int cvmx_pko_dq_open(int node, int dq)
 	if (dqstatus == PKO_DQSTATUS_ALREADY)
 		return 0;
 	if (dqstatus != PKO_DQSTATUS_PASS) {
-		cvmx_dprintf("%s: ERROR: Failed to open dq :%u: %s\n",
+		cvmx_printf("%s: ERROR: Failed to open dq :%u: %s\n",
 				__FUNCTION__, dq,
 				pko_dqstatus_error(dqstatus));
 		return -1;
 	}
 
+	/* Setup the descriptor queue software parameters */
+	pParam = cvmx_pko3_dq_parameters(node, dq);
+	if (pParam != NULL) {
+		pParam->depth = pko_status.s.depth;
+		if (pParam->limit == 0)
+			pParam->limit = 1024;	/* last-resort default */
+	}
+
 	return 0;
 }
 
@@ -391,7 +446,7 @@ int cvmx_pko3_dq_close(int node, int dq)
 		return 0;
 
 	if (dqstatus != PKO_DQSTATUS_PASS) {
-		cvmx_dprintf("WARNING: %s: Failed to close dq :%u: %s\n",
+		cvmx_printf("WARNING: %s: Failed to close dq :%u: %s\n",
 				__FUNCTION__, dq,
 				pko_dqstatus_error(dqstatus));
 		cvmx_dprintf("DEBUG: %s: dq %u depth %u\n",
@@ -448,7 +503,7 @@ int cvmx_pko3_dq_query(int node, int dq)
 	dqstatus = pko_status.s.dqstatus;
 
 	if (dqstatus != PKO_DQSTATUS_PASS) {
-		cvmx_dprintf("%s: ERROR: Failed to query dq :%u: %s\n",
+		cvmx_printf("%s: ERROR: Failed to query dq :%u: %s\n",
 				__FUNCTION__, dq,
 				pko_dqstatus_error(dqstatus));
 		return -1;
@@ -471,6 +526,8 @@ int cvmx_pko3_dq_query(int node, int dq)
  *
  * @param node is to specify which node's pko block for this setup.
  * @return returns 0 if successful and -1 on failure.
+ *
+ * Note: This function contains model-specific code.
  */
 static int cvmx_pko_setup_macs(int node)
 {
@@ -482,6 +539,7 @@ static int cvmx_pko_setup_macs(int node)
 	uint8_t fifo_group_cfg[8];
 	uint8_t fifo_group_spd[8];
 	unsigned fifo_count = 0;
+	unsigned max_fifos = 0, fifo_groups = 0;
 	struct {
 		cvmx_helper_interface_mode_t mac_mode;
 		uint8_t fifo_cnt;
@@ -489,7 +547,16 @@ static int cvmx_pko_setup_macs(int node)
 		uint8_t pri;
 		uint8_t spd;
 		uint8_t mac_fifo_cnt;
-	} cvmx_pko3_mac_table[CVMX_PKO_MAX_MACS];
+	} cvmx_pko3_mac_table[32];
+
+        if(OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		max_fifos = 28;	/* exclusive of NULL FIFO */
+		fifo_groups = 8;/* inclusive of NULL PTGF */
+	}
+        if(OCTEON_IS_MODEL(OCTEON_CN73XX) || OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+		max_fifos = 16;
+		fifo_groups = 5;
+	}
 
 	/* Initialize FIFO allocation table */
 	memset(&fifo_group_cfg, 0, sizeof(fifo_group_cfg));
@@ -497,7 +564,7 @@ static int cvmx_pko_setup_macs(int node)
 	memset(cvmx_pko3_mac_table, 0, sizeof(cvmx_pko3_mac_table));
 
 	/* Initialize all MACs as disabled */
-	for(mac_num = 0; mac_num < CVMX_PKO_MAX_MACS; mac_num++) {
+	for(mac_num = 0; mac_num < __cvmx_pko3_num_macs(); mac_num++) {
 		cvmx_pko3_mac_table[mac_num].mac_mode =
 			CVMX_HELPER_INTERFACE_MODE_DISABLED;
 		cvmx_pko3_mac_table[mac_num].pri = 0;
@@ -525,9 +592,11 @@ static int cvmx_pko_setup_macs(int node)
 
 			/* convert interface/port to mac number */
 			i = __cvmx_pko3_get_mac_num(xiface, port);
-			if (i < 0 || i>= CVMX_PKO_MAX_MACS) {
-				cvmx_dprintf("%s: ERROR: interface %d:%u port %d has no MAC\n",
-					     __func__, node, interface, port);
+			if (i < 0 || i >= (int) __cvmx_pko3_num_macs()) {
+				cvmx_printf("%s: ERROR: interface %d:%u "
+				    "port %d has no MAC %d/%d\n",
+				    __func__, node, interface, port,
+				    i, __cvmx_pko3_num_macs());
 				continue;
 			}
 
@@ -553,10 +622,11 @@ static int cvmx_pko_setup_macs(int node)
 				cvmx_pko3_mac_table[i].pri = 4;
 				cvmx_pko3_mac_table[i].spd = 40;
 				cvmx_pko3_mac_table[i].mac_fifo_cnt = 4;
-			} else if (mode == CVMX_HELPER_INTERFACE_MODE_ILK) {
+			} else if (mode == CVMX_HELPER_INTERFACE_MODE_ILK ||
+				mode == CVMX_HELPER_INTERFACE_MODE_SRIO) {
 				cvmx_pko3_mac_table[i].fifo_cnt = 4;
 				cvmx_pko3_mac_table[i].pri = 3;
-				/* ILK: 40 Gbps or 20 Gbps */
+				/* ILK/SRIO: speed depends on lane count */
 				cvmx_pko3_mac_table[i].spd = 40;
 				cvmx_pko3_mac_table[i].mac_fifo_cnt = 4;
 			} else {
@@ -577,7 +647,7 @@ static int cvmx_pko_setup_macs(int node)
 	} /* for interface */
 
 	/* Count the number of requested FIFOs */
-	for(fifo_count = mac_num = 0; mac_num < CVMX_PKO_MAX_MACS; mac_num ++)
+	for(fifo_count = mac_num = 0; mac_num < __cvmx_pko3_num_macs(); mac_num ++)
 		fifo_count += cvmx_pko3_mac_table[mac_num].fifo_cnt;
 
 	if(debug)
@@ -586,15 +656,15 @@ static int cvmx_pko_setup_macs(int node)
 
 	/* Heuristically trim FIFO count to fit in available number */
 	pri = 1; cnt = 4;
-	while(fifo_count > 28) {
-		for(mac_num=0; mac_num < CVMX_PKO_MAX_MACS; mac_num ++) {
+	while(fifo_count > max_fifos) {
+		for(mac_num=0; mac_num < __cvmx_pko3_num_macs(); mac_num ++) {
 			if (cvmx_pko3_mac_table[mac_num].fifo_cnt == cnt &&
 			    cvmx_pko3_mac_table[mac_num].pri <= pri) {
 				cvmx_pko3_mac_table[mac_num].fifo_cnt >>= 1;
 				fifo_count -=
 					cvmx_pko3_mac_table[mac_num].fifo_cnt;
 			}
-			if (fifo_count <= 28)
+			if (fifo_count <= max_fifos)
 				break;
 		}
 		if (pri >= 4) {
@@ -612,13 +682,13 @@ static int cvmx_pko_setup_macs(int node)
 
 
 	/* Special case for NULL Virtual FIFO */
-	fifo_group_cfg[28 >> 2] = 0;
+	fifo_group_cfg[fifo_groups-1] = 0;
 	/* there is no MAC connected to NULL FIFO */
 
 	/* Configure MAC units, and attach a FIFO to each */
 	for(fifo = 0, cnt = 4; cnt > 0; cnt >>= 1 ) {
 		unsigned g;
-		for(mac_num = 0; mac_num < CVMX_PKO_MAX_MACS; mac_num++) {
+		for(mac_num = 0; mac_num < __cvmx_pko3_num_macs(); mac_num++) {
 			if(cvmx_pko3_mac_table[mac_num].fifo_cnt < cnt ||
 			  cvmx_pko3_mac_table[mac_num].fifo_id != 0x1f)
 				continue;
@@ -643,16 +713,16 @@ static int cvmx_pko_setup_macs(int node)
 			else if (cnt == 1)
 				fifo_group_cfg[g] = 0; /* 2.5k x 4 */
 			else
-				cvmx_dprintf("%s: internal error\n",__func__);
+				cvmx_printf("ERROR: %s: internal error\n",__func__);
 
 			fifo += cnt;
 		}
 	}
 
 	/* Check if there was no error in FIFO allocation */
-	if( fifo > 28 ){
-		cvmx_dprintf("%s: ERROR: Internal error FIFO %u\n",
-			__FUNCTION__, fifo);
+	if (fifo > max_fifos) {
+		cvmx_printf("ERROR: %s: Internal error FIFO %u\n",
+			__func__, fifo);
 		return -1;
 	}
 
@@ -661,7 +731,7 @@ static int cvmx_pko_setup_macs(int node)
 			__FUNCTION__, fifo);
 
 	/* Now configure all FIFO groups */
-	for(fifo = 0; fifo < 8; fifo++) {
+	for(fifo = 0; fifo < fifo_groups; fifo++) {
 		cvmx_pko_ptgfx_cfg_t pko_ptgfx_cfg;
 
 		pko_ptgfx_cfg.u64 =
@@ -693,8 +763,8 @@ static int cvmx_pko_setup_macs(int node)
 					pko_ptgfx_cfg.u64);
 	}
 
-	/* Configure all 28 MACs assigned FIFO number */
-	for(mac_num = 0; mac_num < CVMX_PKO_MAX_MACS; mac_num++) {
+	/* Configure all MACs assigned FIFO number */
+	for(mac_num = 0; mac_num < __cvmx_pko3_num_macs(); mac_num++) {
 		cvmx_pko_macx_cfg_t pko_mac_cfg;
 
 		if(debug)
@@ -712,7 +782,7 @@ static int cvmx_pko_setup_macs(int node)
 	}
 
 	/* Setup PKO MCI0/MCI1/SKID credits */
-	for(mac_num = 0; mac_num < CVMX_PKO_MAX_MACS; mac_num++) {
+	for(mac_num = 0; mac_num < __cvmx_pko3_num_macs(); mac_num++) {
 		cvmx_pko_mci0_max_credx_t pko_mci0_max_cred;
 		cvmx_pko_mci1_max_credx_t pko_mci1_max_cred;
 		cvmx_pko_macx_cfg_t pko_mac_cfg;
@@ -734,28 +804,32 @@ static int cvmx_pko_setup_macs(int node)
 
 		fifo_size = (2 * 1024) + (1024 / 2); /* 2.5KiB */
 		fifo_credit = pko_fifo_cnt * fifo_size;
+		
+		if (mac_num == 0) {
+			/* loopback */
+			mac_credit = 4096; /* From HRM Sec 13.0 */
+			skid_credit = 0;
+		} else if (mac_num == 1) {
+			 /* DPI */
+			mac_credit = 2 * 1024;
+			skid_credit = 0;
+		} else if (octeon_has_feature(OCTEON_FEATURE_ILK) &&
+			(mac_num & 0xfe) == 2) {
+			/* ILK0, ILK1: MAC 2,3 */
+			mac_credit = 4 * 1024; /* 4KB fifo */
+			skid_credit = 0;
+		} else if (octeon_has_feature(OCTEON_FEATURE_SRIO) &&
+			(mac_num >= 6) && (mac_num <= 9)) {
+			/* SRIO0, SRIO1: MAC 6..9 */
+			mac_credit = 1024 / 2;
+			skid_credit = 0;
+		} else {
+			/* BGX */
+			mac_credit = mac_fifo_cnt * 8 * 1024;
+			skid_credit = mac_fifo_cnt * 256;
+		}
 
-		switch (mac_num) {
-			case 0: /* loopback */
-				mac_credit = 4096; /* From HRM Sec 13.0 */
-				skid_credit = 0;
-				break;
-			case 1: /* DPI */
-				mac_credit = 2 * 1024;
-				skid_credit = 0;
-				break;
-			case 2: /* ILK0 */
-			case 3: /* ILK1 */
-				mac_credit = 4 * 1024; /* 4KB fifo */
-				skid_credit = 0;
-				break;
-			default: /* BGX */
-				mac_credit = mac_fifo_cnt * 8 * 1024;
-				skid_credit = mac_fifo_cnt * 256;
-				break;
-		} /* switch mac_num */
-
-		if(debug) cvmx_dprintf(
+		if (debug) cvmx_dprintf(
 			"%s: mac %u "
 			"pko_fifo_credit=%u mac_credit=%u\n",
 			__FUNCTION__, mac_num, fifo_credit, mac_credit);
@@ -766,7 +840,7 @@ static int cvmx_pko_setup_macs(int node)
 
 		/* Check for overflow */
 		if (pko_mci0_max_cred.s.max_cred_lim != tmp) {
-			cvmx_dprintf("%s: MCI0 credit overflow\n",__FUNCTION__);
+			cvmx_printf("WARNING: %s: MCI0 credit overflow\n",__func__);
 			pko_mci0_max_cred.s.max_cred_lim = 0xfff;
 		}
 
@@ -783,7 +857,7 @@ static int cvmx_pko_setup_macs(int node)
 
 		/* Check for overflow */
 		if (pko_mci1_max_cred.s.max_cred_lim != tmp) {
-			cvmx_dprintf("%s: MCI1 credit overflow\n",__FUNCTION__);
+			cvmx_printf("WARNING: %s: MCI1 credit overflow\n",__func__);
 			pko_mci1_max_cred.s.max_cred_lim = 0xfff;
 		}
 
@@ -880,7 +954,7 @@ int cvmx_pko3_interface_options(int xiface, int index,
 
 	mac_num = __cvmx_pko3_get_mac_num(xiface, index);
 	if(mac_num < 0) {
-		cvmx_dprintf("ERROR: %s: invalid interface %u:%u/%u\n",
+		cvmx_printf("ERROR: %s: invalid interface %u:%u/%u\n",
 			__func__, xi.node, xi.interface, index);
 		return -1;
 	}
@@ -889,7 +963,7 @@ int cvmx_pko3_interface_options(int xiface, int index,
 
 	/* If MAC is not assigned, return an error */
 	if (pko_mac_cfg.s.fifo_num == 0x1f) {
-		cvmx_dprintf("ERROR: %s: unused interface %u:%u/%u\n",
+		cvmx_printf("ERROR: %s: unused interface %u:%u/%u\n",
 			__func__, xi.node, xi.interface, index);
 		return -1;
 	}
@@ -1070,6 +1144,165 @@ cvmx_pko3_port_fifo_size(unsigned int xiface, unsigned index)
 }
 EXPORT_SYMBOL(cvmx_pko3_port_fifo_size);
 
+/**
+ * @INTERNAL
+ *
+ * Stop an interface port transmission and wait until its FIFO is emopty.
+ *
+ */
+int cvmx_pko3_port_xoff(unsigned int xiface, unsigned index)
+{
+	cvmx_pko_l1_sqx_topology_t pko_l1_topology;
+	cvmx_pko_l1_sqx_sw_xoff_t pko_l1_xoff;
+	cvmx_pko_ptfx_status_t pko_ptfx_status;
+	cvmx_pko_macx_cfg_t pko_mac_cfg;
+	cvmx_pko_mci1_cred_cntx_t cred_cnt;
+	unsigned node, pq, num_pq, mac_num, fifo_num;
+	int ret;
+	cvmx_xiface_t xi = cvmx_helper_xiface_to_node_interface(xiface);
+
+	node = xi.node;
+	ret = __cvmx_pko3_get_mac_num(xiface, index);
+
+	if (debug)
+		cvmx_dprintf("%s: iface=%u:%u/%u mac %d\n",
+			__func__, xi.node, xi.interface, index, ret);
+
+	if (ret < 0)
+		return ret;
+
+	mac_num = ret;
+
+	if (mac_num == 0x1f)
+		return 0;
+
+	pko_mac_cfg.u64 = cvmx_read_csr_node(node, CVMX_PKO_MACX_CFG(mac_num));
+	fifo_num = pko_mac_cfg.s.fifo_num;
+
+	/* Verify the FIFO number is correct */
+	pko_ptfx_status.u64 = cvmx_read_csr_node(node,
+			CVMX_PKO_PTFX_STATUS(fifo_num));
+
+	if (debug)
+		cvmx_dprintf("%s: mac %d fifo %d, fifo mac %d\n",
+		__func__, mac_num, fifo_num, pko_ptfx_status.s.mac_num);
+
+	cvmx_warn_if (pko_ptfx_status.s.mac_num != mac_num,
+		"PKO3 FIFO number does not match MAC\n");
+
+	num_pq = cvmx_pko3_num_level_queues(CVMX_PKO_PORT_QUEUES);
+	/* Fint the L1/PQ connected to the MAC for this interface */
+	for (pq = 0; pq < num_pq; pq ++) {
+		pko_l1_topology.u64 = cvmx_read_csr_node(node,
+			CVMX_PKO_L1_SQX_TOPOLOGY(pq));
+		if (pko_l1_topology.s.link == mac_num)
+			break;
+	}
+
+
+	if (debug)
+		cvmx_dprintf("%s: L1_PQ%u LINK %d MAC_NUM %d\n",
+			__func__, pq, pko_l1_topology.s.link, mac_num);
+
+	if (pq >= num_pq)
+		return -1;
+
+	if (debug) {
+		pko_ptfx_status.u64 = cvmx_read_csr_node(node,
+			CVMX_PKO_PTFX_STATUS(fifo_num));
+		ret = pko_ptfx_status.s.in_flight_cnt;
+		cvmx_dprintf("%s: FIFO %d in-flight %d packets\n",
+		    __func__, fifo_num, ret);
+	}
+
+	/* Turn the XOFF bit on */
+	pko_l1_xoff.u64 = cvmx_read_csr_node(node,
+		CVMX_PKO_L1_SQX_SW_XOFF(pq));
+	pko_l1_xoff.s.xoff = 1;
+	cvmx_write_csr_node(node,
+		CVMX_PKO_L1_SQX_SW_XOFF(pq), pko_l1_xoff.u64);
+
+	ret = 1 << 22;
+	/* Wait for PKO TX FIFO to drain */
+	do {
+		CVMX_SYNC;
+		pko_ptfx_status.u64 = cvmx_read_csr_node(node,
+			CVMX_PKO_PTFX_STATUS(fifo_num));
+	} while (pko_ptfx_status.s.in_flight_cnt != 0 && ret--);
+
+	if (pko_ptfx_status.s.in_flight_cnt != 0) {
+		cvmx_warn("%s: FIFO %d failed to drain\n",
+		    __func__, fifo_num);
+	}
+
+	if (debug)
+		cvmx_dprintf("%s: FIFO %d drained in %d cycles\n",
+		    __func__, fifo_num, (1 << 22) - ret);
+
+	/* Wait for MAC TX FIFO to drain. */
+	do {
+		cred_cnt.u64 = cvmx_read_csr_node(node, CVMX_PKO_MCI1_CRED_CNTX(mac_num));
+	} while (cred_cnt.s.cred_cnt != 0);
+
+	return 0;
+}
+
+/**
+ * @INTERNAL
+ *
+ * Resume transmission on an interface port.
+ *
+ */
+int cvmx_pko3_port_xon(unsigned int xiface, unsigned index)
+{
+	cvmx_pko_l1_sqx_topology_t pko_l1_topology;
+	cvmx_pko_l1_sqx_sw_xoff_t pko_l1_xoff;
+	unsigned node, pq, num_pq, mac_num;
+	int ret;
+        cvmx_xiface_t xi = cvmx_helper_xiface_to_node_interface(xiface);
+
+	num_pq = cvmx_pko3_num_level_queues(CVMX_PKO_PORT_QUEUES);
+	node = xi.node;
+	ret = __cvmx_pko3_get_mac_num(xiface, index);
+
+	if (debug)
+		cvmx_dprintf("%s: iface=%u:%u/%u mac %d\n",
+			__func__, xi.node, xi.interface, index, ret);
+
+	if (ret < 0)
+		return ret;
+
+	mac_num = ret;
+
+	if (mac_num == 0x1f)
+		return 0;
+
+	/* Fint the L1/PQ connected to the MAC for this interface */
+	for (pq = 0; pq < num_pq; pq ++) {
+		pko_l1_topology.u64 = cvmx_read_csr_node(node,
+			CVMX_PKO_L1_SQX_TOPOLOGY(pq));
+		if (pko_l1_topology.s.link == mac_num)
+			break;
+	}
+
+	if (debug)
+		cvmx_dprintf("%s: L1_PQ%u LINK %d MAC_NUM %d\n",
+			__func__, pq, pko_l1_topology.s.link, mac_num);
+
+	if (pq >= num_pq)
+		return -1;
+
+	/* Turn the XOFF bit off */
+	pko_l1_xoff.u64 = cvmx_read_csr_node(node,
+		CVMX_PKO_L1_SQX_SW_XOFF(pq));
+	ret = pko_l1_xoff.s.xoff;
+	pko_l1_xoff.s.xoff = 0;
+	cvmx_write_csr_node(node,
+		CVMX_PKO_L1_SQX_SW_XOFF(pq), pko_l1_xoff.u64);
+
+	return ret;
+}
+
 /******************************************************************************
 *
 * New PKO3 API - Experimental
@@ -1145,7 +1378,7 @@ int cvmx_pko3_pdesc_from_wqe(cvmx_pko3_pdesc_t *pdesc, cvmx_wqe_78xx_t *wqe,
 
 	/* Verufy the WQE is legit */
 	if (cvmx_unlikely(wqe->word2.software || wqe->pki_wqe_translated)) {
-		cvmx_dprintf("%s: ERROR: invalid WQE\n", __func__);
+		cvmx_printf("%s: ERROR: invalid WQE\n", __func__);
 		return -1;
 	}
 
@@ -1189,7 +1422,7 @@ int cvmx_pko3_pdesc_from_wqe(cvmx_pko3_pdesc_t *pdesc, cvmx_wqe_78xx_t *wqe,
 	hdr_s->s.le = style_buf_reg.s.pkt_lend;
 #if	CVMX_ENABLE_PARAMETER_CHECKING
 	if (hdr_s->s.le != __native_le)
-		cvmx_dprintf("%s: WARNING: "
+		cvmx_printf("%s: WARNING: "
 			"packet endianness mismatch\n",__func__);
 #endif
 
@@ -1266,7 +1499,7 @@ static int cvmx_pko3_pdesc_subdc_add(cvmx_pko3_pdesc_t *pdesc,
 
         /* SEND_JUMP_S missing on Pass1 */
         if(OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X)) {
-                cvmx_dprintf("%s: ERROR: too many segments\n",__func__);
+                cvmx_printf("%s: ERROR: too many segments\n",__func__);
                 return -E2BIG;
         }
 
@@ -1310,7 +1543,7 @@ static int cvmx_pko3_pdesc_subdc_add(cvmx_pko3_pdesc_t *pdesc,
 
 	/* Avoid overrunning jump buffer */
 	if (i >= (jump_buf_size-2)) {
-                cvmx_dprintf("%s: ERROR: too many segments\n",__func__);
+                cvmx_printf("%s: ERROR: too many segments\n",__func__);
 		return -E2BIG;
 	}
 
@@ -1333,10 +1566,12 @@ static int cvmx_pko3_pdesc_subdc_add(cvmx_pko3_pdesc_t *pdesc,
  *
  * @param pdesc Packet Desciptor.
  * @param dq Descriptor Queue associated with the desired output port
+ * @param tag Flow Tag pointer for packet ordering or NULL
  * @return Returns 0 on success, -1 on error.
  *
  */
-int cvmx_pko3_pdesc_transmit(cvmx_pko3_pdesc_t *pdesc, uint16_t dq)
+int cvmx_pko3_pdesc_transmit(cvmx_pko3_pdesc_t *pdesc, uint16_t dq,
+	uint32_t *tag)
 {
         cvmx_pko_query_rtn_t pko_status;
 	cvmx_pko_send_aura_t aura_s;
@@ -1370,6 +1605,10 @@ int cvmx_pko3_pdesc_transmit(cvmx_pko3_pdesc_t *pdesc, uint16_t dq)
 	port_node = dq >> 10;
 	dq &= (1<<10)-1;
 
+	/* To preserve packet order, go atomic with DQ-specific tag */
+	if (tag != NULL)
+		cvmx_pow_tag_sw_nocheck(*tag ^ dq, CVMX_POW_TAG_TYPE_ATOMIC);
+
         /* Send the PKO3 command into the Descriptor Queue */
         pko_status = __cvmx_pko3_do_dma(port_node, dq,
                 pdesc->word, pdesc->num_words, CVMX_PKO_DQ_SEND);
@@ -1379,7 +1618,7 @@ int cvmx_pko3_pdesc_transmit(cvmx_pko3_pdesc_t *pdesc, uint16_t dq)
                 return 0;
 
 #if 0
-        cvmx_dprintf("%s: ERROR: failed to enqueue: %s\n",
+        cvmx_printf("%s: ERROR: failed to enqueue: %s\n",
                                 __FUNCTION__,
                                 pko_dqstatus_error(pko_status.s.dqstatus));
 #endif
@@ -1452,7 +1691,7 @@ int cvmx_pko3_pdesc_buf_append(cvmx_pko3_pdesc_t *pdesc, void *p_data,
 	int rc;
 
 	if (pdesc->mem_s_ix > 0) {
-		cvmx_dprintf("%s: subcommand restriction violated\n", __func__);
+		cvmx_printf("ERROR: %s: subcommand restriction violated\n", __func__);
 		return -1;
 	}
 
@@ -1530,7 +1769,7 @@ int cvmx_pko3_pdesc_notify_wqe(cvmx_pko3_pdesc_t *pdesc, cvmx_wqe_78xx_t *wqe,
 	 * and it must be the very last subcommand
 	 */
 	if (pdesc->send_work_s != 0) {
-		cvmx_dprintf("%s: Only one SEND_WORK_S is allowed\n", __func__);
+		cvmx_printf("ERROR: %s: Only one SEND_WORK_S is allowed\n", __func__);
 		return -1;
 	}
 
diff --git a/arch/mips/cavium-octeon/executive/cvmx-qlm.c b/arch/mips/cavium-octeon/executive/cvmx-qlm.c
index 75fada5..186f01e 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-qlm.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-qlm.c
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2011-2014  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2011-2015  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -42,7 +42,7 @@
  *
  * Helper utilities for qlm.
  *
- * <hr>$Revision: 113334 $<hr>
+ * <hr>$Revision: 122709 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/cvmx.h>
@@ -150,7 +150,9 @@ int cvmx_qlm_get_num(void)
 		return 2;
 	else if (OCTEON_IS_MODEL(OCTEON_CN78XX))
 		return 8;
-	//cvmx_dprintf("Warning: cvmx_qlm_get_num: This chip does not have QLMs\n");
+	else if (OCTEON_IS_MODEL(OCTEON_CN73XX))
+		return 7;
+	/* cvmx_dprintf("Warning: cvmx_qlm_get_num: This chip does not have QLMs\n"); */
 	return 0;
 }
 
@@ -159,7 +161,7 @@ int cvmx_qlm_get_num(void)
  *
  * @param xiface  interface to look up
  *
- * @return 0 on success other on failure
+ * @return the qlm number based on the xiface
  */
 int cvmx_qlm_interface(int xiface)
 {
@@ -173,7 +175,33 @@ int cvmx_qlm_interface(int xiface)
 			return 0;
 		else
 			cvmx_dprintf("Warning: cvmx_qlm_interface: Invalid interface %d\n", xi.interface);
-	} else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+	} else if (octeon_has_feature(OCTEON_FEATURE_BGX)) {
+		cvmx_dprintf("Warning: not supported\n");
+		return -1;
+	} else {
+		/* Must be cn68XX */
+		switch (xi.interface) {
+		case 1:
+			return 0;
+		default:
+			return xi.interface;
+		}
+	}
+	return -1;
+}
+
+/**
+ * Return the qlm number based for a port in the interface
+ *
+ * @param xiface  interface to look up
+ * @param index  index in an interface
+ *
+ * @return the qlm number based on the xiface
+ */
+int cvmx_qlm_lmac(int xiface, int index)
+{
+	struct cvmx_xiface xi = cvmx_helper_xiface_to_node_interface(xiface);
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
 		cvmx_bgxx_cmr_global_config_t gconfig;
 		cvmx_gserx_phy_ctl_t phy_ctl;
 		cvmx_gserx_cfg_t gserx_cfg;
@@ -181,7 +209,8 @@ int cvmx_qlm_interface(int xiface)
 
 		if (xi.interface < 6) {
 			if (xi.interface < 2) {
-				gconfig.u64 = cvmx_read_csr_node(xi.node, CVMX_BGXX_CMR_GLOBAL_CONFIG(xi.interface));
+				gconfig.u64 = cvmx_read_csr_node(xi.node,
+						CVMX_BGXX_CMR_GLOBAL_CONFIG(xi.interface));
 				if (gconfig.s.pmux_sds_sel)
 					qlm = xi.interface + 2; /* QLM 2 or 3 */
 				else
@@ -212,19 +241,130 @@ int cvmx_qlm_interface(int xiface)
 			}
 		}
 		return -1;
-	} else {
-		/* Must be cn68XX */
-		switch (xi.interface) {
-		case 1:
-			return 0;
-		default:
-			return xi.interface;
+	} else if (OCTEON_IS_MODEL(OCTEON_CN73XX)) {
+		cvmx_gserx_phy_ctl_t phy_ctl;
+		cvmx_gserx_cfg_t gserx_cfg;
+		int qlm;
+
+		/* (interface)0->QLM2, 1->QLM3, 2->DLM5/3->DLM6 */
+		if (xi.interface < 2) {
+			qlm = xi.interface + 2; /* (0,1)->ret(2,3) */
+
+			phy_ctl.u64 = cvmx_read_csr(CVMX_GSERX_PHY_CTL(qlm));
+			if (phy_ctl.s.phy_pd || phy_ctl.s.phy_reset) {
+				return -1;
+			}
+			gserx_cfg.u64 = cvmx_read_csr(CVMX_GSERX_CFG(qlm));
+			if (gserx_cfg.s.bgx)
+				return qlm;
+			else
+				return -1;
+		} else if (xi.interface == 2) {
+			cvmx_gserx_cfg_t g1, g2;
+			g1.u64 = cvmx_read_csr(CVMX_GSERX_CFG(5));
+			g2.u64 = cvmx_read_csr(CVMX_GSERX_CFG(6));
+			/* Check if both QLM5 & QLM6 are BGX2 */
+			if (g2.s.bgx) {
+				if (g1.s.bgx) {
+					cvmx_gserx_phy_ctl_t phy_ctl1;
+					phy_ctl.u64 = cvmx_read_csr(CVMX_GSERX_PHY_CTL(5));
+					phy_ctl1.u64 = cvmx_read_csr(CVMX_GSERX_PHY_CTL(6));
+					if ((phy_ctl.s.phy_pd
+					     || phy_ctl.s.phy_reset)
+					    && (phy_ctl1.s.phy_pd
+					        || phy_ctl1.s.phy_reset))
+						return -1;
+					if (index >= 2)
+						return 6;
+					return 5;
+				} else { /* QLM6 is BGX2 */
+					phy_ctl.u64 = cvmx_read_csr(CVMX_GSERX_PHY_CTL(6));
+					if (phy_ctl.s.phy_pd
+					    || phy_ctl.s.phy_reset)
+						return -1;
+					return 6;
+				}
+			} else if (g1.s.bgx) {
+				phy_ctl.u64 = cvmx_read_csr(CVMX_GSERX_PHY_CTL(5));
+				if (phy_ctl.s.phy_pd || phy_ctl.s.phy_reset)
+					return -1;
+				return 5;
+			}
 		}
+		return -1;
+	} else if (OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+		cvmx_gserx_phy_ctl_t phy_ctl;
+		cvmx_gserx_cfg_t gserx_cfg;
+		int qlm;
+		if (xi.interface == 0) {
+			cvmx_gserx_cfg_t g1, g2;
+			g1.u64 = cvmx_read_csr(CVMX_GSERX_CFG(4));
+			g2.u64 = cvmx_read_csr(CVMX_GSERX_CFG(5));
+			/* Check if both QLM4 & QLM5 are BGX0 */
+			if (g2.s.bgx) {
+				if (g1.s.bgx) {
+					cvmx_gserx_phy_ctl_t phy_ctl1;
+					phy_ctl.u64 = cvmx_read_csr(CVMX_GSERX_PHY_CTL(5));
+					phy_ctl1.u64 = cvmx_read_csr(CVMX_GSERX_PHY_CTL(6));
+					if ((phy_ctl.s.phy_pd
+					     || phy_ctl.s.phy_reset)
+					    && (phy_ctl1.s.phy_pd
+					        || phy_ctl1.s.phy_reset))
+						return -1;
+					return 4;
+				} else { /* QLM5 is BGX0 */
+					phy_ctl.u64 = cvmx_read_csr(CVMX_GSERX_PHY_CTL(6));
+					if (phy_ctl.s.phy_pd
+					    || phy_ctl.s.phy_reset)
+						return -1;
+					return 5;
+				}
+			} else if (g1.s.bgx) {
+				phy_ctl.u64 = cvmx_read_csr(CVMX_GSERX_PHY_CTL(5));
+				if (phy_ctl.s.phy_pd || phy_ctl.s.phy_reset)
+					return -1;
+				return 4;
+			}
+		} else if (xi.interface < 2) {
+			qlm = (xi.interface == 1) ? 2 : 3;
+			gserx_cfg.u64 = cvmx_read_csr(CVMX_GSERX_CFG(qlm));
+			if (gserx_cfg.s.srio)
+				return qlm;
+		}
+		return -1;
 	}
 	return -1;
 }
 
 /**
+ * Return if only DLM5/DLM6/DLM5+DLM6 is used by BGX
+ *
+ * @param BGX  BGX to search for.
+ *
+ * @return muxes used 0 = DLM5+DLM6, 1 = DLM5, 2 = DLM6.
+ */
+int cvmx_qlm_mux_interface(int bgx)
+{
+	int mux = 0;
+	cvmx_gserx_cfg_t gser1, gser2;
+
+	if (OCTEON_IS_MODEL(OCTEON_CN73XX) && bgx != 2)
+		return -1;
+
+	gser1.u64 = cvmx_read_csr(CVMX_GSERX_CFG(5));
+	gser2.u64 = cvmx_read_csr(CVMX_GSERX_CFG(6));
+
+	if (gser1.s.bgx && gser2.s.bgx) {
+		mux = 0;
+	} else if (gser1.s.bgx) {
+		mux = 1;  // BGX2 is using DLM5 only
+	} else if (gser2.s.bgx) {
+		mux = 2;  // BGX2 is using DLM6 only
+	}
+	return mux;
+}
+
+/**
  * Return number of lanes for a given qlm
  *
  * @param qlm    QLM to examine
@@ -237,7 +377,10 @@ int cvmx_qlm_get_lanes(int qlm)
 		return 2;
 	else if (OCTEON_IS_MODEL(OCTEON_CNF71XX))
 		return 2;
-
+	else if (OCTEON_IS_MODEL(OCTEON_CN73XX))
+		return (qlm < 4) ? 4/*QLM0,1,2,3*/ : 2/*DLM4,5,6*/;
+	else if (OCTEON_IS_MODEL(OCTEON_CNF75XX))
+		return (qlm == 2 || qlm == 3) ? 4/*QLM2,3*/ : 2/*DLM0,1,4,5*/;
 	return 4;
 }
 
@@ -264,7 +407,7 @@ const __cvmx_qlm_jtag_field_t *cvmx_qlm_jtag_get_field(void)
 		return __cvmx_qlm_jtag_field_cn52xx;
 #endif
 	else {
-		//cvmx_dprintf("cvmx_qlm_jtag_get_field: Needs update for this chip\n");
+		/* cvmx_dprintf("cvmx_qlm_jtag_get_field: Needs update for this chip\n"); */
 		return NULL;
 	}
 }
@@ -879,9 +1022,42 @@ int cvmx_qlm_get_gbaud_mhz(int qlm)
 		mpll_multiplier.u64 = cvmx_read_csr(CVMX_GSERX_DLMX_MPLL_MULTIPLIER(qlm, 0));
 		freq = meas_refclock * mpll_multiplier.s.mpll_multiplier;
 		freq = (freq + 500000) / 1000000;
+
 		return freq;
 	} else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
 		return cvmx_qlm_get_gbaud_mhz_node(cvmx_get_node_num(), qlm);
+	} else if (OCTEON_IS_MODEL(OCTEON_CN73XX)
+		   || OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+		cvmx_gserx_lane_mode_t lane_mode;
+		lane_mode.u64 = cvmx_read_csr(CVMX_GSERX_LANE_MODE(qlm));
+		switch (lane_mode.s.lmode) {
+		case R_25G_REFCLK100:
+			return 2500;
+		case R_5G_REFCLK100:
+			return 5000;
+		case R_8G_REFCLK100:
+			return 8000;
+		case R_125G_REFCLK15625_KX:
+			return 1250;
+		case R_3125G_REFCLK15625_XAUI:
+			return 3125;
+		case R_103125G_REFCLK15625_KR:
+			return 10312;
+		case R_125G_REFCLK15625_SGMII:
+			return 1250;
+		case R_5G_REFCLK15625_QSGMII:
+			return 5000;
+		case R_625G_REFCLK15625_RXAUI:
+			return 6250;
+		case R_25G_REFCLK125:
+			return 2500;
+		case R_5G_REFCLK125:
+			return 5000;
+		case R_8G_REFCLK125:
+			return 8000;
+		default:
+			return 0;
+		}
 	}
 	return 0;
 }
@@ -1242,6 +1418,7 @@ static enum cvmx_qlm_mode __cvmx_qlm_get_mode_cn6xxx(int qlm)
 void __cvmx_qlm_set_mult(int qlm, int baud_mhz, int old_multiplier)
 {
 	cvmx_gserx_dlmx_mpll_multiplier_t mpll_multiplier;
+	cvmx_gserx_dlmx_ref_clkdiv2_t clkdiv;
 	uint64_t meas_refclock, mult;
 
 	if (!OCTEON_IS_MODEL(OCTEON_CN70XX))
@@ -1256,7 +1433,15 @@ void __cvmx_qlm_set_mult(int qlm, int baud_mhz, int old_multiplier)
 		return;
 	}
 
-	mult = (uint64_t)baud_mhz * 1000000 + (meas_refclock/2);
+	/* The baud rate multiplier needs to be adjusted on the CN70XX if
+	 * the reference clock is > 100MHz.
+	 */
+	if (qlm == 0) {
+		clkdiv.u64 = cvmx_read_csr(CVMX_GSERX_DLMX_REF_CLKDIV2(qlm, 0));
+		if (clkdiv.s.ref_clkdiv2)
+			baud_mhz *= 2;
+	}
+	mult = (uint64_t)baud_mhz * 1000000 + (meas_refclock / 2);
 	mult /= meas_refclock;
 
 #ifdef CVMX_BUILD_FOR_UBOOT
@@ -1275,9 +1460,11 @@ void __cvmx_qlm_set_mult(int qlm, int baud_mhz, int old_multiplier)
 	     21-1, 21-2, and 21-3. This is not required with the HRM
 	     sequence. */
 	do {
-		mpll_multiplier.u64 = cvmx_read_csr(CVMX_GSERX_DLMX_MPLL_MULTIPLIER(qlm, 0));
+		mpll_multiplier.u64 =
+			cvmx_read_csr(CVMX_GSERX_DLMX_MPLL_MULTIPLIER(qlm, 0));
 		mpll_multiplier.s.mpll_multiplier = --old_multiplier;
-		cvmx_write_csr(CVMX_GSERX_DLMX_MPLL_MULTIPLIER(qlm, 0), mpll_multiplier.u64);
+		cvmx_write_csr(CVMX_GSERX_DLMX_MPLL_MULTIPLIER(qlm, 0),
+			       mpll_multiplier.u64);
 		/* Wait for 1 ms */
 		cvmx_wait_usec(1000);
 	} while (old_multiplier > (int)mult);
@@ -1289,11 +1476,11 @@ enum cvmx_qlm_mode cvmx_qlm_get_mode_cn78xx(int node, int qlm)
 #ifdef CVMX_BUILD_FOR_UBOOT
 	int qlm_mode[2][9] = {
 		{-1, -1, -1, -1, -1, -1, -1, -1},
-		{-1, -1, -1, -1, -1, -1, -1, -1} };
+		{-1, -1, -1, -1, -1, -1, -1, -1}};
 #else
 	static int qlm_mode[2][9] = {
 		{-1, -1, -1, -1, -1, -1, -1, -1},
-		{-1, -1, -1, -1, -1, -1, -1, -1} };
+		{-1, -1, -1, -1, -1, -1, -1, -1}};
 #endif
 
 	if (qlm >= 8)
@@ -1361,39 +1548,37 @@ enum cvmx_qlm_mode cvmx_qlm_get_mode_cn78xx(int node, int qlm)
 		cvmx_bgxx_cmrx_config_t cmr_config;
 		cvmx_bgxx_spux_br_pmd_control_t pmd_control;
 		int bgx = (qlm < 2) ? qlm : qlm - 2;
-		
+
 		cmr_config.u64 = cvmx_read_csr_node(node, CVMX_BGXX_CMRX_CONFIG(0, bgx));
 		pmd_control.u64 = cvmx_read_csr_node(node, CVMX_BGXX_SPUX_BR_PMD_CONTROL(0, bgx));
-		
+
 		switch(cmr_config.s.lmac_type) {
-		case 0:
-			qlm_mode[node][qlm] = CVMX_QLM_MODE_SGMII;
-			break;
-		case 1:
-			qlm_mode[node][qlm] = CVMX_QLM_MODE_XAUI;
-			break;
-		case 2:
-			qlm_mode[node][qlm] = CVMX_QLM_MODE_RXAUI;
-			break;
-		case 3:	
+		case 0: qlm_mode[node][qlm] = CVMX_QLM_MODE_SGMII; break;
+		case 1:	qlm_mode[node][qlm] = CVMX_QLM_MODE_XAUI; break;
+		case 2:	qlm_mode[node][qlm] = CVMX_QLM_MODE_RXAUI; break;
+		case 3:
 			/* Use training to determine if we're in 10GBASE-KR or XFI */
 			if (pmd_control.s.train_en)
 				qlm_mode[node][qlm] = CVMX_QLM_MODE_10G_KR;
 			else
 				qlm_mode[node][qlm] = CVMX_QLM_MODE_XFI;
+#ifndef CVMX_BUILD_FOR_UBOOT
 			pmd_control.s.train_en = 0;
 			cvmx_write_csr_node(node,
 				CVMX_BGXX_SPUX_BR_PMD_CONTROL(0, bgx), pmd_control.u64);
+#endif
 			break;
-		case 4:	
-			/* Use training to determine if we're in 10GBASE-KR or XFI */
+		case 4:
+			/* Use training to determine if we're in 40GBASE-KR or XLAUI */
 			if (pmd_control.s.train_en)
 				qlm_mode[node][qlm] = CVMX_QLM_MODE_40G_KR4;
 			else
 				qlm_mode[node][qlm] = CVMX_QLM_MODE_XLAUI;
+#ifndef CVMX_BUILD_FOR_UBOOT
 			pmd_control.s.train_en = 0;
 			cvmx_write_csr_node(node,
 				CVMX_BGXX_SPUX_BR_PMD_CONTROL(0, bgx), pmd_control.u64);
+#endif
 			break;
 		default:
 			qlm_mode[node][qlm] = CVMX_QLM_MODE_DISABLED;
@@ -1405,6 +1590,340 @@ enum cvmx_qlm_mode cvmx_qlm_get_mode_cn78xx(int node, int qlm)
 	return qlm_mode[node][qlm];
 }
 
+enum cvmx_qlm_mode __cvmx_qlm_get_mode_cn73xx(int qlm)
+{
+	cvmx_gserx_cfg_t gserx_cfg;
+#ifdef CVMX_BUILD_FOR_UBOOT
+	int qlm_mode[7] = {-1, -1, -1, -1, -1, -1, -1};
+#else
+	static int qlm_mode[7] = {-1, -1, -1, -1, -1, -1, -1};
+#endif
+
+	if (qlm_mode[qlm] != -1)
+		return qlm_mode[qlm];
+
+	if (qlm > 6) {
+		cvmx_dprintf("Invalid QLM(%d) passed\n", qlm);
+		return -1;
+	}
+
+	gserx_cfg.u64 = cvmx_read_csr(CVMX_GSERX_CFG(qlm));
+	if (gserx_cfg.s.pcie) {
+		cvmx_pemx_cfg_t pemx_cfg;
+		switch (qlm) {
+		case 0: /* Either PEM0 x4 or PEM0 x8 */
+		case 1: /* Either PEM0 x8 or PEM1 x4 */
+		{
+			pemx_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(0));
+			if (pemx_cfg.cn78xx.lanes8)
+				qlm_mode[qlm] = CVMX_QLM_MODE_PCIE_1X8; /* PEM0 x8 */
+			else
+				qlm_mode[qlm] = CVMX_QLM_MODE_PCIE;     /* PEM0/PEM1 x4 */
+			break;
+		}
+		case 2: /* Either PEM2 x4 or PEM2 x8 */
+		{
+			pemx_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(2));
+			if (pemx_cfg.cn78xx.lanes8)
+				qlm_mode[qlm] = CVMX_QLM_MODE_PCIE_1X8;  /* PEM2 x8 */
+			else
+				qlm_mode[qlm] = CVMX_QLM_MODE_PCIE;      /* PEM2 x4 */
+			break;
+		}
+		case 5:
+		case 6:	/* PEM3 x2 */
+			qlm_mode[qlm] = CVMX_QLM_MODE_PCIE_1X2; /* PEM3 x2 */
+			break;
+		case 3: /* Either PEM2 x8 or PEM3 x4 */
+		{
+			pemx_cfg.u64 = cvmx_read_csr(CVMX_PEMX_CFG(2));
+			if (pemx_cfg.cn78xx.lanes8)
+				qlm_mode[qlm] = CVMX_QLM_MODE_PCIE_1X8;  /* PEM2 x8 */
+			else
+				qlm_mode[qlm] = CVMX_QLM_MODE_PCIE; /* PEM3 x4 */
+			break;
+		}
+		default:
+			qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+			break;
+		}
+	} else if (gserx_cfg.s.bgx) {
+		cvmx_bgxx_cmrx_config_t cmr_config;
+		cvmx_bgxx_cmr_rx_lmacs_t bgx_cmr_rx_lmacs;
+		cvmx_bgxx_spux_br_pmd_control_t pmd_control;
+		int bgx = 0;
+		int start = 0, end = 4, index;
+		int lane_mask = 0, train_mask = 0;
+		int mux = 0; // 0:BGX2 (DLM5/DLM6), 1:BGX2(DLM5), 2:BGX2(DLM6)
+		if (qlm < 4)
+			bgx = qlm - 2;
+		else if (qlm == 5 || qlm == 6) {
+			bgx = 2;
+			mux = cvmx_qlm_mux_interface(bgx);
+			if (mux == 0) {
+				start = 0;
+				end = 4;
+			} else if (mux == 1) {
+				start = 0;
+				end = 2;
+			} else if (mux == 2) {
+				start = 2;
+				end = 4;
+			} else {
+				qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+				return qlm_mode[qlm];
+			}
+		}
+
+		for (index = start; index < end; index++) {
+			cmr_config.u64 = cvmx_read_csr(CVMX_BGXX_CMRX_CONFIG(index, bgx));
+			pmd_control.u64 = cvmx_read_csr(CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, bgx));
+			lane_mask |= (cmr_config.s.lmac_type << (index * 4));
+			train_mask |= (pmd_control.s.train_en << (index * 4));
+		}
+
+		/* Need to include DLM5 lmacs when only DLM6 DLM is used */
+		if (mux == 2)
+			bgx_cmr_rx_lmacs.u64 = cvmx_read_csr(CVMX_BGXX_CMR_RX_LMACS(2));
+		switch(lane_mask) {
+		case 0:
+			if (mux == 1)
+				qlm_mode[qlm] = CVMX_QLM_MODE_SGMII_2X1;
+			else if (mux == 2) {
+				qlm_mode[qlm] = CVMX_QLM_MODE_SGMII_2X1;
+				bgx_cmr_rx_lmacs.s.lmacs = 4;
+			}
+				qlm_mode[qlm] = CVMX_QLM_MODE_SGMII;
+			break;
+		case 0x1:
+			qlm_mode[qlm] = CVMX_QLM_MODE_XAUI;
+			break;
+		case 0x2:
+			if (mux == 1)
+				qlm_mode[qlm] = CVMX_QLM_MODE_RXAUI_1X2; // NONE+RXAUI
+			else if (mux == 0)
+				qlm_mode[qlm] = CVMX_QLM_MODE_MIXED; // RXAUI+SGMII
+			else
+				qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+			break;
+		case 0x202:
+			if (mux == 2) {
+				qlm_mode[qlm] = CVMX_QLM_MODE_RXAUI_1X2; // RXAUI+RXAUI
+				bgx_cmr_rx_lmacs.s.lmacs = 4;
+			} else if (mux == 1)
+				qlm_mode[qlm] = CVMX_QLM_MODE_RXAUI_1X2; // RXAUI+RXAUI
+			else if (mux == 0)
+				qlm_mode[qlm] = CVMX_QLM_MODE_RXAUI;
+			else
+				qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+			break;
+		case 0x22:
+			qlm_mode[qlm] = CVMX_QLM_MODE_RXAUI;
+			break;
+		case 0x3333:
+			/* Use training to determine if we're in 10GBASE-KR or XFI */
+			if (train_mask)
+				qlm_mode[qlm] = CVMX_QLM_MODE_10G_KR;
+			else
+				qlm_mode[qlm] = CVMX_QLM_MODE_XFI;
+			break;
+		case 0x4:
+			/* Use training to determine if we're in 40GBASE-KR or XLAUI */
+			if (train_mask)
+				qlm_mode[qlm] = CVMX_QLM_MODE_40G_KR4;
+			else
+				qlm_mode[qlm] = CVMX_QLM_MODE_XLAUI;
+			break;
+		case 0x0005:
+			qlm_mode[qlm] = CVMX_QLM_MODE_RGMII_SGMII;
+			break;
+		case 0x3335:
+			if (train_mask)
+				qlm_mode[qlm] = CVMX_QLM_MODE_RGMII_10G_KR;
+			else
+				qlm_mode[qlm] = CVMX_QLM_MODE_RGMII_XFI;
+			break;
+		case 0x45:
+			if (train_mask)
+				qlm_mode[qlm] = CVMX_QLM_MODE_RGMII_40G_KR4;
+			else
+				qlm_mode[qlm] = CVMX_QLM_MODE_RGMII_XLAUI;
+			break;
+		case 0x225:
+			qlm_mode[qlm] = CVMX_QLM_MODE_RGMII_RXAUI;
+			break;
+		case 0x15:
+			qlm_mode[qlm] = CVMX_QLM_MODE_RGMII_XAUI;
+			break;
+
+		case 0x200:
+			if (mux == 2) {
+				qlm_mode[qlm] = CVMX_QLM_MODE_RXAUI_1X2;
+				bgx_cmr_rx_lmacs.s.lmacs = 4;
+			} else
+		case 0x205:
+		case 0x233:
+		case 0x3302:
+		case 0x3305:
+			if (mux == 0)
+				qlm_mode[qlm] = CVMX_QLM_MODE_MIXED;
+			else
+				qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+			break;
+		case 0x3300:
+			if (mux == 0)
+				qlm_mode[qlm] = CVMX_QLM_MODE_MIXED;
+			else if (mux == 2) {
+				if (train_mask)
+					qlm_mode[qlm] = CVMX_QLM_MODE_10G_KR_1X2;
+				else
+					qlm_mode[qlm] = CVMX_QLM_MODE_XFI_1X2;
+				bgx_cmr_rx_lmacs.s.lmacs = 4;
+			} else
+				qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+			break;
+		case 0x33:
+			if (mux == 1 || mux == 2) {
+				if (train_mask)
+					qlm_mode[qlm] = CVMX_QLM_MODE_10G_KR_1X2;
+				else
+					qlm_mode[qlm] = CVMX_QLM_MODE_XFI_1X2;
+				if (mux == 2)
+					bgx_cmr_rx_lmacs.s.lmacs = 4;
+			} else
+				qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+			break;
+		case 0x0035:
+			if (mux == 0)
+				qlm_mode[qlm] = CVMX_QLM_MODE_MIXED;
+			else if (train_mask)
+				qlm_mode[qlm] = CVMX_QLM_MODE_RGMII_10G_KR_1X1;
+			else
+				qlm_mode[qlm] = CVMX_QLM_MODE_RGMII_XFI_1X1;
+			break;
+		case 0x235:
+			if (mux == 0)
+				qlm_mode[qlm] = CVMX_QLM_MODE_MIXED;
+			else
+				qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+			break;
+		default:
+			qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+			break;
+		}
+		if (mux == 2) {
+			cvmx_write_csr(CVMX_BGXX_CMR_RX_LMACS(2), bgx_cmr_rx_lmacs.u64);
+			cvmx_write_csr(CVMX_BGXX_CMR_TX_LMACS(2), bgx_cmr_rx_lmacs.u64);
+		}
+	} else if (gserx_cfg.s.sata)
+		qlm_mode[qlm] = CVMX_QLM_MODE_SATA_2X1;
+	else
+		qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+
+	return qlm_mode[qlm];
+}
+
+enum cvmx_qlm_mode __cvmx_qlm_get_mode_cnf75xx(int qlm)
+{
+	cvmx_gserx_cfg_t gserx_cfg;
+#ifdef CVMX_BUILD_FOR_UBOOT
+	int qlm_mode[9] = {-1, -1, -1, -1, -1, -1, -1};
+#else
+	static int qlm_mode[9] = {-1, -1, -1, -1, -1, -1, -1};
+#endif
+
+	if (qlm_mode[qlm] != -1)
+		return qlm_mode[qlm];
+
+	if (qlm > 9) {
+		cvmx_dprintf("Invalid QLM(%d) passed\n", qlm);
+		return -1;
+	}
+
+	if (qlm == 2 || qlm == 3) {
+		cvmx_sriox_status_reg_t status_reg;
+		int port = (qlm == 2) ? 0 : 1;
+		status_reg.u64 = cvmx_read_csr(CVMX_SRIOX_STATUS_REG(port));
+		/* FIXME add different width */
+		if (status_reg.s.srio)
+			qlm_mode[qlm] = CVMX_QLM_MODE_SRIO_1X4;
+		else
+			qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+		return qlm_mode[qlm];
+	}
+
+	gserx_cfg.u64 = cvmx_read_csr(CVMX_GSERX_CFG(qlm));
+	if (gserx_cfg.s.pcie) {
+		switch (qlm) {
+		case 0: /* Either PEM0 x2 or PEM0 x4 */
+		case 1: /* Either PEM1 x2 or PEM0 x4 */
+		{
+			/* FIXME later */
+			qlm_mode[qlm] = CVMX_QLM_MODE_PCIE;
+			break;
+		}
+		default:
+			qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+			break;
+		}
+	} else if (gserx_cfg.s.bgx) {
+		cvmx_bgxx_cmrx_config_t cmr_config;
+		cvmx_bgxx_spux_br_pmd_control_t pmd_control;
+		int bgx = 0;
+		int start = 0, end = 4, index;
+		int lane_mask = 0, train_mask = 0;
+		int mux = 0; // 0:BGX0 (DLM4/DLM5), 1:BGX0(DLM4), 2:BGX0(DLM5)
+		cvmx_gserx_cfg_t gser1, gser2;
+		gser1.u64 = cvmx_read_csr(CVMX_GSERX_CFG(4));
+		gser2.u64 = cvmx_read_csr(CVMX_GSERX_CFG(5));
+		if (gser1.s.bgx && gser2.s.bgx) {
+			start = 0;
+			end = 4;
+		} else if (gser1.s.bgx) {
+			start = 0;
+			end = 2;
+			mux = 1;
+		} else if (gser2.s.bgx) {
+			start = 2;
+			end = 4;
+			mux = 2;
+		} else {
+			qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+			return qlm_mode[qlm];
+		}
+
+		for (index = start; index < end; index++) {
+			cmr_config.u64 = cvmx_read_csr(CVMX_BGXX_CMRX_CONFIG(index, bgx));
+			pmd_control.u64 = cvmx_read_csr(CVMX_BGXX_SPUX_BR_PMD_CONTROL(index, bgx));
+			lane_mask |= (cmr_config.s.lmac_type << (index * 4));
+			train_mask |= (pmd_control.s.train_en << (index * 4));
+		}
+
+		switch(lane_mask) {
+		case 0:
+			if (mux == 1 || mux == 2)
+				qlm_mode[qlm] = CVMX_QLM_MODE_SGMII_2X1;
+			else
+				qlm_mode[qlm] = CVMX_QLM_MODE_SGMII;
+			break;
+		case 0x3300:
+			if (mux == 0)
+				qlm_mode[qlm] = CVMX_QLM_MODE_MIXED;
+			else if (mux == 2)
+				qlm_mode[qlm] = CVMX_QLM_MODE_XFI_1X2;
+			else
+				qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+			break;
+		default:
+			qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+			break;
+		}
+	} else
+		qlm_mode[qlm] = CVMX_QLM_MODE_DISABLED;
+
+	return qlm_mode[qlm];
+}
+
 /*
  * Read QLM and return mode.
  */
@@ -1416,18 +1935,30 @@ enum cvmx_qlm_mode cvmx_qlm_get_mode(int qlm)
 		return __cvmx_qlm_get_mode_cn70xx(qlm);
 	else if (OCTEON_IS_MODEL(OCTEON_CN78XX))
 		return cvmx_qlm_get_mode_cn78xx(cvmx_get_node_num(), qlm);
+	else if (OCTEON_IS_MODEL(OCTEON_CN73XX))
+		return __cvmx_qlm_get_mode_cn73xx(qlm);
+	else if (OCTEON_IS_MODEL(OCTEON_CNF75XX))
+		return __cvmx_qlm_get_mode_cnf75xx(qlm);
 
 	return CVMX_QLM_MODE_DISABLED;
 }
 
-int cvmx_qlm_measure_clock_cn78xx(int node, int qlm)
+int cvmx_qlm_measure_clock_cn7xxx(int node, int qlm)
 {
 	cvmx_gserx_cfg_t cfg;
 	cvmx_gserx_refclk_sel_t refclk_sel;
 	cvmx_gserx_lane_mode_t lane_mode;
 
-	if (qlm >= 8)
-		return -1; /* FIXME for OCI */
+	if (OCTEON_IS_MODEL(OCTEON_CN73XX)) {
+		if (node != 0 || qlm >= 7)
+			return -1;
+	} else if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		if (qlm >= 8 || node > 1)
+			return -1; /* FIXME for OCI */
+	} else {
+		cvmx_dprintf("%s: Unsupported OCTEON model\n", __func__);
+		return -1;
+	}
 
 	cfg.u64 = cvmx_read_csr_node(node, CVMX_GSERX_CFG(qlm));
 
@@ -1471,18 +2002,35 @@ int cvmx_qlm_measure_clock_cn78xx(int node, int qlm)
 }
 
 /**
+ * Measure the reference clock of a QLM on a multi-node setup
+ *
+ * @param node   node to measure
+ * @param qlm    QLM to measure
+ *
+ * @return Clock rate in Hz
+ */
+int cvmx_qlm_measure_clock_node(int node, int qlm)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_MULTINODE))
+		return cvmx_qlm_measure_clock_cn7xxx(node, qlm);
+	else
+		return cvmx_qlm_measure_clock(qlm);
+}
+
+/**
  * Measure the reference clock of a QLM
  *
  * @param qlm    QLM to measure
  *
  * @return Clock rate in Hz
- *       */
+ */
 int cvmx_qlm_measure_clock(int qlm)
 {
 	cvmx_mio_ptp_clock_cfg_t ptp_clock;
 	uint64_t count;
 	uint64_t start_cycle, stop_cycle;
 	int evcnt_offset = 0x10;
+	int incr_count = 1;
 #ifdef CVMX_BUILD_FOR_UBOOT
 	int ref_clock[16] = {0};
 #else
@@ -1495,8 +2043,8 @@ int cvmx_qlm_measure_clock(int qlm)
 	if (OCTEON_IS_MODEL(OCTEON_CN3XXX) || OCTEON_IS_MODEL(OCTEON_CN5XXX))
 		return -1;
 
-	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
-		return cvmx_qlm_measure_clock_cn78xx(cvmx_get_node_num(), qlm);
+	if (OCTEON_IS_OCTEON3() && !OCTEON_IS_MODEL(OCTEON_CN70XX))
+		return cvmx_qlm_measure_clock_cn7xxx(cvmx_get_node_num(), qlm);
 
 	/* Force the reference to 156.25Mhz when running in simulation.
 	   This supports the most speeds */
@@ -1507,6 +2055,14 @@ int cvmx_qlm_measure_clock(int qlm)
 	if (cvmx_sysinfo_get()->board_type == CVMX_BOARD_TYPE_SIM)
 		return 156250000;
 #endif
+	if (OCTEON_IS_MODEL(OCTEON_CN70XX) && qlm == 0) {
+		cvmx_gserx_dlmx_ref_clkdiv2_t ref_clkdiv2;
+
+		ref_clkdiv2.u64 = cvmx_read_csr(CVMX_GSERX_DLMX_REF_CLKDIV2(qlm, 0));
+		if (ref_clkdiv2.s.ref_clkdiv2)
+			incr_count = 2;
+	}
+
 	/* Fix reference clock for OCI QLMs */
 
 	/* Disable the PTP event counter while we configure it */
@@ -1541,11 +2097,114 @@ int cvmx_qlm_measure_clock(int qlm)
 	cvmx_write_csr(CVMX_MIO_PTP_CLOCK_CFG, ptp_clock.u64);
 	/* Clock counted down, so reverse it */
 	count = 1000000000 - count;
+	count *= incr_count;
 	/* Return the rate */
 	ref_clock[qlm] = count * cvmx_clock_get_rate(CVMX_CLOCK_CORE) / (stop_cycle - start_cycle);
 	return ref_clock[qlm];
 }
 
+/*
+ * Perform RX equalization on a QLM
+ *
+ * @param node	Node the QLM is on
+ * @param qlm	QLM to perform RX equalization on
+ * @param lane	Lane to use, or -1 for all lanes
+ *
+ * @return Zero on sucess, negative if any lane failed RX equalization
+ */
+int __cvmx_qlm_rx_equalization(int node, int qlm, int lane)
+{
+	cvmx_gserx_phy_ctl_t phy_ctl;
+	cvmx_gserx_spd_t gserx_spd;
+	int fail, gbaud, l;
+	enum cvmx_qlm_mode mode;
+	int max_lanes = 4;
+
+	/* Errata (GSER-20075) GSER(0..13)_BR_RX3_EER[RXT_ERR] is
+	   GSER(0..13)_BR_RX2_EER[RXT_ERR]. Since lanes 2-3 are tied together,
+	   we only do RX equalization on 2 and ignore 3 */
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX_PASS1_X))
+		max_lanes = 3;
+
+	/* Don't touch QLMs if it is reset or powered down */
+	phy_ctl.u64 = cvmx_read_csr_node(node, CVMX_GSERX_PHY_CTL(qlm));
+	if (phy_ctl.s.phy_pd || phy_ctl.s.phy_reset)
+		return -1;
+
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
+		gbaud = cvmx_qlm_get_gbaud_mhz_node(node, qlm);
+	else
+		gbaud = cvmx_qlm_get_gbaud_mhz(qlm);
+
+	/* Apply RX Equalization for speed >= 8G */
+	if (qlm < 8) {
+		if (gbaud < 6250)
+			return 0;
+	} else { // OCI
+		gserx_spd.u64 = cvmx_read_csr_node(node, CVMX_GSERX_SPD(qlm));
+		/* Supported with SW init at 6.25G */
+		if (gserx_spd.s.spd == 0xf) {
+			if (gbaud < 6250)
+				return 0;
+		} else { /* Only supported at 8G or higher */
+			if (gbaud < 8000)
+				return 0;
+		}
+	}
+
+	/* Don't run on PCIe Links */
+	mode = cvmx_qlm_get_mode(qlm);
+	if (mode == CVMX_QLM_MODE_PCIE
+	    || mode == CVMX_QLM_MODE_PCIE_1X8
+	    || mode == CVMX_QLM_MODE_PCIE_1X2
+	    || mode == CVMX_QLM_MODE_PCIE_2X1)
+		return -1;
+
+	fail = 0;
+
+	for (l = 0; l < max_lanes; l++) {
+		cvmx_gserx_br_rxx_ctl_t rxx_ctl;
+		cvmx_gserx_br_rxx_eer_t rxx_eer;
+
+		if ((lane != -1) && (lane != l))
+			continue;
+
+		/* Enable software control */
+		rxx_ctl.u64 = cvmx_read_csr_node(node, CVMX_GSERX_BR_RXX_CTL(l, qlm));
+		rxx_ctl.s.rxt_swm = 1;
+		cvmx_write_csr_node(node, CVMX_GSERX_BR_RXX_CTL(l, qlm), rxx_ctl.u64);
+
+		/* Clear the completion flag and initiate a new request */
+		rxx_eer.u64 = cvmx_read_csr_node(node, CVMX_GSERX_BR_RXX_EER(l, qlm));
+		rxx_eer.s.rxt_esv = 0;
+		rxx_eer.s.rxt_eer = 1;
+		cvmx_write_csr_node(node, CVMX_GSERX_BR_RXX_EER(l, qlm), rxx_eer.u64);
+	}
+
+	/* Wait for RX equalization to complete */
+	for (l = 0; l < max_lanes; l++) {
+		cvmx_gserx_br_rxx_eer_t rxx_eer;
+		cvmx_gserx_br_rxx_ctl_t rxx_ctl;
+
+		if ((lane != -1) && (lane != l))
+			continue;
+
+		CVMX_WAIT_FOR_FIELD64_NODE(node, CVMX_GSERX_BR_RXX_EER(l, qlm),
+				cvmx_gserx_br_rxx_eer_t, rxt_esv, ==, 1, 1000000);
+		rxx_eer.u64 = cvmx_read_csr_node(node, CVMX_GSERX_BR_RXX_EER(l, qlm));
+		/* Switch back to hardware control */
+		rxx_ctl.u64 = cvmx_read_csr_node(node, CVMX_GSERX_BR_RXX_CTL(l, qlm));
+		rxx_ctl.s.rxt_swm = 0;
+		cvmx_write_csr_node(node, CVMX_GSERX_BR_RXX_CTL(l, qlm), rxx_ctl.u64);
+		if (!rxx_eer.s.rxt_esv) {
+			//cvmx_dprintf("%d:QLM%d: Lane %d RX equalization timeout\n", node, qlm, lane);
+			fail = 1;
+		}
+	}
+
+	return (fail) ? -1 : 0;
+}
+
 void cvmx_qlm_display_registers(int qlm)
 {
 	int num_lanes = cvmx_qlm_get_lanes(qlm);
@@ -1618,3 +2277,781 @@ void __cvmx_helper_errata_qlm_disable_2nd_order_cdr(int qlm)
 	}
 	cvmx_helper_qlm_jtag_update(qlm);
 }
+
+
+#ifdef CVMX_DUMP_GSER
+/**
+ * Dump configuration and status of GSER (use for 73xx, 75xx, 76xx, 78xx)
+ */
+/* The following (high level) funcs are implemented in this section */
+int cvmx_dump_gser_config(unsigned gser);
+int cvmx_dump_gser_status(unsigned gser);
+int cvmx_dump_gser_config_node(unsigned node, unsigned gser);
+int cvmx_dump_gser_status_node(unsigned node, unsigned gser);
+
+/*
+ * The following macros helps to easyly print 'formated table'
+ * for up to 4 Lanes of single GSER device (smae macros used for BGX)
+ * MACROS automaticaly handle data types ('unsigned' or 'const char *') and
+ * indexing (by predefined 'ind' string used as index).
+ * NOTE that there different groups of macros:
+ *  'PRn' 	means it prints unconditionaly, data are 'unsigned' type
+ *  'PRns'	means it prints unconditionaly, data are 'const char *' type
+ *  'PRc' 	means it print only if 'cond' argument is 'true'
+ *  'PRd' 	means it print only if 'data' argument is != 0
+ *  'PRcd' 	means it print only if ('cond'== true && 'data' != 0)
+ *  'PRM' 	means it print only when 'mask' bits are != 0, skip otherwise
+ * 		NOTE: the loop is 'for (_mask = mask; _mask > 0; _mask >>= 1)'
+ * 		which means order is mask_bits 0,1,2,3.. and the last unset bits
+ * 		will not be handled at all (i.e. not 'skipped' with '<   ---   >')
+ * Other useful combinations of them exists like:
+ *  'PRMc' 	means it print only when 'mask' bits are != 0 and 'cond' = true
+ *  'PRMcs' 	same as above but data type is 'const char *' instead of 'unsigned'
+ * For example one use 'PRMcd' to print 'data' only for 'mask' mapped LMAC fields
+ * if 'cond' is met and 'data' != 0 (example: Frame Check 'FSCERR' (check for
+ * FCS errors) was enabled with 'cond' and err realy happens (detected) (i.e.
+ * 'data' != 0) and only then it will be printed
+ * (and only for LMACs mapped with 'mask' bits = 1, other fields skipped)
+ * NOTE: Where applicable references to Hardware Manual are available.
+ */
+
+
+/* define FORCE_COND=1 in order to force prints unconditionally (DEBUG, test)
+ * define FORCE_COND=0 in order to prints conditionally (NORMAL MODE, skip unimportant)
+ * if run time control is needed just define FORCE_COND as local static variable
+ * Enable only one of the following '#define FORCED_COND(cond)' macros
+ * use it like this: 'if ( FORCED_COND(cond) )'
+ */
+/* #define FORCE_COND	1	*/
+/* #define FORCED_COND(cond)	((cond) || FORCE_COND) */
+/* ..or.. the next line to exclude all overhead code */
+#define FORCED_COND(cond)	(cond)
+
+#ifndef USE_PRx_MACROS
+#define USE_PRx_MACROS
+
+/* Always print - no test of 'data' values */
+/* for (1..N) printf(format, data) */
+#define PRn(header, N, format, data)			\
+do {							\
+	unsigned ind;					\
+	cvmx_dprintf("%-48s", header);			\
+	for (ind = 0; ind < N; ind++)			\
+		cvmx_dprintf(format, data);		\
+	cvmx_dprintf("\n");				\
+} while(0);
+
+#define PRns	PRn
+
+/* Always print - no test, skip data for mask[x]=0 */
+#define PRMn(header, mask, format, data)				\
+do {									\
+	unsigned ind, _mask;						\
+	cvmx_dprintf("%-48s", header);					\
+	for (_mask = mask, ind = 0; _mask > 0; _mask >>= 1, ind++)	\
+		if (_mask & 1)						\
+			cvmx_dprintf(format, data);			\
+		else							\
+			cvmx_dprintf("%15s","");			\
+	cvmx_dprintf("\n");						\
+} while(0);
+
+#define PRMns PRMn
+
+/* mask + data != 0 */
+#define PRMd(header, mask, format, data)					\
+do {										\
+	unsigned cnt, ind, _mask = mask;					\
+	for (cnt = 0, ind = 0; _mask > 0; _mask >>= 1, ind++)			\
+		if (data != 0)							\
+			cnt++;							\
+	if (FORCED_COND(cnt)) { /* at least one item != 0 =>print */		\
+		cvmx_dprintf("%-48s", header);					\
+		for (_mask = mask, ind = 0; _mask > 0; _mask >>= 1, ind++)	\
+			if (FORCED_COND((_mask & 1) && data!= 0))		\
+				cvmx_dprintf(format, data);			\
+			else							\
+				cvmx_dprintf("%15s","");			\
+		cvmx_dprintf("\n");						\
+	}									\
+} while(0);
+
+/* mask + cond != 0 */
+#define PRMc(header, mask, cond, format, data)					\
+do {										\
+	unsigned cnt, ind, _mask = mask;					\
+	for (cnt = 0, ind = 0; _mask > 0; _mask >>= 1, ind++)			\
+		if (cond != 0)							\
+			cnt++;							\
+	if (FORCED_COND(cnt)) { /* at least one item != 0 =>print */		\
+		cvmx_dprintf("%-48s", header);					\
+		for (_mask = mask, ind = 0; _mask > 0; _mask >>= 1, ind++)	\
+			if (FORCED_COND((_mask & 1) && cond))			\
+				cvmx_dprintf(format, data);			\
+			else							\
+				cvmx_dprintf("%15s","");			\
+		cvmx_dprintf("\n");						\
+	}									\
+} while(0);
+
+#define PRMcs	PRMc
+
+/* for (mask[i]==1) if (cond && data) printf(format, data) */
+#define PRMcd(header, mask, cond, format, data)					\
+do {										\
+	unsigned cnt, ind, _mask = mask;					\
+	for (cnt = 0, ind = 0; _mask > 0; _mask >>= 1, ind++)			\
+		if (cond && data != 0)						\
+			cnt++;							\
+	if (FORCED_COND(cnt)) { /* at least one item != 0 =>print */		\
+		cvmx_dprintf("%-48s", header);					\
+		for (_mask = mask, ind = 0; _mask > 0; _mask >>= 1, ind++)	\
+		if ( FORCED_COND(cond && (data!=0) && (_mask & 1)) )		\
+			cvmx_dprintf(format, data);				\
+		else								\
+			cvmx_dprintf("%15s","");				\
+		cvmx_dprintf("\n");						\
+	}									\
+} while(0);
+
+/* Test 'data' int values and print only if data != 0 */
+/* for (1..N) if (data != 0) printf(format, data) */
+#define PRd(header, N, format, data)					\
+do {									\
+	unsigned ind, cnt;						\
+	for (cnt = 0, ind = 0; ind < N; ind++)				\
+		if (data != 0)						\
+			cnt++;						\
+	if (FORCED_COND(cnt)) { /* at least one item != 0 =>print */	\
+		cvmx_dprintf("%-48s", header);				\
+		for (ind = 0; ind < N; ind++)				\
+			cvmx_dprintf(format, data);			\
+		cvmx_dprintf("\n");					\
+	}								\
+} while(0);
+
+/* for (1..N) if (cond) printf(format, data) */
+#define PRc(header, N, cond, format, data)				\
+do {									\
+	unsigned ind, cnt;						\
+	for (cnt = 0, ind = 0; ind < N; ind++)				\
+		if (cond != 0)						\
+			cnt++;						\
+	if (FORCED_COND(cnt)) { /* at least one item != 0 =>print */	\
+		cvmx_dprintf("%-48s", header);				\
+		for (ind = 0; ind < N; ind++)				\
+		if ( FORCED_COND(cond) )				\
+			cvmx_dprintf(format, data);			\
+		else							\
+			cvmx_dprintf("%15s","");			\
+		cvmx_dprintf("\n");					\
+	}								\
+} while(0);
+
+#define PRcs PRc
+
+/* for (1..N) if (cond && data) printf(format, data) */
+#define PRcd(header, N, cond, format, data)				\
+do {									\
+	unsigned ind, cnt;						\
+	for (cnt = 0, ind = 0; ind < N; ind++)				\
+		if (cond && data != 0)					\
+			cnt++;						\
+	if (FORCED_COND(cnt)) { /* at least one item != 0 =>print */	\
+		cvmx_dprintf("%-48s", header);				\
+		for (ind = 0; ind < N; ind++)				\
+		if ( FORCED_COND(cond && data) )			\
+			cvmx_dprintf(format, data);			\
+		else							\
+			cvmx_dprintf("%15s","");			\
+		cvmx_dprintf("\n");					\
+	}								\
+} while(0);
+
+#endif
+
+static const char *SPD_string[] = {
+	"100 MHz, 1.25 Gb, R_125G_REFCLK15625_KX",
+	"100 MHz, 2.5 Gb, R_25G_REFCLK100",
+	"100 MHz, 5 Gb, R_5G_REFCLK100",
+	"100 MHz, 8 Gb, R_8G_REFCLK100",
+	"125 MHz, 1.25 Gb, R_125G_REFCLK15625_KX",
+	"125 MHz, 2.5 Gb, R_25G_REFCLK125",
+	"125 MHz, 3.125, Gb R_3125G_REFCLK15625_XAUI",
+	"125 MHz, 5 Gb, R_5G_REFCLK125",
+	"125 MHz, 6.25 Gb, R_625G_REFCLK15625_RXAUI",
+	"125 MHz, 8 Gb, R_8G_REFCLK125",
+	"156.25 MHz, 2.5 Gb, R_25G_REFCLK100",
+	"156.25 MHz, 3.125 Gb, R_3125G_REFCLK15625_XAUI",
+	"156.25 MHz, 5 Gb, R_5G_REFCLK125",
+	"156.25 MHz, 6.25 Gb, R_625G_REFCLK15625_RXAUI",
+	"156.25 MHz, 10.3125 Gb, R_103125G_REFCLK15625_KR",
+	"SW_MODE"
+};
+
+static const char *LMODE_string[] = {
+	"R_25G_REFCLK100",
+	"R_5G_REFCLK100",
+	"R_8G_REFCLK100",
+	"R_125G_REFCLK15625_KX (not supported)",
+	"R_3125G_REFCLK15625_XAUI",
+	"R_103125G_REFCLK15625_KR",
+	"R_125G_REFCLK15625_SGMII",
+	"R_5G_REFCLK15625_QSGMII (not supported)",
+	"R_625G_REFCLK15625_RXAUI",
+	"R_25G_REFCLK125",
+	"R_5G_REFCLK125",
+	"R_8G_REFCLK125",
+	"UNKNOWN LMODE",
+	"UNKNOWN LMODE",
+	"UNKNOWN LMODE",
+	"UNKNOWN LMODE"
+};
+
+
+
+/**
+ * Return number of GSERs per model
+ * (FIXME: Add models when needed - (return -1 if model is not defined)
+ */
+int cvmx_get_gser_num(void)
+{
+	if (!octeon_has_feature(OCTEON_FEATURE_MULTINODE))/*cn78xx*/
+		return 8; /* (8 QLMs + 6 OCI)* 2 nodes */
+	else if (OCTEON_IS_MODEL(OCTEON_CN73XX))
+		return 7; /* QLM0,1,2,3 + DLM5,6,7 */
+	else if (OCTEON_IS_MODEL(OCTEON_CNF75XX))
+		return 9;/* QLM0,1 + DLM0,1,2,3,4,5,6 */
+	return -1;
+}
+
+const char * qlm_mode_name(enum cvmx_qlm_mode mode) {
+	switch(mode) {
+		case CVMX_QLM_MODE_DISABLED:
+			return "CVMX_QLM_MODE_DISABLED";
+		case CVMX_QLM_MODE_SGMII:
+			return "CVMX_QLM_MODE_SGMII";
+		case CVMX_QLM_MODE_XAUI:
+			return "CVMX_QLM_MODE_XAUI";
+		case CVMX_QLM_MODE_RXAUI:
+			return "CVMX_QLM_MODE_RXAUI";
+		case CVMX_QLM_MODE_PCIE:
+			return "CVMX_QLM_MODE_PCIE";/* gen3 / gen2 / gen1 */
+		case CVMX_QLM_MODE_PCIE_1X2:
+			return "CVMX_QLM_MODE_PCIE_1X2";/* 1x2 gen2 / gen1 */
+		case CVMX_QLM_MODE_PCIE_2X1:
+			return "CVMX_QLM_MODE_PCIE_2X1";/* 2x1 gen2 / gen1 */
+		case CVMX_QLM_MODE_PCIE_1X1:
+			return "CVMX_QLM_MODE_PCIE_1X1";/* 1x1 gen2 / gen1 */
+		case CVMX_QLM_MODE_SRIO_1X4:
+			return "CVMX_QLM_MODE_SRIO_1X4";/* 1x4 short / long */
+		case CVMX_QLM_MODE_SRIO_2X2:
+			return "CVMX_QLM_MODE_SRIO_2X2";/* 2x2 short / long */
+		case CVMX_QLM_MODE_SRIO_4X1:
+			return "CVMX_QLM_MODE_SRIO_4X1";/* 4x1 short / long */
+		case CVMX_QLM_MODE_ILK:
+			return "CVMX_QLM_MODE_ILK";
+		case CVMX_QLM_MODE_QSGMII:
+			return "CVMX_QLM_MODE_QSGMII";
+		case CVMX_QLM_MODE_SGMII_SGMII:
+			return "CVMX_QLM_MODE_SGMII_SGMII";
+		case CVMX_QLM_MODE_SGMII_DISABLED:
+			return "CVMX_QLM_MODE_SGMII_DISABLED";
+		case CVMX_QLM_MODE_DISABLED_SGMII:
+			return "CVMX_QLM_MODE_DISABLED_SGMII";
+		case CVMX_QLM_MODE_SGMII_QSGMII:
+			return "CVMX_QLM_MODE_SGMII_QSGMII";
+		case CVMX_QLM_MODE_QSGMII_QSGMII:
+			return "CVMX_QLM_MODE_QSGMII_QSGMII";
+		case CVMX_QLM_MODE_QSGMII_DISABLED:
+			return "CVMX_QLM_MODE_QSGMII_DISABLED";
+		case CVMX_QLM_MODE_DISABLED_QSGMII:
+			return "CVMX_QLM_MODE_DISABLED_QSGMII";
+		case CVMX_QLM_MODE_QSGMII_SGMII:
+			return "CVMX_QLM_MODE_QSGMII_SGMII";
+		case CVMX_QLM_MODE_RXAUI_1X2:
+			return "CVMX_QLM_MODE_RXAUI_1X2";
+		case CVMX_QLM_MODE_SATA_2X1:
+			return "CVMX_QLM_MODE_SATA_2X1";
+		case CVMX_QLM_MODE_XLAUI:
+			return "CVMX_QLM_MODE_XLAUI";
+		case CVMX_QLM_MODE_XFI:
+			return "CVMX_QLM_MODE_XFI";
+		case CVMX_QLM_MODE_10G_KR:
+			return "CVMX_QLM_MODE_10G_KR";
+		case CVMX_QLM_MODE_40G_KR4:
+			return "CVMX_QLM_MODE_40G_KR4";
+		case CVMX_QLM_MODE_PCIE_1X8:
+			return "CVMX_QLM_MODE_PCIE_1X8";/* 1x8 gen3/gen2/gen1 */
+		case CVMX_QLM_MODE_RGMII_SGMII:
+			return "CVMX_QLM_MODE_RGMII_SGMII";
+		case CVMX_QLM_MODE_RGMII_XFI:
+			return "CVMX_QLM_MODE_RGMII_XFI";
+		case CVMX_QLM_MODE_RGMII_10G_KR:
+			return "CVMX_QLM_MODE_RGMII_10G_KR";
+		case CVMX_QLM_MODE_RGMII_RXAUI:
+			return "CVMX_QLM_MODE_RGMII_RXAUI";
+		case CVMX_QLM_MODE_RGMII_XAUI:
+			return "CVMX_QLM_MODE_RGMII_XAUI";
+		case CVMX_QLM_MODE_RGMII_XLAUI:
+			return "CVMX_QLM_MODE_RGMII_XLAUI";
+		case CVMX_QLM_MODE_RGMII_40G_KR4:
+			return "CVMX_QLM_MODE_RGMII_40G_KR4";
+		case CVMX_QLM_MODE_OCI:
+			return "CVMX_QLM_MODE_OCI";
+		default: return "UNKNOWN QLM MODE";
+	}
+};
+
+
+int cvmx_dump_gser_sata_config(unsigned node, unsigned gser, unsigned N)
+{
+	cvmx_gserx_sata_lane_rst_t	sata_lane_rst;
+	cvmx_gserx_sata_tx_invert_t	sata_tx_invert;
+	/* cvmx_gserx_sata_lanex_tx_ampx_t	sata_lane_tx_amp; */
+	/* cvmx_gserx_sata_lanex_tx_preemphx_t	sata_lane_tx_preemph; */
+
+	cvmx_dprintf("/* GSER%d SATA configuration */\n", gser);
+	PRn("Lanes:", N, "      lane%d    ", ind);
+
+	/* SATA_LANE_RST	(lane0,1) */
+	sata_lane_rst.u64 = cvmx_read_csr_node(node,
+				CVMX_GSERX_SATA_LANE_RST(gser));
+	PRns("Lane is hold in Reset(l0_rst)", N,
+		"       %3s     ",
+		sata_lane_rst.u64 & (1<<ind) ? "Yes" : " No");
+
+	/* TX_INVERT	(lane0,1) */
+	sata_tx_invert.u64 = cvmx_read_csr_node(node,
+				CVMX_GSERX_SATA_TX_INVERT(gser));
+	PRns("Tx Invert(l0_inv)", N,
+		"       %3s     ",
+		sata_tx_invert.u64 & (1<<ind) ? "Yes" : " No");
+
+	/* TX_PREEMPT	(gen1,2,3) - skip */
+	/* TX_AMPL	(gen1,2,3) - skip */
+
+	return 0;
+}
+
+int cvmx_dump_gser_pcie_config(unsigned node, unsigned gser, unsigned N)
+{
+	cvmx_gserx_pipe_lpbk_t 		pipe_lpbk;
+
+	cvmx_dprintf("/* GSER%d PCIe configuration */\n", gser);
+
+	/* GSER(0..6)_PIPE_LPBK - PCIE PCS PIPE Lookback Registers */
+	pipe_lpbk.u64 = cvmx_read_csr_node(node, CVMX_GSERX_PIPE_LPBK(gser));
+	PRns("PCIe Loopback? (pcie_lpbk)", 1, "       %3s     ",
+		pipe_lpbk.s.pcie_lpbk ? "Yes" : " No");
+	return 0;
+}
+
+int cvmx_dump_gser_lane_config(unsigned node, unsigned gser, unsigned N)
+{
+	cvmx_gserx_cfg_t 		cfg;
+	cvmx_gserx_lane_poff_t		lane_poff;
+	cvmx_gserx_lane_lpbken_t	lane_lpbken;
+	cvmx_gserx_rx_coast_t 		rx_coast;
+	cvmx_gserx_rx_eie_deten_t	rx_eie_deten;
+	cvmx_gserx_rx_polarity_t	rx_polarity;
+
+	cvmx_dprintf("/* GSER%d LANE configuration */\n", gser);
+	PRn("Lanes:", N, "      lane%d    ", ind);
+
+	cfg.u64 = cvmx_read_csr(CVMX_GSERX_CFG(gser));
+
+	/* GSER(0..6)_LANE_POFF */
+	lane_poff.u64 = cvmx_read_csr_node(node, CVMX_GSERX_LANE_POFF(gser));
+	PRcs("Per-lane Power Down (non-PCIe) (lpoff)", N,
+		/*cond*/(cfg.s.pcie==0)/* i.e non-PCIe mode */,
+		"       %3s     ",
+		(lane_poff.s.lpoff & (1<<ind)) ? "Yes" : " No");
+
+	/* GSER(0..6)_LANE_LPBKEN */
+	lane_lpbken.u64 = cvmx_read_csr_node(node, CVMX_GSERX_LANE_LPBKEN(gser));
+	PRcs("Tx-to-Rx Loopback(non-PCIe, non-SATA)(lpbken)", N,
+		/*cond*/!(cfg.s.pcie || cfg.s.sata),/* i.e non-PCIe, non-SATA */
+		"       %3s     ",
+	(lane_lpbken.s.lpbken & (1<<ind)) ? "Yes" : " No");
+
+	/* GSER(0..6)_RX_COAST */
+	rx_coast.u64 = cvmx_read_csr_node(node, CVMX_GSERX_RX_COAST(gser));
+	PRcs("Freeze CDR frequency (non-PCIe, non-SATA)(coast)", N,
+		/*cond*/!(cfg.s.pcie || cfg.s.sata),/* i.e non-PCIe, non-SATA */
+		"       %3s     ",
+		(rx_coast.s.coast & (1<<ind)) ? "Yes" : " No");
+
+	/* GSER(0..6)_RX_EIE_DETEN - RX Electrical Idle Detect Enable Register */
+	rx_eie_deten.u64 = cvmx_read_csr_node(node, CVMX_GSERX_RX_EIE_DETEN(gser));
+	PRns("EIE (Electrical Idle Exit) Detect Enabled(eiede)", N,
+		"    %8s   ",
+		(rx_eie_deten.s.eiede & (1<<ind)) ? " Enabled" : "Disabled");
+
+	/* GSER(0..6)_RX_POLARITY */
+	rx_polarity.u64 = cvmx_read_csr_node(node, CVMX_GSERX_RX_POLARITY(gser));
+	PRns("Rx Inverted? (Polarity)(rx_inv)", N, "       %3s     ",
+		(rx_polarity.s.rx_inv & (1<<ind)) ? "Yes" : " No");
+	/*
+	 * training/diag only or raw PCS regs or not related to normal mode - skip
+	 * GSER(0..6)_BR_RX(0..3)_CTL - Base-R RX Control Registers
+	 * GSER(0..6)_BR_RX(0..3)_EER - Base-R RX Equalization Evaluation Request
+	 * GSER(0..6)_BR_TX(0..3)_CTL - Base-R TX Control Registers
+	 * GSER Base-R TX Coefficient Update Registers - GSER(0..6)_BR_TX(0..3)_CUR
+	 * GSER Base-R TX Coefficient Tap Registers - GSER(0..6)_BR_TX(0..3)_TAP
+	 * GSER Lane SerDes RX Configuration 0 Registers - GSER(0..6)_LANE(0..3)_RX_CFG_0
+	 * GSER Lane SerDes RX Configuration 1 Registers - GSER(0..6)_LANE(0..3)_RX_CFG_1
+	 * GSER Lane SerDes RX Configuration 2 Registers - GSER(0..6)_LANE(0..3)_RX_CFG_2
+	 * GSER Lane SerDes RX Configuration 3 Registers - GSER(0..6)_LANE(0..3)_RX_CFG_3
+	 * GSER Lane SerDes RX Configuration 4 Registers - GSER(0..6)_LANE(0..3)_RX_CFG_4
+	 * GSER Lane SerDes RX Configuration 5 Registers - GSER(0..6)_LANE(0..3)_RX_CFG_5
+	 * GSER Lane SerDes RX CDR Control 1 Registers - GSER(0..6)_LANE(0..3)_RX_CDR_CTRL_1
+	 * GSER Lane SerDes RX CDR Control 2 Registers - GSER(0..6)_LANE(0..3)_RX_CDR_CTRL_2
+	 * GSER Lane RX Loop Control Registers - GSER(0..6)_LANE(0..3)_RX_LOOP_CTRL
+	 * GSER Lane RX Pre-Correlation Control Registers - GSER(0..6)_LANE(0..3)_RX_CTLE_CTRL
+	 * GSER Lane RX Precorrelation Control Registers - GSER(0..6)_LANE(0..3)_RX_PRECORR_CTRL
+	 * GSER Lane RX Precorrelation Count Registers - GSER(0..6)_LANE(0..3)_RX_PRECORR_VAL
+	 * GSER Lane TX Configuration 0 Registers - GSER(0..6)_LANE(0..3)_TX_CFG_0
+	 * GSER Lane TX Configuration 1 Registers - GSER(0..6)_LANE(0..3)_TX_CFG_1
+	 * GSER Lane TX Configuration 2 Registers - GSER(0..6)_LANE(0..3)_TX_CFG_2
+	 * GSER Lane TX Configuration 3 Registers - GSER(0..6)_LANE(0..3)_TX_CFG_3
+	 * GSER Lane TX Configuration Pre-Emphasis Registers - GSER(0..6)_LANE(0..3)_TX_PRE_EMPHASIS
+	 * GSER Lane PMA Loopback Control Registers - GSER(0..6)_LANE(0..3)_PMA_LOOPBACK_CTRL
+	 * GSER Lane Power Control Registers - GSER(0..6)_LANE(0..3)_PWR_CTRL
+	 * GSER Lane SerDes Pin Monitor 1 Registers - GSER(0..6)_LANE(0..3)_SDS_PIN_MON_0
+	 * GSER Lane SerDes Pin Monitor 1 Registers - GSER(0..6)_LANE(0..3)_SDS_PIN_MON_1
+	 * GSER Lane SerDes Pin Monitor 1 Registers - GSER(0..6)_LANE(0..3)_SDS_PIN_MON_2
+	 * GSER Lane RX VMA Control Registers - GSER(0..6)_LANE(0..3)_RX_VMA_CTRL
+	 * GSER Lane SerDes RX CDR Miscellaneous Control 0 Registers - GSER(0..6)_LANE(0..3)_RX_CDR_MISC_CTRL_0
+	 * GSER Lane RX Adaptive Equalizer Control Registers 0 - GSER(0..6)_LANE(0..3)_RX_VALBBD_CTRL_0
+	 * GSER Lane RX Adaptive Equalizer Control Registers 1 - GSER(0..6)_LANE(0..3)_RX_VALBBD_CTRL_1
+	 * GSER Lane RX Adaptive Equalizer Control Registers 2 - GSER(0..6)_LANE(0..3)_RX_VALBBD_CTRL_2
+	 * GSER Lane RX Miscellaneous Override Registers - GSER(0..6)_LANE(0..3)_RX_MISC_OVRRD
+	 * GSER Lane SerDes RX Adaptive Equalizer 0 Registers - GSER(0..6)_LANE(0..3)_RX_AEQ_OUT_0
+	 * GSER Lane SerDes RX Adaptive Equalizer 1 Registers - GSER(0..6)_LANE(0..3)_RX_AEQ_OUT_1
+	 * GSER Lane SerDes RX Adaptive Equalizer 2 Registers - GSER(0..6)_LANE(0..3)_RX_AEQ_OUT_2
+	 * GSER Lane SerDes RX CDR Status 0 Registers - GSER(0..6)_LANE(0..3)_RX_VMA_STATUS_0
+	 * GSER Lane SerDes RX CDR Status 1 Registers - GSER(0..6)_LANE(0..3)_RX_VMA_STATUS_1
+	 * GSER Lane SerDes RX CDR Status 1 Registers - GSER(0..6)_LANE(0..3)_RX_CDR_STATUS_1
+	 * GSER Lane SerDes RX CDR Status 2 Registers - GSER(0..6)_LANE(0..3)_RX_CDR_STATUS_2
+	 *
+	 * GSER Lane Miscellaneous Configuration 0 Registers - GSER(0..6)_LANE(0..3)_MISC_CFG_0
+	 * GSER Lane Miscellaneous Configuration 1 Registers - GSER(0..6)_LANE(0..3)_MISC_CFG_1
+	 * GSER Lane LBERT Pattern Configuration Registers - GSER(0..6)_LANE(0..3)_LBERT_PAT_CFG
+	 * GSER Lane LBERT Pattern Configuration Registers - GSER(0..6)_LANE(0..3)_LBERT_PAT_CFG
+	 * GSER Lane LBERT Configuration Registers - GSER(0..6)_LANE(0..3)_LBERT_CFG
+	 * GSER Lane LBERT Error Counter Registers - GSER(0..6)_LANE(0..3)_LBERT_ECNT
+	 * GSER Lane Raw PCS Control Interface Configuration 0 Registers - GSER(0..6)_LANE(0..3)_PCS_CTLIFC_0
+	 * GSER Lane Raw PCS Control Interface Configuration 1 Registers - GSER(0..6)_LANE(0..3)_PCS_CTLIFC_1
+	 * GSER Lane Raw PCS Control Interface Configuration 2 Registers - GSER(0..6)_LANE(0..3)_PCS_CTLIFC_2
+	 */
+
+	return 0;
+}
+
+int cvmx_dump_gser_common_config(unsigned node, unsigned gser, unsigned N)
+{
+	unsigned gser_max, modes;
+	int spd_mhz;
+	enum cvmx_qlm_mode gser_mode;
+	cvmx_gserx_cfg_t 		cfg;
+	cvmx_gserx_phy_ctl_t	 	phy_ctl;
+	cvmx_gserx_refclk_sel_t		refclk_sel;
+	cvmx_gserx_iddq_mode_t		iddq_mode;
+	cvmx_gserx_spd_t		spd;
+	cvmx_gserx_dbg_t		dbg;
+	cvmx_gserx_lane_mode_t 		lane_mode;
+	cvmx_gserx_rx_eie_filter_t	rx_eie_filter;
+	cvmx_gserx_srst_t		srst;
+	cvmx_gserx_lane_srst_t		lane_srst;
+
+	gser_max = cvmx_get_gser_num();
+	if (gser > gser_max)
+		return -1;
+
+	gser_mode = cvmx_qlm_get_mode(gser);
+
+	cvmx_dprintf("/* GSER%d common configuration */\n", gser);
+
+	/* check that only one of the model supported modes is set */
+	cfg.u64 = cvmx_read_csr(CVMX_GSERX_CFG(gser));
+
+	modes = cfg.s.pcie + cfg.s.bgx + cfg.s.sata + cfg.s.ila;
+	if (1 != modes) {
+		cvmx_dprintf("ERROR: Zero or more than one(%d) mode configured\n",
+			     modes);
+		return -1;
+	}
+	if (!OCTEON_IS_MODEL(OCTEON_CN73XX) && cfg.s.sata) {
+		/* i.e cfg.s.sata can be =1(supported) only on 73xx */
+		cvmx_dprintf("ERROR: GSER SATA mode supported only for 73xx.\n");
+		return -1;
+	}
+	if (!OCTEON_IS_MODEL(OCTEON_CN78XX) && cfg.s.ila) {
+		/* i.e cfg.s.ila can be =1(supported) only on 78xx */
+		cvmx_dprintf("ERROR: GSER ILK mode supported only for 78xx.\n");
+		return -1;
+	}
+
+	PRns("GSER Mode (PCIe/ILK/SATA/BGX) is", 1, "    %8s   ",
+		cfg.s.pcie ? "  PCIe  " :
+		cfg.s.sata ? "  SATA  " : /* .sata reserved if not supported */
+		cfg.s.ila  ? " ILK/ILA" : /* .ila=0(reset) if ILK is not supported */
+		(cfg.s.bgx && cfg.s.bgx_quad) ? "BGX_QUAD" :
+		(cfg.s.bgx && cfg.s.bgx_dual) ? "BGX_DUAL" :
+		cfg.s.bgx ? "   BGX  " : "???????");
+	PRns("GSER mode is", 1, " %s", qlm_mode_name(gser_mode));
+	PRns("GSER Number of Lanes is", 1, "        %1d      ", N);
+
+	spd_mhz = cvmx_qlm_get_gbaud_mhz_node(node, gser);
+	PRns("GSER Speed [MHz] is", 1, "     %5d     ", spd_mhz);
+
+	/* GSERX_PHY_CTL */
+	phy_ctl.u64 = cvmx_read_csr_node(node, CVMX_GSERX_PHY_CTL(gser));
+	PRns("Powered Down (phy_pd)", 1, "       %3s     ",
+		phy_ctl.s.phy_pd ? "Yes" : " No");
+	PRns("Hold in Reset (phy_reset)", 1, "       %3s     ",
+		phy_ctl.s.phy_reset ? "Yes" : " No");
+
+	/* GSERX_REFCLK_SEL */
+	refclk_sel.u64 = cvmx_read_csr_node(node, CVMX_GSERX_REFCLK_SEL(gser));
+	PRns("Reference Clock is (com_clk_sel)", 1, "    %8s   ",
+		refclk_sel.s.com_clk_sel ? "External" : "Internal");
+	PRcs("Use External Clock (use_com1)", 1,
+		/*cond*/ refclk_sel.s.com_clk_sel,/* i.e. External clk */
+		"      %4s     ", refclk_sel.s.use_com1 ? "CLK1" : "CLK0");
+	PRcs("(PCIe ONLY)Reference Clock is(pcie_refclk125)", 1,
+		/*cond*/cfg.s.pcie/* i.e PCIe mode only */,
+		"    %7s    ",
+		refclk_sel.s.pcie_refclk125 ? "125 Mhz" : " 100 Mhz");
+
+	/* GSERX_IDDQ_MODE */
+	iddq_mode.u64 = cvmx_read_csr_node(node, CVMX_GSERX_IDDQ_MODE(gser));
+	PRns("PHY powered down for IDDQ testing(phy_iddq_mode)", 1,
+		"       %3s     ", iddq_mode.s.phy_iddq_mode ? "Yes" : " No");
+	if (OCTEON_IS_MODEL(OCTEON_CN78XX)) {
+		/*  NOTE: this reg is reserved for 73xx and 75xx */
+		/*  GSERX_SPD */
+		spd.u64 = cvmx_read_csr_node(node, CVMX_GSERX_SPD(gser));
+		PRns("Speed (if supported) REF_CLK/RATE/LMODE", 1,
+			"       %s     ", SPD_string[spd.s.spd]);
+	}
+
+	/* GSERX_SRST */
+	srst.u64 = cvmx_read_csr_node(node, CVMX_GSERX_SRST(gser));
+	PRns("All per-lane State Reset(excl. PHY & CFG)(srst)", 1,
+		"       %3s     ", srst.s.srst ? "Yes" : " No");
+
+	/* GSERX_DBG */
+	dbg.u64 = cvmx_read_csr_node(node, CVMX_GSERX_DBG(gser));
+	PRcs("DEBUG Enabled for non-BGX (rxqtm_on)", 1,
+		/*cond*/ (cfg.s.bgx==0)/* i.e. non-BGX */,
+		"       %3s     ",
+		dbg.s.rxqtm_on ? "Yes" : " No");
+
+	/* GSER(0..13)_LANE_SRST - skip -diag only */
+	lane_srst.u64 = cvmx_read_csr_node(node, CVMX_GSERX_LANE_SRST(gser));
+	PRcs("All lanes State Reset(non-PCIe, non-SATA)(lsrst)", 1,
+		/*cond*/!(cfg.s.pcie || cfg.s.sata),/* i.e non-PCIe, non-SATA */
+		"       %3s     ",
+		lane_srst.s.lsrst ? "Yes" : " No");
+
+	/* GSER(0..13)_LANE_MODE */
+	lane_mode.u64 = cvmx_read_csr_node(node, CVMX_GSERX_LANE_MODE(gser));
+	PRns("LMODE name (LMODE)->index PHY table", 1,
+		"       %s     ", LMODE_string[lane_mode.s.lmode]);
+	/* GSER(0..6)_TX_VBOOST - skip */
+
+	/* GSER(0..6)_RX_EIE_FILTER - RX Electrical Idle Detect Filter Settings */
+	rx_eie_filter.u64 = cvmx_read_csr_node(node, CVMX_GSERX_RX_EIE_FILTER(gser));
+	PRn("EIE (Electrical Idle Exit) Filter count(eii_filt)", 1,
+		"     %5d     ", rx_eie_filter.s.eii_filt);
+
+	/* training/diag only or raw PCS regs or not related to normal mode - skip
+	 * GSER(0..6)_REFCLK_EVT_CTRL - skip
+	 * GSER(0..6)_REFCLK_EVT_CNTR - skip
+	 * GSER(0..6)_TXCLK_EVT_CTRL  - skip
+	 * GSER(0..6)_TXCLK_EVT_CNTR  - skip
+	 * GSER Analog Test Registers - GSER(0..6)_ANA_ATEST
+	 * GSER Analog Select Registers - GSER(0..6)_ANA_SEL
+	 * GSER Slice Configuration Registers - GSER(0..6)_SLICE_CFG
+	 * GSER RX Power Controls in Power State P2 Registers - GSER(0..6)_RX_PWR_CTRL_P2
+	 * GSER Monitor for SerDes Global to Raw PCS Global interface Registers - GSER(0..6)_GLBL_PLL_MONITOR
+	 * GSER Slice RX SDLL Registers - GSER(0..6)_SLICE(0..1)_RX_SDLL_CTRL
+	 * GSER Global Test Analog and Digital Monitor Registers - GSER(0..6)_GLBL_TAD
+	 * GSER Global Test Mode Analog/Digital Monitor Enable Registers - GSER(0..6)_GLBL_TM_ADMON
+	 * GSER TX and RX Equalization Wait Times Registers - GSER(0..6)_EQ_WAIT_TIME
+	 * GSER Receiver Detect Wait Times Registers - GSER(0..6)_RDET_TIME
+	 * GSER PLL Protocol Mode 0 Registers - GSER(0..6)_PLL_P(0..11)_MODE_0
+	 * GSER PLL Protocol Mode 1 Registers - GSER(0..6)_PLL_P(0..11)_MODE_1
+	 * GSER Lane Protocol Mode 0 Registers - GSER(0..6)_LANE_P(0..11)_MODE_0
+	 * GSER Lane Protocol Mode 1 Registers - GSER(0..6)_LANE_P(0..11)_MODE_1
+	 * GSER Lane VMA Coarse Control 0 Registers - GSER(0..6)_LANE_VMA_COARSE_CTRL_0
+	 * GSER Lane VMA Coarse Control 1 Registers - GSER(0..6)_LANE_VMA_COARSE_CTRL_1
+	 * GSER Lane VMA Coarse Control 2 Registers - GSER(0..6)_LANE_VMA_COARSE_CTRL_2
+	 * GSER Lane VMA Fine Control 0 Registers - GSER(0..6)_LANE_VMA_FINE_CTRL_0
+	 * GSER Lane VMA Fine Control 1 Registers - GSER(0..6)_LANE_VMA_FINE_CTRL_1
+	 * GSER Lane VMA Coarse Control 2 Registers - GSER(0..6)_LANE_VMA_FINE_CTRL_2
+	 */
+	return 0;
+}
+
+int cvmx_dump_gser_sata_status(unsigned node, unsigned gser, unsigned N)
+{
+	cvmx_gserx_sata_status_t	sata_status;
+
+	cvmx_dprintf("/* GSER%d SATA status */\n", gser);
+	PRn("Lanes:", N, "      lane%d    ", ind);
+
+	/* STATUS	(lane0,1) */
+	sata_status.u64 = cvmx_read_csr_node(node,
+				CVMX_GSERX_SATA_STATUS(gser));
+	PRns("PHY Lane is Ready to send/receive data(p0_rdy)", N,
+		"       %3s     ",
+		sata_status.u64 & (1<<ind) ? "Yes" : " No");
+	return 0;
+}
+
+int cvmx_dump_gser_lane_status(unsigned node, unsigned gser, unsigned N)
+{
+	cvmx_gserx_rx_eie_detsts_t	rx_eie_detsts;
+
+	cvmx_dprintf("/* GSER%d LANE status */\n", gser);
+	PRn("Lanes:", N, "      lane%d    ", ind);
+	/* GSER(0..6)_RX_EIE_DETSTS */
+	rx_eie_detsts.u64 = cvmx_read_csr_node(node, CVMX_GSERX_RX_EIE_DETSTS(gser));
+	PRns("CDR (Clock/Data Recovery) locked(cdrlock)", N,
+		"       %3s     ",
+		(rx_eie_detsts.s.cdrlock & (1<<ind)) ? "Yes" : " No");
+	PRns("EIE (Electrical Idle Exit) detect status(eiests)", N,
+		"  %12s ", (rx_eie_detsts.s.eiests & (1<<ind))
+			? "  Detected  " : "NOT Detected");
+	PRns("EIE (Electrical Idle Exit) latch status(eieltch)", N,
+		"   %11s ", (rx_eie_detsts.s.eieltch & (1<<ind))
+			? "  Latched  " : "NOT Latched");
+	return 0;
+}
+
+int cvmx_dump_gser_common_status(unsigned node, unsigned gser, unsigned N)
+{
+	enum cvmx_qlm_mode gser_mode;
+	int spd_mhz;
+	cvmx_gserx_qlm_stat_t		qlm_stat;
+	cvmx_gserx_pll_stat_t		pll_stat;
+
+	cvmx_dprintf("/* GSER%d common status */\n", gser);
+
+	gser_mode = cvmx_qlm_get_mode(gser);
+	PRns("GSER mode is", 1, " %s", qlm_mode_name(gser_mode));
+	PRns("GSER Number of Lanes is", 1, "        %1d      ", N);
+	spd_mhz = cvmx_qlm_get_gbaud_mhz_node(node, gser);
+	PRns("GSER Speed [MHz] is", 1, "     %5d     ", spd_mhz);
+
+	/* GSERX_QLM_STAT */
+	qlm_stat.u64 = cvmx_read_csr_node(node, CVMX_GSERX_QLM_STAT(gser));
+	PRns("Reset Ready/Completed (rst_rdy)", 1, "       %3s     ",
+		qlm_stat.s.rst_rdy ? "Yes" : " No");
+	PRns("There is a PLL ref clk(GSER is Powered)(dcok)", 1,
+		"       %3s     ", qlm_stat.s.dcok ? "Yes" : " No");
+	/* GSERX_PLL_STAT */
+	pll_stat.u64 = cvmx_read_csr_node(node, CVMX_GSERX_PLL_STAT(gser));
+	PRns("PLL Locked (pll_lock)", 1, "       %3s     ",
+		pll_stat.s.pll_lock ? "Yes" : " No");
+
+	return 0;
+}
+
+/**
+ * Dump QLM/DLM configuration per GSER
+ * @param node - node (use '0' for single node)
+ * @param gser - which QLM/DLM to dump config for
+ * CN73xx: GSER(0..6) mapped to [Q|D]LM(0..6)
+ */
+int cvmx_dump_gser_config_node(unsigned node, unsigned gser)
+{
+	unsigned gser_max;
+	cvmx_gserx_cfg_t cfg;
+	int N;
+
+	gser_max = cvmx_get_gser_num();
+	if (gser > gser_max) {
+		cvmx_dprintf("Unsupported model - add to cvmx_get_gser_num()\n");
+		return -1;
+	}
+
+	N = cvmx_qlm_get_lanes(gser);
+	if (N < 0) {
+		cvmx_dprintf("Unsupported model - add to cvmx_qlm_get_lanes()\n");
+		return -1;
+	}
+
+	/* Check if QLM is configured */
+	cfg.u64 = cvmx_read_csr_node(node, CVMX_GSERX_CFG(gser));
+	if (cfg.u64 == 0) {
+		cvmx_dprintf("ERROR:GSER%d: QLM mode is not configured(gser_cfg.u64=0)\n", gser);
+		return -1;
+	}
+
+	cvmx_dump_gser_common_config(node, gser, N);
+	cvmx_dump_gser_lane_config(node, gser, N);
+	if (cfg.s.pcie)
+		cvmx_dump_gser_pcie_config(node, gser, N);
+
+	else if (OCTEON_IS_MODEL(OCTEON_CN73XX) && cfg.s.sata)
+
+		/* NOTE: 78xx, 76xx and 75xx GSERs do not support SATA */
+		cvmx_dump_gser_sata_config(node, gser, N);
+
+	return 0;
+}
+
+/**
+ * Dump QLM/DLM status per GSER
+ * @param node - node (use '0' for single node)
+ * @param gser - which QLM/DLM to dump config for
+ * CN73xx: GSER(0..6) mapped to [Q|D]LM(0..6)
+ */
+int cvmx_dump_gser_status_node(unsigned node, unsigned gser)
+{
+	unsigned gser_max;
+	cvmx_gserx_cfg_t cfg;
+	int N;
+
+	gser_max = cvmx_get_gser_num();
+	if (gser > gser_max) {
+		cvmx_dprintf("Unsupported model - add to cvmx_get_gser_num()\n");
+		return -1;
+	}
+
+	N = cvmx_qlm_get_lanes(gser);
+	if (N < 0) {
+		cvmx_dprintf("Unsupported model - add to cvmx_qlm_get_lanes()\n");
+		return -1;
+	}
+
+	/* Check if QLM is configured */
+	cfg.u64 = cvmx_read_csr_node(node, CVMX_GSERX_CFG(gser));
+	if (cfg.u64 == 0) {
+		cvmx_dprintf("ERROR:GSER%d: QLM mode is not configured(gser_cfg.u64=0)\n", gser);
+		return -1;
+	}
+
+	cvmx_dump_gser_common_status(node, gser, N);
+	cvmx_dump_gser_lane_status(node, gser, N);
+	if (cfg.s.sata)
+		cvmx_dump_gser_sata_status(node, gser, N);
+
+	return 0;
+}
+
+int cvmx_dump_gser_config(unsigned gser)
+{
+	return cvmx_dump_gser_config_node(0, gser);
+}
+
+int cvmx_dump_gser_status(unsigned gser)
+{
+	return cvmx_dump_gser_status_node(0, gser);
+}
+
+#endif
diff --git a/arch/mips/cavium-octeon/executive/cvmx-range.c b/arch/mips/cavium-octeon/executive/cvmx-range.c
index aac3832..86bac75 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-range.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-range.c
@@ -78,8 +78,9 @@ static int64_t cvmx_range_find_next_available(uint64_t range_addr, uint64_t inde
 	for (i = index; i < size; i += align) {
 		uint64_t r_owner = cvmx_read64_uint64(addr_of_element(range_addr,i));
 		if (debug)
-			cvmx_dprintf("index=%d owner=%llx\n", (int) i,
-				     (unsigned long long) r_owner);
+			cvmx_dprintf("%s: index=%d owner=%llx\n",
+				__func__, (int) i, 
+				(unsigned long long) r_owner);
 		if (r_owner == CVMX_RANGE_AVAILABLE)
 			return i;
 	}
@@ -101,7 +102,8 @@ static int64_t cvmx_range_find_last_available(uint64_t range_addr, uint64_t inde
 		uint64_t r_owner = cvmx_read64_uint64(
 			addr_of_element(range_addr, i));
 		if (debug)
-			cvmx_dprintf("index=%d owner=%llx\n", (int) i,
+			cvmx_dprintf("%s: index=%d owner=%llx\n",
+				__func__, (int) i,
 				(unsigned long long) r_owner);
 		if (r_owner == CVMX_RANGE_AVAILABLE)
 			return i;
@@ -154,8 +156,13 @@ int cvmx_range_alloc_ordered(uint64_t range_addr, uint64_t owner, uint64_t cnt,
 			return first_available;
 		}
 	}
-	cvmx_dprintf("ERROR: failed to allocate range cnt=%d \n", (int)cnt);
-	cvmx_range_show(range_addr);
+
+	if (debug) {
+		cvmx_dprintf("ERROR: %s: failed to allocate range cnt=%d\n",
+			__func__, (int)cnt);
+		cvmx_range_show(range_addr);
+	}
+
 	return -1;
 }
 
@@ -174,8 +181,9 @@ int  cvmx_range_alloc_non_contiguos(uint64_t range_addr, uint64_t owner, uint64_
 	for (i = 0; i < size; i++) {
 		uint64_t r_owner = cvmx_read64_uint64(addr_of_element(range_addr,i));
 		if (debug)
-			cvmx_dprintf("index=%d owner=%llx\n", (int) i,
-				     (unsigned long long) r_owner);
+			cvmx_dprintf("%s: index=%d owner=%llx\n",
+				__func__, (int) i,
+				(unsigned long long) r_owner);
 		if (r_owner == CVMX_RANGE_AVAILABLE)
 			elements[element_index++] = (int) i;
 
@@ -184,8 +192,9 @@ int  cvmx_range_alloc_non_contiguos(uint64_t range_addr, uint64_t owner, uint64_
 	}
 	if (element_index != cnt) {
 		if (debug)
-			cvmx_dprintf("ERROR: failed to allocate non contiguous cnt=%d"
-				     " available=%d\n", (int)cnt, (int) element_index);
+			cvmx_dprintf("%s: failed to allocate non contiguous cnt=%d"
+				     " available=%d\n",
+				     __func__, (int)cnt, (int) element_index);
 		return -1;
 	}
 	for (i = 0; i < cnt; i++) {
@@ -203,8 +212,9 @@ int cvmx_range_reserve(uint64_t range_addr, uint64_t owner, uint64_t base, uint6
 
 	size = cvmx_read64_uint64(addr_of_size(range_addr));
 	if (up > size) {
-		cvmx_dprintf("ERROR: invalid base or cnt. "
+		cvmx_dprintf("ERROR: %s: invalid base or cnt. "
 			    "range_addr=0x%llx, owner=0x%llx, size=%d base+cnt=%d\n",
+			     __func__,
 			     (unsigned long long)range_addr,
 			     (unsigned long long)owner,
 			     (int)size, (int)up);
@@ -213,9 +223,13 @@ int cvmx_range_reserve(uint64_t range_addr, uint64_t owner, uint64_t base, uint6
 	for (i = base; i < up; i++) {
 		r_owner = cvmx_read64_uint64(addr_of_element(range_addr,i));
 		if (debug)
-			cvmx_dprintf("%d: %llx\n", (int) i, (unsigned long long) r_owner);
+			cvmx_dprintf("%s: %d: %llx\n",
+				__func__, (int) i,
+				(unsigned long long) r_owner);
 		if (r_owner != CVMX_RANGE_AVAILABLE) {
-			cvmx_dprintf("INFO: resource already reserved base+cnt=%d %llu %llu %llx %llx %llx\n",
+		    if (debug)
+			cvmx_dprintf("%s: resource already reserved base+cnt=%d %llu %llu %llx %llx %llx\n",
+				     __func__,
 				     (int)i, (unsigned long long)cnt, (unsigned long long)base,
 				     (unsigned long long)r_owner, (unsigned long long)range_addr,
 				     (unsigned long long)owner);
@@ -254,8 +268,9 @@ int __cvmx_range_is_allocated(uint64_t range_addr, int bases[], int count)
 	for (i = 0; i < cnt; i++) {
 		uint64_t base = bases[i];
 		if (base >= size) {
-			cvmx_dprintf("ERROR: invalid base or cnt size=%d "
-				     "base=%d \n", (int) size, (int)base);
+			cvmx_dprintf("ERROR: %s: invalid base or cnt size=%d "
+				     "base=%d \n",
+				     __func__, (int) size, (int)base);
 			return 0;
 		}
 		r_owner = cvmx_read64_uint64(addr_of_element(range_addr,base));
@@ -292,7 +307,9 @@ int cvmx_range_free_with_base(uint64_t range_addr, int base, int cnt)
 
 	size = cvmx_read64_uint64(addr_of_size(range_addr));
 	if (up > size) {
-		cvmx_dprintf("ERROR: invalid base or cnt size=%d base+cnt=%d \n", (int) size, (int)up);
+		cvmx_dprintf("ERROR: %s: invalid base or cnt size=%d"
+			" base+cnt=%d\n",
+			__func__, (int) size, (int)up);
 		return -1;
 	}
 	for (i = base; i < up; i++) {
@@ -304,9 +321,10 @@ int cvmx_range_free_with_base(uint64_t range_addr, int base, int cnt)
 uint64_t cvmx_range_get_owner(uint64_t range_addr, uint64_t base)
 {
 	uint64_t size = cvmx_read64_uint64(addr_of_size(range_addr));
+
 	if (base >= size) {
-		cvmx_dprintf("ERROR: invalid base or cnt size=%d base=%d\n",
-			(int) size, (int)base);
+		cvmx_dprintf("ERROR: %s: invalid base or cnt size=%d base=%d\n",
+			__func__, (int) size, (int)base);
 		return 0;
 	}
 	return cvmx_read64_uint64(addr_of_element(range_addr, base));
@@ -319,13 +337,18 @@ void cvmx_range_show(uint64_t range_addr)
 	size = cvmx_read64_uint64(addr_of_size(range_addr));
 	pval = cvmx_read64_uint64(addr_of_element(range_addr, 0));
 	pindex = 0;
+
+	cvmx_dprintf ("index=%d: owner %llx\n",
+				(int) pindex, CAST_ULL(pval));
+
 	for (i = 1; i < size; i++) {
 		val = cvmx_read64_uint64(addr_of_element(range_addr,i));
 		if (val != pval) {
-			cvmx_dprintf ("i=%d : %llx \n", (int) pindex, (unsigned long long)pval);
+			cvmx_dprintf ("index=%d: owner %llx\n",
+				(int) pindex, CAST_ULL(pval));
 			pindex = i;
 			pval = val;
 		}
 	}
-	cvmx_dprintf ("i=%d : %d \n", (int) pindex, (int)pval);
+	cvmx_dprintf("index=%d: owner %llx\n", (int) pindex, CAST_ULL(pval));
 }
diff --git a/arch/mips/cavium-octeon/executive/cvmx-srio.c b/arch/mips/cavium-octeon/executive/cvmx-srio.c
index 3bf4b7f..5760e9d 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-srio.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-srio.c
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2003-2015  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -72,6 +72,8 @@
 #include "cvmx-helper.h"
 #endif
 
+static const int debug = 0;
+
 #define CVMX_SRIO_CONFIG_TIMEOUT        10000	/* 10ms */
 #define CVMX_SRIO_DOORBELL_TIMEOUT      10000	/* 10ms */
 #define CVMX_SRIO_CONFIG_PRIORITY       0
@@ -137,6 +139,40 @@ static CVMX_SHARED __cvmx_srio_state_t __cvmx_srio_state[4];
 static CVMX_SHARED cvmx_spinlock_t cvmx_srio_lock[2];	/* 2 srio interfaces */
 #endif
 
+/**
+ * @INTERNAL
+ * Convert sRIO port number to RST unit number
+ */
+static unsigned __cvmx_srio_to_rst(unsigned srio_port)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_CIU3)) /* CNF75XX/CNF74XX */
+		return srio_port + 2;
+	else
+		return srio_port;
+}
+
+/**
+ * @INTERNAL
+ * Convert sRIO port number to SLI unit number
+ */
+static unsigned __cvmx_srio_to_sli(unsigned srio_port)
+{
+	if (octeon_has_feature(OCTEON_FEATURE_CIU3)) /* CNF75XX/CNF74XX */
+		return srio_port + 2;
+	else
+		return srio_port;
+}
+
+/**
+ * @INTERNAL
+ * Return the number of S2M_TYPE registers per sRIO port
+ * for the given model.
+ */
+static unsigned __cvmx_srio_num_s2m(unsigned srio_port)
+{
+	return 16;
+}
+
 #ifndef CVMX_BUILD_FOR_LINUX_HOST
 /**
  * @INTERNAL
@@ -153,10 +189,12 @@ static CVMX_SHARED cvmx_spinlock_t cvmx_srio_lock[2];	/* 2 srio interfaces */
  */
 static int __cvmx_srio_alloc_s2m(int srio_port, cvmx_sriox_s2m_typex_t s2m)
 {
-	int s2m_index;
+	unsigned s2m_index;
+	unsigned s2m_max = __cvmx_srio_num_s2m(srio_port);
+
 	/* Search through the S2M_TYPE registers looking for an unsed one or one
 	   setup the way we need it */
-	for (s2m_index = 0; s2m_index < 16; s2m_index++) {
+	for (s2m_index = 0; s2m_index < s2m_max; s2m_index++) {
 		/* Increment ref count by 2 since we count read and write
 		   independently. We might need a more complicated search in the
 		   future */
@@ -175,7 +213,9 @@ static int __cvmx_srio_alloc_s2m(int srio_port, cvmx_sriox_s2m_typex_t s2m)
 				cvmx_atomic_add32(&__cvmx_srio_state[srio_port].s2m_ref_count[s2m_index], -2);
 		}
 	}
-	cvmx_dprintf("SRIO%d: Unable to find free SRIOX_S2M_TYPEX\n", srio_port);
+	cvmx_printf("ERROR: %s: SRIO%d: "
+		"Unable to find free SRIOX_S2M_TYPEX\n", __func__,
+		srio_port);
 	return -1;
 }
 
@@ -205,6 +245,11 @@ static void __cvmx_srio_free_s2m(int srio_port, int index)
 static int __cvmx_srio_alloc_subid(cvmx_sli_mem_access_subidx_t subid)
 {
 	int mem_index;
+
+	// FIXME:
+	// This resource is shared with PCIe, so the reference counters
+	// must be shared with PCIe as well.
+	
 	/* Search through the mem access subid registers looking for an unsed one
 	   or one setup the way we need it. PCIe uses the low indexes, so search
 	   backwards */
@@ -335,6 +380,62 @@ static int __cvmx_srio_local_write32(int srio_port, uint32_t offset, uint32_t da
 	return 0;
 }
 
+
+#ifdef CVMX_BUILD_FOR_STANDALONE
+/**
+ * Convert an ipd port number to its sRIO link number per SoC model.
+ *
+ * @param ipd_port Ipd port number to convert
+ *
+ * @return Srio link number
+ */
+int cvmx_srio_ipd2srio(int ipd_port)
+{
+	int xiface;
+
+	xiface = cvmx_helper_get_interface_num(ipd_port);
+	return __cvmx_helper_srio_port(xiface);
+}
+
+/**
+ * Get our device ID
+ *
+ * @return Device ID, or negative number on error
+ */
+int cvmx_srio_get_did(int srio_port)
+{
+	cvmx_sriomaintx_pri_dev_id_t	dev_id;
+
+	if (__cvmx_srio_local_read32(srio_port,
+				     CVMX_SRIOMAINTX_PRI_DEV_ID(srio_port),
+				     &dev_id.u32))
+		return -1;
+
+	return dev_id.s.id8;
+}
+
+/**
+ * Set our device ID
+ *
+ * @return Device ID, or negative number on error
+ */
+int cvmx_srio_set_did(int srio_port, int did)
+{
+	cvmx_sriomaintx_pri_dev_id_t	dev_id;
+
+	dev_id.u32 = 0;
+	dev_id.s.id8 = did;
+	dev_id.s.id16 = did;
+
+	if (__cvmx_srio_local_write32(srio_port,
+				      CVMX_SRIOMAINTX_PRI_DEV_ID(srio_port),
+				      dev_id.u32))
+		return -1;
+
+	return 0;
+}
+#endif
+
 /**
  * Reset SRIO to link partner
  *
@@ -345,10 +446,25 @@ static int __cvmx_srio_local_write32(int srio_port, uint32_t offset, uint32_t da
 int cvmx_srio_link_rst(int srio_port)
 {
 	cvmx_sriomaintx_port_0_link_resp_t link_resp;
+	unsigned rst_port = __cvmx_srio_to_rst(srio_port);
+	uint64_t rst_reg_addr;
 
 	if (OCTEON_IS_MODEL(OCTEON_CN63XX_PASS1_X))
 		return -1;
 
+	if (octeon_has_feature(OCTEON_FEATURE_CIU3))
+		rst_reg_addr = CVMX_RST_SOFT_PRSTX(rst_port);
+	else if (srio_port == 0)
+		rst_reg_addr = CVMX_CIU_SOFT_PRST;
+	else if (srio_port == 1)
+		rst_reg_addr = CVMX_CIU_SOFT_PRST1;
+	else if (srio_port == 2)
+		rst_reg_addr = CVMX_CIU_SOFT_PRST2;
+	else if (srio_port == 3)
+		rst_reg_addr = CVMX_CIU_SOFT_PRST3;
+	else
+		return -1;
+
 	/* Generate a symbol reset to the link partner by writing 0x3. */
 	if (cvmx_srio_config_write32(srio_port, 0, -1, 0, 0, CVMX_SRIOMAINTX_PORT_0_LINK_REQ(srio_port), 3))
 		return -1;
@@ -359,30 +475,230 @@ int cvmx_srio_link_rst(int srio_port)
 	/* Poll until link partner has received the reset. */
 	while (link_resp.s.valid == 0) {
 		//cvmx_dprintf("Waiting for Link Response\n");
+		// FIXME: Add timeout !!
 		if (cvmx_srio_config_read32(srio_port, 0, -1, 0, 0, CVMX_SRIOMAINTX_PORT_0_LINK_RESP(srio_port), &link_resp.u32))
 			return -1;
 	}
 
 	/* Valid response, Asserting MAC reset */
-	cvmx_write_csr(CVMX_CIU_SOFT_PRST, 0x1);
+	cvmx_write_csr(rst_reg_addr, 0x1);
 
 	cvmx_wait(10);
 
 	/* De-asserting MAC Reset */
-	cvmx_write_csr(CVMX_CIU_SOFT_PRST, 0x0);
+	cvmx_write_csr(rst_reg_addr, 0x0);
 
 	return 0;
 }
 
-/**
- * Initialize a SRIO port for use.
- *
- * @param srio_port SRIO port to initialize
- * @param flags     Optional flags
- *
- * @return Zero on success
- */
-int cvmx_srio_initialize(int srio_port, cvmx_srio_initialize_flags_t flags)
+static int
+cvmx_srio_init_cn75xx(int srio_port, cvmx_srio_initialize_flags_t flags)
+{
+	cvmx_sriomaintx_port_lt_ctl_t port_lt_ctl;
+	cvmx_sriomaintx_port_rt_ctl_t port_rt_ctl;
+	cvmx_sriomaintx_port_0_ctl_t port_0_ctl;
+	cvmx_sriomaintx_core_enables_t core_enables;
+	cvmx_sriomaintx_port_gen_ctl_t port_gen_ctl;
+	cvmx_sriox_status_reg_t status_reg;
+	cvmx_sriox_imsg_vport_thr_t sriox_imsg_vport_thr;
+	cvmx_rst_ctlx_t rst_ctl;
+	cvmx_rst_soft_prstx_t prst;
+	cvmx_sli_mem_access_ctl_t sli_mem_access_ctl;
+	cvmx_dpi_sli_prtx_cfg_t prt_cfg;
+	cvmx_sriomaintx_port_0_ctl2_t port_0_ctl2;
+	unsigned rst_port, sli_port;
+
+	/* Check 'srio_port" range */
+	if (srio_port > 1 || srio_port < 0) {
+		cvmx_printf("ERROR: SRIO port %d out of range\n", srio_port);
+		return -1;
+	}
+
+	rst_port = __cvmx_srio_to_rst(srio_port);
+	rst_ctl.u64 = cvmx_read_csr(CVMX_RST_CTLX(rst_port));
+
+	cvmx_dprintf("INFO: SRIO%d: Port in %s mode\n",
+		srio_port, (rst_ctl.s.host_mode) ? "host" : "endpoint");
+
+	/* Reset sequence is different if SRIO is in host mode or EP mode */
+	if (rst_ctl.s.host_mode) { /* host mode */
+		prst.u64 = cvmx_read_csr(CVMX_RST_SOFT_PRSTX(rst_port));
+		if (prst.s.soft_prst == 0) {
+			/* Reset the port */
+			prst.s.soft_prst = 1;
+			cvmx_write_csr(CVMX_RST_SOFT_PRSTX(rst_port), prst.u64);
+			/* Wait until SRIO resets the port */
+			cvmx_wait_usec(2000);
+		}
+		prst.u64 = cvmx_read_csr(CVMX_RST_SOFT_PRSTX(rst_port));
+		prst.s.soft_prst = 0;
+		cvmx_write_csr(CVMX_RST_SOFT_PRSTX(rst_port), prst.u64);
+
+		/* Wait for SRIO to reset completely */
+		cvmx_wait_usec(1000);
+
+		/* Check and make sure PCIe came out of reset. If it doesn't the board
+	   	probably hasn't wired the clocks up and the interface should be
+	   	skipped */
+		if (CVMX_WAIT_FOR_FIELD64(CVMX_RST_CTLX(rst_port), cvmx_mio_rst_ctlx_t,
+					rst_done, ==, 1, 10000)) {
+			cvmx_dprintf("SRIO%d stuck in reset, skipping.\n", srio_port);
+			return -1;
+		}
+	}
+
+	/* Enable SRIO mode */
+	status_reg.u64 = cvmx_read_csr(CVMX_SRIOX_STATUS_REG(srio_port));
+	status_reg.s.srio = 1;
+	cvmx_write_csr(CVMX_SRIOX_STATUS_REG(srio_port), status_reg.u64);
+
+	/* Make sure SRIOx_STATUS_REG.access is set, needed to program
+	   additional SRIO and SRIOx Maintaince registers */
+	if (CVMX_WAIT_FOR_FIELD64(CVMX_SRIOX_STATUS_REG(srio_port),
+			cvmx_sriox_status_reg_t, access, ==, 1, 250000)) {
+		cvmx_dprintf("SRIO%d access not set, skipping.\n", srio_port);
+		return -1;
+	}
+
+	if (debug) {
+		status_reg.u64 = cvmx_read_csr(CVMX_SRIOX_STATUS_REG(srio_port));
+		cvmx_dprintf("%s: srio_port %d stat_reg=%#llx\n",
+			__func__, srio_port, CAST_ULL(status_reg.u64));
+	}
+
+	__cvmx_srio_state[srio_port].flags = flags;
+
+	// FIXME: copied these settings from old models, review.
+
+	/* Disable the link while we make changes */
+	if (__cvmx_srio_local_read32(srio_port,
+	    CVMX_SRIOMAINTX_PORT_0_CTL(srio_port), &port_0_ctl.u32)) {
+		return -1;
+	}
+	port_0_ctl.cnf75xx.o_enable = 0;
+	port_0_ctl.cnf75xx.i_enable = 0;
+	port_0_ctl.cnf75xx.prt_lock = 1;
+	port_0_ctl.cnf75xx.port_disable = 0;
+	if (__cvmx_srio_local_write32(srio_port,
+	    CVMX_SRIOMAINTX_PORT_0_CTL(srio_port), port_0_ctl.u32)) {
+		return -1;
+	}
+
+	/* Set the link layer timeout to 1ms. The default is too high and causes
+	   core bus errors */
+	if (__cvmx_srio_local_read32(srio_port,
+	    CVMX_SRIOMAINTX_PORT_LT_CTL(srio_port), &port_lt_ctl.u32))
+		return -1;
+	// FIXME: review math below - where is the clock coming from ??
+	port_lt_ctl.s.timeout = 1000000 / 200;	/* 1ms = 1000000ns / 200ns */
+	if (__cvmx_srio_local_write32(srio_port,
+	    CVMX_SRIOMAINTX_PORT_LT_CTL(srio_port), port_lt_ctl.u32))
+		return -1;
+
+	/*
+	 * Set the logical layer timeout to 100ms.
+	 * The default is too high and causes core bus errors
+	 */
+	if (__cvmx_srio_local_read32(srio_port,
+	    CVMX_SRIOMAINTX_PORT_RT_CTL(srio_port), &port_rt_ctl.u32))
+		return -1;
+	/* 100ms = 100000000ns / 200ns */
+	port_rt_ctl.s.timeout = 100000000 / 200;
+
+	if (__cvmx_srio_local_write32(srio_port,
+	    CVMX_SRIOMAINTX_PORT_RT_CTL(srio_port), port_rt_ctl.u32))
+		return -1;
+
+	/* Allow memory and doorbells. Messaging is enabled later */
+	if (__cvmx_srio_local_read32(srio_port,
+	    CVMX_SRIOMAINTX_CORE_ENABLES(srio_port), &core_enables.u32))
+		return -1;
+	core_enables.s.halt = 0;
+	core_enables.s.doorbell = 1;
+	core_enables.s.memory = 1;
+	if (__cvmx_srio_local_write32(srio_port,
+	    CVMX_SRIOMAINTX_CORE_ENABLES(srio_port), core_enables.u32))
+		return -1;
+
+	/* Allow us to master transactions */
+	if (__cvmx_srio_local_read32(srio_port,
+	    CVMX_SRIOMAINTX_PORT_GEN_CTL(srio_port), &port_gen_ctl.u32))
+		return -1;
+	port_gen_ctl.s.menable = 1;
+	if (__cvmx_srio_local_write32(srio_port,
+	    CVMX_SRIOMAINTX_PORT_GEN_CTL(srio_port), port_gen_ctl.u32))
+		return -1;
+
+	/* Configure SLI for sRIO operation */
+	sli_port = __cvmx_srio_to_sli(srio_port);
+
+	/* Set the MRRS and MPS for optimal SRIO performance */
+	prt_cfg.u64 = cvmx_read_csr(CVMX_DPI_SLI_PRTX_CFG(sli_port));
+	prt_cfg.cnf75xx.mps = 1;
+	prt_cfg.cnf75xx.mrrs = 1;
+	prt_cfg.cnf75xx.molr = 32;
+
+	if (debug)
+		cvmx_dprintf("%s: DPI_SLI_PRTX_CFG QLM_CFG=%u\n",
+			__func__, (int)prt_cfg.cnf75xx.qlm_cfg);
+
+	/* Note: QLM_CFG here always reads as 0 */
+	cvmx_write_csr(CVMX_DPI_SLI_PRTX_CFG(sli_port), prt_cfg.u64);
+
+	/* Setup RX messaging thresholds */
+	sriox_imsg_vport_thr.u64 = cvmx_read_csr(CVMX_SRIOX_IMSG_VPORT_THR(srio_port));
+	sriox_imsg_vport_thr.cnf75xx.base = 8 + (44 * srio_port);
+	sriox_imsg_vport_thr.cnf75xx.max_tot = 44;
+	sriox_imsg_vport_thr.cnf75xx.sp_vport = 1;
+	sriox_imsg_vport_thr.cnf75xx.buf_thr = 8;
+	sriox_imsg_vport_thr.cnf75xx.max_p1 = 12;	// FIXME: try 22 ?
+	sriox_imsg_vport_thr.cnf75xx.max_p0 = 12;
+	cvmx_write_csr(CVMX_SRIOX_IMSG_VPORT_THR(srio_port), sriox_imsg_vport_thr.u64);
+
+	/* Bring the link down, then up,
+	 * by writing to the SRIO port's PORT_0_CTL2 CSR.
+	 */
+	if (__cvmx_srio_local_read32(srio_port,
+	    CVMX_SRIOMAINTX_PORT_0_CTL2(srio_port), &port_0_ctl2.u32))
+		return -1;
+	if (__cvmx_srio_local_write32(srio_port,
+	    CVMX_SRIOMAINTX_PORT_0_CTL2(srio_port), port_0_ctl2.u32))
+		return -1;
+
+	/* Clear any pending interrupts */
+	cvmx_write_csr(CVMX_SRIOX_INT_REG(srio_port),
+		cvmx_read_csr(CVMX_SRIOX_INT_REG(srio_port)));
+
+	/* Enable error reporting */
+#if defined(CVMX_BUILD_FOR_STANDALONE)
+	cvmx_error_enable_group(CVMX_ERROR_GROUP_SRIO, srio_port);
+#endif
+
+	/* Finally enable the link */
+	if (__cvmx_srio_local_read32(srio_port,
+	    CVMX_SRIOMAINTX_PORT_0_CTL(srio_port), &port_0_ctl.u32))
+		return -1;
+	port_0_ctl.cnf75xx.o_enable = 1;
+	port_0_ctl.cnf75xx.i_enable = 1;
+	port_0_ctl.cnf75xx.port_disable = 0;
+	port_0_ctl.cnf75xx.prt_lock = 0;
+	if (__cvmx_srio_local_write32(srio_port,
+	    CVMX_SRIOMAINTX_PORT_0_CTL(srio_port), port_0_ctl.u32))
+		return -1;
+
+	if (debug)
+		cvmx_dprintf("%s: SRIO%u link enabled\n", __func__, srio_port);
+
+	/* Store merge control (SLI_MEM_ACCESS_CTL[TIMER,MAX_WORD]) */
+	sli_mem_access_ctl.u64 = cvmx_read_csr(CVMX_PEXP_SLI_MEM_ACCESS_CTL);
+	sli_mem_access_ctl.s.max_word = 0;	/* Allow 16 words to combine */
+	sli_mem_access_ctl.s.timer = 127;	/* Wait up to 127 cycles for more data */
+	cvmx_write_csr(CVMX_PEXP_SLI_MEM_ACCESS_CTL, sli_mem_access_ctl.u64);
+	return 0;
+}
+
+static int
+cvmx_srio_init_cn6xxx(int srio_port, cvmx_srio_initialize_flags_t flags)
 {
 	cvmx_sriomaintx_port_lt_ctl_t port_lt_ctl;
 	cvmx_sriomaintx_port_rt_ctl_t port_rt_ctl;
@@ -390,13 +706,16 @@ int cvmx_srio_initialize(int srio_port, cvmx_srio_initialize_flags_t flags)
 	cvmx_sriomaintx_core_enables_t core_enables;
 	cvmx_sriomaintx_port_gen_ctl_t port_gen_ctl;
 	cvmx_sriox_status_reg_t sriox_status_reg;
-	cvmx_mio_rst_ctlx_t mio_rst_ctl;
 	cvmx_sriox_imsg_vport_thr_t sriox_imsg_vport_thr;
 	cvmx_dpi_sli_prtx_cfg_t prt_cfg;
 	cvmx_sli_s2m_portx_ctl_t sli_s2m_portx_ctl;
 	cvmx_sli_mem_access_ctl_t sli_mem_access_ctl;
 	cvmx_sriomaintx_port_0_ctl2_t port_0_ctl2;
 
+	/* FIXME: Check 'srio_port" range */
+	if (debug)
+		cvmx_dprintf("%s: srio_port %d\n", __func__, srio_port);
+
 	sriox_status_reg.u64 = cvmx_read_csr(CVMX_SRIOX_STATUS_REG(srio_port));
 	if (OCTEON_IS_MODEL(OCTEON_CN66XX)) {
 		/* All SRIO ports are connected to QLM0 */
@@ -436,6 +755,7 @@ int cvmx_srio_initialize(int srio_port, cvmx_srio_initialize_flags_t flags)
 
 	/* Don't receive or drive reset signals for the SRIO QLM */
 	if (OCTEON_IS_MODEL(OCTEON_CN66XX)) {
+		cvmx_mio_rst_ctlx_t mio_rst_ctl;
 		/* The reset signals are available only for srio_port == 0. */
 		if (srio_port == 0 || (OCTEON_IS_MODEL(OCTEON_CN66XX_PASS1_2) && srio_port == 1)) {
 			cvmx_mio_rst_cntlx_t mio_rst_cntl;
@@ -447,7 +767,11 @@ int cvmx_srio_initialize(int srio_port, cvmx_srio_initialize_flags_t flags)
 		}
 		/* MIO_RST_CNTL2<prtmode> is initialized to 0 on cold reset */
 		mio_rst_ctl.u64 = cvmx_read_csr(CVMX_MIO_RST_CNTLX(srio_port));
+		cvmx_dprintf("SRIO%d: Port in %s mode\n",
+			srio_port, (mio_rst_ctl.s.prtmode) ? "host" : "endpoint");
 	} else {
+		cvmx_mio_rst_ctlx_t mio_rst_ctl;
+
 		mio_rst_ctl.u64 = cvmx_read_csr(CVMX_MIO_RST_CTLX(srio_port));
 		mio_rst_ctl.s.rst_drv = 0;
 		mio_rst_ctl.s.rst_rcv = 0;
@@ -455,10 +779,10 @@ int cvmx_srio_initialize(int srio_port, cvmx_srio_initialize_flags_t flags)
 		cvmx_write_csr(CVMX_MIO_RST_CTLX(srio_port), mio_rst_ctl.u64);
 
 		mio_rst_ctl.u64 = cvmx_read_csr(CVMX_MIO_RST_CTLX(srio_port));
+		cvmx_dprintf("SRIO%d: Port in %s mode\n",
+			srio_port, (mio_rst_ctl.s.prtmode) ? "host" : "endpoint");
 	}
 
-	cvmx_dprintf("SRIO%d: Port in %s mode\n", srio_port, (mio_rst_ctl.s.prtmode) ? "host" : "endpoint");
-
 	/* Bring the port out of reset if necessary */
 	switch (srio_port) {
 	case 0:
@@ -468,9 +792,6 @@ int cvmx_srio_initialize(int srio_port, cvmx_srio_initialize_flags_t flags)
 			if (prst.s.soft_prst) {
 				prst.s.soft_prst = 0;
 				cvmx_write_csr(CVMX_CIU_SOFT_PRST, prst.u64);
-				/* Wait up to 250ms for the port to come out of reset */
-				if (CVMX_WAIT_FOR_FIELD64(CVMX_SRIOX_STATUS_REG(srio_port), cvmx_sriox_status_reg_t, access, ==, 1, 250000))
-					return -1;
 			}
 			break;
 		}
@@ -481,9 +802,6 @@ int cvmx_srio_initialize(int srio_port, cvmx_srio_initialize_flags_t flags)
 			if (prst.s.soft_prst) {
 				prst.s.soft_prst = 0;
 				cvmx_write_csr(CVMX_CIU_SOFT_PRST1, prst.u64);
-				/* Wait up to 250ms for the port to come out of reset */
-				if (CVMX_WAIT_FOR_FIELD64(CVMX_SRIOX_STATUS_REG(srio_port), cvmx_sriox_status_reg_t, access, ==, 1, 250000))
-					return -1;
 			}
 			break;
 		}
@@ -494,9 +812,6 @@ int cvmx_srio_initialize(int srio_port, cvmx_srio_initialize_flags_t flags)
 			if (prst.s.soft_prst) {
 				prst.s.soft_prst = 0;
 				cvmx_write_csr(CVMX_CIU_SOFT_PRST2, prst.u64);
-				/* Wait up to 250ms for the port to come out of reset */
-				if (CVMX_WAIT_FOR_FIELD64(CVMX_SRIOX_STATUS_REG(srio_port), cvmx_sriox_status_reg_t, access, ==, 1, 250000))
-					return -1;
 			}
 			break;
 		}
@@ -507,21 +822,24 @@ int cvmx_srio_initialize(int srio_port, cvmx_srio_initialize_flags_t flags)
 			if (prst.s.soft_prst) {
 				prst.s.soft_prst = 0;
 				cvmx_write_csr(CVMX_CIU_SOFT_PRST3, prst.u64);
-				/* Wait up to 250ms for the port to come out of reset */
-				if (CVMX_WAIT_FOR_FIELD64(CVMX_SRIOX_STATUS_REG(srio_port), cvmx_sriox_status_reg_t, access, ==, 1, 250000))
-					return -1;
 			}
 			break;
 		}
 	}
 
+	/* Wait up to 250ms for the registers to become accessible */
+	if (CVMX_WAIT_FOR_FIELD64(CVMX_SRIOX_STATUS_REG(srio_port),
+				  cvmx_sriox_status_reg_t, access, ==, 1,
+				  250000))
+		return -1;
+
 	/* Disable the link while we make changes */
 	if (__cvmx_srio_local_read32(srio_port, CVMX_SRIOMAINTX_PORT_0_CTL(srio_port), &port_0_ctl.u32))
 		return -1;
 	port_0_ctl.s.o_enable = 0;
 	port_0_ctl.s.i_enable = 0;
 	port_0_ctl.s.prt_lock = 1;
-	port_0_ctl.s.disable = 0;
+	port_0_ctl.cn63xx.disable = 0;
 	if (__cvmx_srio_local_write32(srio_port, CVMX_SRIOMAINTX_PORT_0_CTL(srio_port), port_0_ctl.u32))
 		return -1;
 
@@ -734,7 +1052,7 @@ int cvmx_srio_initialize(int srio_port, cvmx_srio_initialize_flags_t flags)
 		return -1;
 	port_0_ctl.s.o_enable = 1;
 	port_0_ctl.s.i_enable = 1;
-	port_0_ctl.s.disable = 0;
+	port_0_ctl.cn63xx.disable = 0;
 	port_0_ctl.s.prt_lock = 0;
 	if (__cvmx_srio_local_write32(srio_port, CVMX_SRIOMAINTX_PORT_0_CTL(srio_port), port_0_ctl.u32))
 		return -1;
@@ -804,6 +1122,24 @@ int cvmx_srio_initialize(int srio_port, cvmx_srio_initialize_flags_t flags)
 	return 0;
 }
 
+/**
+ * Initialize a SRIO port for use.
+ *
+ * @param srio_port SRIO port to initialize
+ * @param flags     Optional flags
+ *
+ * @return Zero on success
+ */
+int cvmx_srio_initialize(int srio_port, cvmx_srio_initialize_flags_t flags)
+{
+	if (OCTEON_IS_MODEL(OCTEON_CNF75XX)) {
+		return cvmx_srio_init_cn75xx(srio_port, flags);
+	} else {
+		return cvmx_srio_init_cn6xxx(srio_port, flags);
+	}
+
+}
+
 extern int _cvmx_srio_config_read32(int, int, int, int, uint8_t, uint32_t, uint32_t *);
 
 /**
@@ -1006,12 +1342,12 @@ EXPORT_SYMBOL(cvmx_srio_config_read32);
  *
  * @return Zero on success, negative on failure.
  */
-int _cvmx_srio_config_write32(int, int, int, int, uint8_t, uint32_t, uint32_t);
-
 int cvmx_srio_config_write32(int srio_port, int srcid_index, int destid, int is16bit, uint8_t hopcount, uint32_t offset, uint32_t data)
 {
 	int ret_val = -1;
 	uint32_t result;
+	extern int _cvmx_srio_config_write32(int, int, int, int, uint8_t, uint32_t, uint32_t);
+
 
 #if (!defined(CVMX_BUILD_FOR_LINUX_KERNEL) && !defined(CVMX_BUILD_FOR_LINUX_HOST))
 	cvmx_spinlock_lock(&cvmx_srio_lock[srio_port]);
@@ -1199,7 +1535,7 @@ int cvmx_srio_send_doorbell(int srio_port, int srcid_index, int destid, int is16
 	tx_bell.s.dest_id = destid;
 	tx_bell.s.src_id = srcid_index;
 	tx_bell.s.id16 = ! !is16bit;
-	tx_bell.s.priority = priority;
+	tx_bell.cn63xx.priority = priority;
 
 	/* Make sure the previous doorbell has completed */
 	if (CVMX_WAIT_FOR_FIELD64(CVMX_SRIOX_TX_BELL(srio_port), cvmx_sriox_tx_bell_t, pending, ==, 0, CVMX_SRIO_DOORBELL_TIMEOUT)) {
@@ -1298,14 +1634,14 @@ cvmx_srio_doorbell_status_t cvmx_srio_receive_doorbell(int srio_port, int *desti
 	rx_bell.u64 = cvmx_read_csr(CVMX_SRIOX_RX_BELL(srio_port));
 	*sequence_num = rx_bell_seq.s.seq;
 	*srcid = rx_bell.s.src_id;
-	*priority = rx_bell.s.priority;
+	*priority = rx_bell.cn63xx.priority;
 	*is16bit = rx_bell.s.id16;
 	*data = rx_bell.s.data;
 	*destid_index = rx_bell.s.dest_id;
 
 	if (__cvmx_srio_state[srio_port].flags & CVMX_SRIO_INITIALIZE_DEBUG)
 		cvmx_dprintf("SRIO%d: Receive doorbell sequence=0x%x, srcid=0x%x, priority=%d, data=0x%x\n",
-			     srio_port, rx_bell_seq.s.seq, rx_bell.s.src_id, rx_bell.s.priority, rx_bell.s.data);
+			     srio_port, rx_bell_seq.s.seq, rx_bell.s.src_id, rx_bell.cn63xx.priority, rx_bell.s.data);
 
 	return CVMX_SRIO_DOORBELL_DONE;
 }
@@ -1422,6 +1758,21 @@ int cvmx_srio_receive_spf(int srio_port, void *buffer, int buffer_length)
 	return sriomaintx_ir_sp_rx_stat.s.octets;
 }
 
+/**
+ * Configure PKIND value for SRIO ports
+ *
+ * NOTE: This is for the use of global or PKI helper.
+ */
+void cvmx_srio_set_pkind(int srio_link, int index,int pkind)
+{
+	cvmx_sriox_imsg_pkindx_t srio_pkind;
+
+	srio_pkind.u64 = 0;
+	srio_pkind.cnf75xx.pknd = pkind;
+	cvmx_write_csr(CVMX_SRIOX_IMSG_PKINDX(index, srio_link),
+		srio_pkind.u64);
+}
+
 #ifndef CVMX_BUILD_FOR_LINUX_HOST
 /**
  * Map a remote device's memory region into Octeon's physical
@@ -1634,4 +1985,4 @@ int cvmx_srio_omsg_desc(uint64_t port, cvmx_buf_ptr_t * buf_ptr, cvmx_srio_tx_me
 	ret_val = 0;
 	return ret_val;
 }
-#endif
+#endif /* CVMX_BUILD_FOR_LINUX_HOST */
diff --git a/arch/mips/cavium-octeon/executive/cvmx-sso-resources.c b/arch/mips/cavium-octeon/executive/cvmx-sso-resources.c
index c6c406f..f5b0468 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-sso-resources.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-sso-resources.c
@@ -64,6 +64,8 @@ static int cvmx_sso_get_num_groups(void)
 {
 	if (OCTEON_IS_MODEL(OCTEON_CN78XX))
 		return 256;
+	if (OCTEON_IS_MODEL(OCTEON_CN73XX))
+		return 64;
 	if (OCTEON_IS_MODEL(OCTEON_CN68XX))
 		return 64;
 	return 16;
@@ -83,7 +85,7 @@ static struct global_resource_tag cvmx_sso_get_resource(int node)
 
 int cvmx_sso_reserve_group_range(int node, int *base_group, int count)
 {
-	int start;
+	int start, i;
 	uint64_t owner = 0;
 	struct global_resource_tag tag = cvmx_sso_get_resource(node);
 
@@ -103,7 +105,8 @@ int cvmx_sso_reserve_group_range(int node, int *base_group, int count)
 		if (start < 0) {
 			return CVMX_RESOURCE_ALREADY_RESERVED;
 		} else {
-			*base_group = start;
+			for (i = 0; i < count; i++)
+				base_group[i] = start + i;
 			return 0;
 		}
 	}
diff --git a/arch/mips/cavium-octeon/executive/cvmx-twsi.c b/arch/mips/cavium-octeon/executive/cvmx-twsi.c
index 074b4ee..50cc709 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-twsi.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-twsi.c
@@ -42,7 +42,7 @@
  *
  * Interface to the TWSI / I2C bus
  *
- * <hr>$Revision: 109848 $<hr>
+ * <hr>$Revision: 109574 $<hr>
  *
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
diff --git a/arch/mips/cavium-octeon/executive/cvmx-usb.c b/arch/mips/cavium-octeon/executive/cvmx-usb.c
index da6940a..4e49a63 100644
--- a/arch/mips/cavium-octeon/executive/cvmx-usb.c
+++ b/arch/mips/cavium-octeon/executive/cvmx-usb.c
@@ -2109,8 +2109,8 @@ static void __cvmx_usb_schedule(cvmx_usb_internal_state_t * usb, int is_sof)
 		channel = 31 - channel;
 		if (cvmx_unlikely(channel > 7)) {
 			if (cvmx_unlikely(usb->init_flags & CVMX_USB_INITIALIZE_FLAGS_DEBUG_INFO))
-				cvmx_dprintf("%s: Idle hardware channels has a channel higher than 7. This is wrong\n",
-					     __func__);
+				cvmx_dprintf("%s: Idle hardware channel %d has a channel higher than 7. This is wrong\n",
+					     __func__, channel);
 			break;
 		}
 
diff --git a/arch/mips/cavium-octeon/executive/octeon-feature.c b/arch/mips/cavium-octeon/executive/octeon-feature.c
index ad97437..00bf736 100644
--- a/arch/mips/cavium-octeon/executive/octeon-feature.c
+++ b/arch/mips/cavium-octeon/executive/octeon-feature.c
@@ -141,8 +141,9 @@ void __init octeon_feature_init(void)
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_FAU);
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_PKO3);
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_HNA);
+	OCTEON_FEATURE_SET(OCTEON_FEATURE_BGX);
 	OCTEON_FEATURE_SET(OCTEON_FEATURE_BGX_MIX);
-	OCTEON_FEATURE_SET(OCTEON_FEATURE_OCX);
+	OCTEON_FEATURE_SET(OCTEON_FEATURE_BGX_XCV);
 	val = OCTEON_FEATURE_SUCCESS;
 
 feature_check:
diff --git a/arch/mips/cavium-octeon/executive/octeon-model.c b/arch/mips/cavium-octeon/executive/octeon-model.c
index c7d5577..74de6c5 100644
--- a/arch/mips/cavium-octeon/executive/octeon-model.c
+++ b/arch/mips/cavium-octeon/executive/octeon-model.c
@@ -1,5 +1,5 @@
 /***********************license start***************
- * Copyright (c) 2003-2010  Cavium Inc. (support@cavium.com). All rights
+ * Copyright (c) 2003-2015  Cavium Inc. (support@cavium.com). All rights
  * reserved.
  *
  *
@@ -43,7 +43,7 @@
  * File defining functions for working with different Octeon
  * models.
  *
- * <hr>$Revision: 113335 $<hr>
+ * <hr>$Revision: 122055 $<hr>
  */
 #ifdef CVMX_BUILD_FOR_LINUX_KERNEL
 #include <asm/octeon/octeon.h>
@@ -72,7 +72,7 @@
  */
 int octeon_model_version_check(uint32_t chip_id __attribute__ ((unused)))
 {
-	/* printf("Model Number: %s\n", octeon_model_get_string(chip_id)); */
+	//printf("Model Number: %s\n", octeon_model_get_string(chip_id));
 #if !OCTEON_IS_COMMON_BINARY()
 	/* Check for special case of mismarked 3005 samples, and adjust cpuid */
 	if (chip_id == OCTEON_CN3010_PASS1 && (cvmx_read_csr(0x80011800800007B8ull) & (1ull << 34)))
@@ -451,14 +451,16 @@ const char *octeon_model_get_string_buffer(uint32_t chip_id, char *buffer)
 		if (fus_dat3.cn78xx.nozip
 		    && fus_dat3.cn78xx.nodfa_dte
 		    && fus_dat3.cn78xx.nohna_dte) {
-			if (fus_dat3.cn78xx.nozip &&
+			if (fus_dat3.cn78xx.nozip && 
 				!fus_dat2.cn78xx.raid_en &&
 				fus_dat3.cn78xx.nohna_dte) {
 				suffix = "CP";
-			} else {
+			}
+			else {
 				suffix = "SCP";
 			}
-		} else if (fus_dat2.cn78xx.raid_en == 0)
+		}
+		else if (fus_dat2.cn78xx.raid_en == 0)
 			suffix = "HCP";
 		else
 			suffix = "AAP";
@@ -474,6 +476,34 @@ const char *octeon_model_get_string_buffer(uint32_t chip_id, char *buffer)
 		else
 			suffix = "AAP";
 		break;
+	case 0x97:		/* CN73XX */
+		if (num_cores == 6)	/* Other core counts match generic */
+			core_model = "35";
+		family = "73";
+		if (fus_dat3.cn73xx.l2c_crip == 2)
+			family = "72";
+		if (fus_dat3.cn73xx.nozip
+				&& fus_dat3.cn73xx.nodfa_dte
+				&& fus_dat3.cn73xx.nohna_dte) {
+			if (!fus_dat2.cn73xx.raid_en) {
+				suffix = "CP";
+			}
+			else {
+				suffix = "SCP";
+			}
+		}
+		else 
+			suffix = "AAP";
+		break;
+	case 0x98:		/* CN75XX */
+		family = "F75";
+		if (fus_dat3.cn78xx.nozip
+		    && fus_dat3.cn78xx.nodfa_dte
+		    && fus_dat3.cn78xx.nohna_dte)
+			suffix = "SCP";
+		else
+			suffix = "AAP";
+		break;
 	default:
 		family = "XX";
 		core_model = "XX";
-- 
2.3.5

